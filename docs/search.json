[
  {
    "objectID": "topics/SART_tutorial.html",
    "href": "topics/SART_tutorial.html",
    "title": "SAR Tomography of Tropical Forests",
    "section": "",
    "text": "This tutorial demonstrates SAR tomography (TomoSAR) analysis of tropical forests using P-band data from the AfriSAR 2016 campaign. We’ll process multi-baseline SAR data to extract 3D forest structure information and compare results with LiDAR measurements. The analysis covers data preprocessing, covariance matrix computation, tomographic inversion, and validation against airborne LiDAR forest height and biomass data.\n\n\n\n\n\n\nNoteTutorial Background\n\n\n\nThis script is developed based on the TomoSAR tutorial from EO-College.\nData: F-SAR P-band TomoSAR data from AfriSAR 2016 campaign Study site: Lopé National Park, Gabon Contact: xiao.liu@mailbox.tu-dresden.de"
  },
  {
    "objectID": "topics/SART_tutorial.html#overview",
    "href": "topics/SART_tutorial.html#overview",
    "title": "SAR Tomography of Tropical Forests",
    "section": "",
    "text": "This tutorial demonstrates SAR tomography (TomoSAR) analysis of tropical forests using P-band data from the AfriSAR 2016 campaign. We’ll process multi-baseline SAR data to extract 3D forest structure information and compare results with LiDAR measurements. The analysis covers data preprocessing, covariance matrix computation, tomographic inversion, and validation against airborne LiDAR forest height and biomass data.\n\n\n\n\n\n\nNoteTutorial Background\n\n\n\nThis script is developed based on the TomoSAR tutorial from EO-College.\nData: F-SAR P-band TomoSAR data from AfriSAR 2016 campaign Study site: Lopé National Park, Gabon Contact: xiao.liu@mailbox.tu-dresden.de"
  },
  {
    "objectID": "topics/SART_tutorial.html#prerequisites",
    "href": "topics/SART_tutorial.html#prerequisites",
    "title": "SAR Tomography of Tropical Forests",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nBefore starting, you’ll need to install the required Python packages and set up your working environment.\n\n# Install required python packages (uncomment if needed)\n# !pip install -r requirements.txt\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sys import exit\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "topics/SART_tutorial.html#part-1-data-preparation",
    "href": "topics/SART_tutorial.html#part-1-data-preparation",
    "title": "SAR Tomography of Tropical Forests",
    "section": "3 Part 1: Data Preparation",
    "text": "3 Part 1: Data Preparation\n\n3.1 Setting Up the Working Directory\nDefine the project paths for input data, code, and output storage:\n\n# Set the project path (UPDATE THIS to your actual path)\nproject_path = 'E:/TomoSAR/'\n\ninpath = os.path.join(project_path, 'data')\nworkspace = os.path.join(project_path, 'code')\nos.chdir(workspace)\n\n\n\n3.2 Importing TomoSAR Functions\nLoad the specialized functions for tomographic processing and visualization:\n\nfrom tomosar_toolbox import tomobox, normalize, topo_residual_correction, \\\n                         covmat_downsampling\nfrom tomosar_plotting import cov_mat_plot, tomo_plot, \\\n                             grouped_tomosar_profiles, tomosar_phase_centre, \\\n                             tomosar_layerd_reflectivity, quick_look, insar_quick_look\n\n\n\n\n\n\n\nImportantRequired Functions\n\n\n\nThese custom functions handle the core TomoSAR processing pipeline. Ensure the tomosar_toolbox.py and tomosar_plotting.py modules are in your workspace directory."
  },
  {
    "objectID": "topics/SART_tutorial.html#part-2-parameter-configuration",
    "href": "topics/SART_tutorial.html#part-2-parameter-configuration",
    "title": "SAR Tomography of Tropical Forests",
    "section": "4 Part 2: Parameter Configuration",
    "text": "4 Part 2: Parameter Configuration\n\n4.1 Defining Processing Parameters\nConfigure the key parameters for TomoSAR processing:\n\n# Pixel spacing\nps_rg = 1.19  # Pixel spacing in range (meters)\nps_az = 0.9   # Pixel spacing in azimuth (meters)\n\n# Define the boxcar smoothing dimension (in meters)\nmulti_look = 25  # Options: 25, 50\n\n# Define the max height for the inversion\nheight = 65\nz_vector = np.arange(-height, height + 1, 1)\n\n# Select polarization\npol = 'hh'  # Options: 'hh', 'hv', 'vv'\n\n# Select TomoSAR algorithm\ntomo_method = 'capon'  # Options: 'capon', 'beamforming'\n\n# Flag for terrain normalization\nterrain_cor_flag = 0  # 0: without terrain normalization, 1: with terrain normalization\n\nif terrain_cor_flag == 0: \n    outpath = os.path.join(project_path, 'out')  # Save results without terrain normalization\nelse:\n    outpath = os.path.join(project_path, 'out/terrain_normalised')  # Save results with terrain normalization\n\n\n\n\n\n\n\nTipParameter Selection\n\n\n\n\nMulti-look size: Larger values (50m) provide more stable estimates but lower spatial resolution\nPolarization: HH is most sensitive to forest structure; HV to volume scattering\nCapon beamformer: Provides better vertical resolution than conventional beamforming\nTerrain correction: Essential for accurate height estimation in sloped terrain"
  },
  {
    "objectID": "topics/SART_tutorial.html#part-3-data-loading-and-visualization",
    "href": "topics/SART_tutorial.html#part-3-data-loading-and-visualization",
    "title": "SAR Tomography of Tropical Forests",
    "section": "5 Part 3: Data Loading and Visualization",
    "text": "5 Part 3: Data Loading and Visualization\n\n5.1 Loading SAR Data\nRead the pre-processed SAR data stack:\n\noutname = os.path.join(inpath, '{}_sample_data.npz'.format(pol))\ndata = np.load(outname)\n\nslc_stack = data['slc_stack']      # Single Look Complex (SLC) image\nkz_stack = data['kz_stack']        # Vertical wavenumber\nphase_stack = data['phase_stack']  # Topographical phase\n\n# Optional: use subset for testing\n# slc_stack = slc_stack[:,:,:5]\n# kz_stack = kz_stack[:,:,:5]\n# phase_stack = phase_stack[:,:,:5]\n\ndel data\n\nn_row, n_col, n_track = slc_stack.shape\n\nprint(f\"Data dimensions: {slc_stack.shape}\")\nprint(f\"Number of tracks: {n_track}\")\n\nData dimensions: (1100, 1000, 10)\nNumber of tracks: 10\n\n\n\n\n\n\n\n\nNoteData Structure\n\n\n\nThe data contains:\n\nslc_stack: Complex SAR image stack (rows × columns × tracks)\nkz_stack: Vertical wavenumber for each baseline\nphase_stack: Topographic phase for each acquisition\n\n\n\n\n\n5.2 Quick Look at SAR Data\nVisualize the SAR intensity, phase, vertical wavenumber, and topographic phase:\n\nslc_id = 3\nimg_path = os.path.join(outpath, 'Track_{}_quick_look.png'.format(slc_id))         \nquick_look(slc_id, slc_stack, kz_stack, phase_stack, img_path,\n          figsize=(10, 8), dpi=150, fontsize=12)\n\n\n\n\nQuick look at SAR data: intensity, phase, kz, and topographic phase"
  },
  {
    "objectID": "topics/SART_tutorial.html#part-4-phase-calibration",
    "href": "topics/SART_tutorial.html#part-4-phase-calibration",
    "title": "SAR Tomography of Tropical Forests",
    "section": "6 Part 4: Phase Calibration",
    "text": "6 Part 4: Phase Calibration\n\n6.1 Removing Flat-Earth and Topography Phase\nCalibrate the SAR data by removing the flat-earth and topographic phase contributions:\n\nprint('Remove flat-earth and topography phase')\noutname = os.path.join(outpath, '{}_normalized_stack.npy'.format(pol))\n\nnormalized_stack = slc_stack.copy()\n# Start from second SLC (kz and dem_phase of master SLC are 0)\nfor n in tqdm(range(1, n_track)):\n    dem_phase = phase_stack[:,:,n].squeeze()\n    slc_uncal = slc_stack[:,:,n].squeeze()\n    normalized_stack[:,:,n] = slc_uncal * np.exp(1j * dem_phase)  # Phase calibration\ndel dem_phase, slc_uncal\n\nnp.save(outname, normalized_stack)\n\nRemove flat-earth and topography phase\n\n\n\n\n\n\n\n\nImportantWhy Phase Calibration Matters\n\n\n\nThe flat-earth and topographic phases dominate the interferometric phase and must be removed to isolate the vertical structure information. This step is crucial for accurate TomoSAR reconstruction.\n\n\n\n\n6.2 Removing Residual Topography Phase\nFor terrain-corrected processing, remove any remaining topographic phase:\n\nif terrain_cor_flag == 1:\n    hh_tomo_path = os.path.join(project_path + 'out/',\n                                'hh_tomo_ml{}_h{}_capon.npy'.format(multi_look, height))\n    \n    if os.path.isfile(hh_tomo_path) == False:\n        print('Please first calculate the HH tomography without terrain correction.')\n        terrain_cor_flag = 0\n        exit(0)\n    else:\n        print('Remove residual topography phase')\n        terrain_path = os.path.join(project_path + 'out/',\n                                    'tomo_ml{}_h{}'.format(multi_look, height) + '_hh_capon_terrain.npy')\n        \n        normalized_stack, terrain = topo_residual_correction(normalized_stack, kz_stack, z_vector, \n                                                             hh_tomo_path, terrain_path)\n        \n        # Plot terrain residual\n        plt.figure(dpi=300)\n        plt.imshow(terrain, cmap='jet', vmax=30, vmin=-30)\n        plt.colorbar()\n        plt.title('Topography residual (m)')\n        plt.show()\n\n\n\n6.3 Visualizing Phase Calibration Results\nCompare the interferometric phase before and after calibration:\n\nslc_1_id, slc_2_id = 1, 3\ninsar_quick_look(slc_1_id, slc_2_id, slc_stack, normalized_stack,\n                figsize=(12, 5), dpi=150)\n\n\n\n\nInSAR phase before and after removing flat-earth and topography phase"
  },
  {
    "objectID": "topics/SART_tutorial.html#part-5-covariance-matrix-computation",
    "href": "topics/SART_tutorial.html#part-5-covariance-matrix-computation",
    "title": "SAR Tomography of Tropical Forests",
    "section": "7 Part 5: Covariance Matrix Computation",
    "text": "7 Part 5: Covariance Matrix Computation\n\n7.1 Computing Covariance Matrix with Multi-looking\nCalculate the covariance matrix with spatial averaging to improve signal-to-noise ratio:\n\nprint('Calculate covariance matrix')\ncovariance_matrix, r_out_smpl, x_out_smpl = covmat_downsampling(normalized_stack, multi_look, ps_rg, ps_az)\noutname = os.path.join(outpath, '{}_cov_matrix_ml{}.npy'.format(pol, multi_look))\nnp.save(outname, covariance_matrix)\n\n# Update kz using downsampling index along azimuth and range\nkz_stack_down = kz_stack[r_out_smpl,:,:][:,x_out_smpl,:]\n\nprint(f\"Covariance matrix shape: {covariance_matrix.shape}\")\n\nCalculate covariance matrix\n\n\nCovariance matrix shape: (180, 139, 10, 10)\n\n\n\n\n\n\n\n\nTipMulti-looking Trade-off\n\n\n\nMulti-looking reduces speckle noise but decreases spatial resolution. The 25m window size balances these considerations for forest structure analysis.\n\n\n\n\n7.2 Visualizing Covariance Matrix\nDisplay the covariance matrix structure:\n\nimg_path = os.path.join(outpath, '{}_covariance_matrix_quick_look.png'.format(pol))\ncov_mat_plot(covariance_matrix, img_path,\n            figsize=(10, 10), dpi=150, fontsize=18)\n\n\n\n\nCovariance matrix visualization"
  },
  {
    "objectID": "topics/SART_tutorial.html#part-6-tomographic-inversion",
    "href": "topics/SART_tutorial.html#part-6-tomographic-inversion",
    "title": "SAR Tomography of Tropical Forests",
    "section": "8 Part 6: Tomographic Inversion",
    "text": "8 Part 6: Tomographic Inversion\n\n8.1 Computing TomoSAR Reflectivity Profiles\nPerform the tomographic inversion to estimate vertical backscatter profiles:\n\ntomo = tomobox(covariance_matrix, kz_stack_down, z_vector, outname, tomo_method)\n\noutname = os.path.join(outpath, '{}_tomo_ml{}_h{}_{}.npy'.format(pol, multi_look, height, tomo_method))\nnp.save(outname, tomo)\n\nprint(f\"TomoSAR result shape: {tomo.shape}\")\n\nTomoSAR result shape: (180, 139, 131)\n\n\n\n\n\n\n\n\nNoteTomoSAR Methods\n\n\n\n\nCapon (MVDR): Minimum Variance Distortionless Response beamformer, provides better resolution\nBeamforming: Conventional delay-and-sum beamformer, more robust but lower resolution\n\n\n\n\n\n8.2 Normalizing Reflectivity Profiles\nNormalize the tomographic reflectivity to [0, 1] for each pixel:\n\ntomo_norm = np.apply_along_axis(normalize, 2, tomo)\n\n\n\n8.3 Visualizing Tomographic Results\nPlot example vertical reflectivity profiles:\n\nrg_ratio, az_ratio = 0.5, 0.6\n\nrg, az = int(rg_ratio * tomo_norm.shape[1]), int(az_ratio * tomo_norm.shape[0])\nimg_path = os.path.join(outpath, '{}_tomosar_example_az_{}_rg_{}_{}.png'.format(pol, az, rg, tomo_method))\ntomo_plot(rg, az, slc_stack, tomo_norm, height, pol, img_path,\n         figsize=(10, 8), dpi=150, fontsize=12)\n\n\n\n\nExample TomoSAR vertical reflectivity profile\n\n\n\n\n\n\n\n\n\n\nImportantInterpreting Reflectivity Profiles\n\n\n\nThe vertical reflectivity profile shows:\n\nGround peak: Strong backscatter from the ground surface\nCanopy volume: Distributed scattering from vegetation layers\nCanopy top: Upper extent of forest structure"
  },
  {
    "objectID": "topics/SART_tutorial.html#part-7-validation-with-lidar-data",
    "href": "topics/SART_tutorial.html#part-7-validation-with-lidar-data",
    "title": "SAR Tomography of Tropical Forests",
    "section": "9 Part 7: Validation with LiDAR Data",
    "text": "9 Part 7: Validation with LiDAR Data\n\n9.1 Loading LiDAR Reference Data\nLoad airborne LiDAR forest height and above-ground biomass data:\n\nif terrain_cor_flag == 1:\n    lvis_rh_path = os.path.join(inpath, 'lidar_rh100_25m.npy')\n    lvis_agb_path = os.path.join(inpath, 'lidar_agb_50m.npy')\n    \n    lvis_rh = np.load(lvis_rh_path)\n    lvis_agb = np.load(lvis_agb_path)\n    \n    from skimage.transform import resize\n    lvis_rh = resize(lvis_rh, (tomo_norm.shape[0], tomo_norm.shape[1]))\n    lvis_agb = resize(lvis_agb, (tomo_norm.shape[0], tomo_norm.shape[1]))\n    \n    print(f\"LiDAR height range: {lvis_rh.min():.2f} - {lvis_rh.max():.2f} m\")\n    print(f\"LiDAR AGB range: {lvis_agb.min():.2f} - {lvis_agb.max():.2f} Mg/ha\")\n\n\n\n\n\n\n\nNoteLiDAR Data Resolution\n\n\n\n\nForest height (RH100): 25 m resolution\nAbove-ground biomass (AGB): 50 m resolution\n\nBoth are resampled to match the TomoSAR grid.\n\n\n\n\n9.2 Comparing TomoSAR with LiDAR Height\nOverlay LiDAR forest height on TomoSAR profiles:\n\nif terrain_cor_flag == 1:\n    rg_ratio, az_ratio = np.random.rand(), np.random.rand()\n    rg, az = int(rg_ratio * tomo_norm.shape[1]), int(az_ratio * tomo_norm.shape[0])\n    \n    img_path = os.path.join(outpath, '{}_tomosar_example_az_{}_rg_{}_lidar.png'.format(pol, az, rg))\n    tomo_plot(rg, az, slc_stack, tomo_norm, height, pol, img_path, lvis_rh,\n             figsize=(10, 8), dpi=150, fontsize=12)\n\n\n\n9.3 Aggregating Profiles by Forest Height and Biomass\nAnalyze TomoSAR reflectivity patterns across different forest height and biomass classes:\n\nif terrain_cor_flag == 1:\n    img_path = os.path.join(outpath, '{}_tomosar_aggregated_profiles.png'.format(pol))\n    grouped_tomosar_profiles(tomo, lvis_rh, lvis_agb, z_vector, img_path,\n                             figsize=(11, 6), dpi=150, fontsize=12)\n\n\n\n9.4 TomoSAR Phase Center vs. LiDAR Height\nCompare the TomoSAR phase center with LiDAR forest height:\n\nif terrain_cor_flag == 1:\n    img_path = os.path.join(outpath, 'lidar_height_{}_tomosar_phase_centre.png'.format(pol))\n    tomosar_phase_centre(tomo, lvis_rh, z_vector, img_path,\n                         figsize=(12, 4), dpi=150)\n\n\n\n\n\n\n\nTipPhase Center Height\n\n\n\nThe phase center represents the weighted center of the vertical backscatter distribution. For forests, it typically falls between the ground and canopy top, depending on penetration depth and canopy structure.\n\n\n\n\n9.5 Layer-wise Reflectivity vs. Biomass\nExamine TomoSAR reflectivity at different height layers in relation to biomass:\n\nif terrain_cor_flag == 1:\n    min_agb = 50  # Minimum AGB threshold (Mg/ha)\n    img_path = os.path.join(outpath, 'lidar_agb_{}_tomosar_reflectivity_at'.format(pol))\n    tomosar_layerd_reflectivity(tomo, lvis_agb, min_agb, height, img_path,\n                                figsize=(12, 4), dpi=150)"
  },
  {
    "objectID": "topics/SART_tutorial.html#summary-and-exercises",
    "href": "topics/SART_tutorial.html#summary-and-exercises",
    "title": "SAR Tomography of Tropical Forests",
    "section": "10 Summary and Exercises",
    "text": "10 Summary and Exercises\n\nKey TakeawaysBest PracticesExercises\n\n\nData Preprocessing - Multi-baseline SAR data requires careful phase calibration - Terrain correction is essential for accurate height estimation - Multi-looking balances noise reduction and spatial resolution\nTomographic Inversion - Capon beamformer provides better vertical resolution than conventional methods - Vertical profiles reveal ground and canopy structure - Phase center height correlates with forest structure\nValidation Results - TomoSAR profiles show strong agreement with LiDAR measurements - Penetration depth varies with polarization and forest density - Biomass estimation benefits from multi-layer information\n\n\n\nAlways perform terrain correction for sloped terrain\nUse HH polarization for maximum ground penetration\nValidate results with independent reference data (LiDAR, field measurements)\nConsider multiple polarizations for comprehensive analysis\nDocument all processing parameters for reproducibility\n\n\n\nCompare results using different:\n\nPolarizations: HH, HV, VV\n\nHow does penetration depth vary?\nWhich polarization best correlates with biomass?\n\nTomography methods: Capon vs. Beamforming\n\nCompare vertical resolution\nAssess robustness to noise\n\nMulti-look sizes: 25m vs. 50m\n\nTrade-off between resolution and stability\nImpact on height estimation accuracy\n\nDataset size: Full dataset vs. subset\n\nEffect of number of baselines\nMinimum tracks required for reliable inversion"
  },
  {
    "objectID": "topics/SART_tutorial.html#conclusion",
    "href": "topics/SART_tutorial.html#conclusion",
    "title": "SAR Tomography of Tropical Forests",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nThis tutorial demonstrates a complete workflow for SAR tomography of tropical forests, from data preprocessing through validation with LiDAR. The ability to extract 3D forest structure from SAR data provides valuable information for biomass estimation, forest monitoring, and carbon accounting, especially in tropical regions where optical remote sensing is limited by cloud cover.\nThe integration of multiple polarizations, processing methods, and independent validation data creates a robust framework for understanding forest vertical structure. These techniques are applicable to various forest types and SAR systems, making them valuable tools for large-scale forest monitoring.\n\n\n\n\n\n\nWarningImportant Considerations\n\n\n\nWhen applying these methods to your own data:\n\nBaseline configuration: Ensure adequate vertical wavenumber sampling\nData quality: Check for phase unwrapping errors and temporal decorrelation\nTerrain effects: Always apply terrain correction in mountainous areas\nValidation data: Use concurrent LiDAR or field measurements when possible\nProcessing parameters: Adjust multi-look size based on forest heterogeneity\n\n\n\n\nThis tutorial provides both theoretical understanding and practical implementation for SAR tomography analysis. The modular structure allows adaptation to different datasets and research questions."
  },
  {
    "objectID": "topics/SART_intro.html",
    "href": "topics/SART_intro.html",
    "title": "SAR Tomography of Forests: An Introduction",
    "section": "",
    "text": "Forests are three-dimensional ecosystems, yet most remote sensing techniques observe them from above, collapsing their vertical structure into a single measurement. Synthetic Aperture Radar (SAR) tomography breaks through this limitation, using the phase information from multiple radar acquisitions to reconstruct the full three-dimensional structure of forests—from the ground surface through the understory to the canopy top.\nFrom Medical Imaging to Forest Mapping\nThe concept of tomography—reconstructing an object’s internal structure from its projections—originated in medical imaging with CT and MRI scans. SAR tomography applies the same principle to forests: by acquiring radar images from multiple slightly different positions (either from repeated aircraft passes or satellite orbits), we create a synthetic aperture perpendicular to the line of sight. This additional dimension enables us to resolve scatterers at different heights within each ground pixel, effectively creating a three-dimensional radar “CT scan” of the forest.\nPenetrating the Canopy with Radar Waves\nUnlike optical sensors that see only the canopy surface, and unlike lidar whose penetration is limited by vegetation density, long-wavelength radar—particularly P-band with its ~70 cm wavelength—can penetrate deep into forest canopies. Different polarizations interact with different structural elements: HH polarization tends to reach the ground, HV cross-polarization highlights volume scattering from branches and leaves, while VV shows intermediate behavior. By analyzing the vertical distribution of radar backscatter and separating ground from volume scattering, TomoSAR provides unique insights into forest structure, height, and biomass.\nFrom Phase Patterns to Forest Metrics\nThe key to SAR tomography lies in interferometric phase—the relative distance encoded in the interference patterns between multiple acquisitions. Through coherence estimation, tomographic inversion algorithms (such as Capon beamforming), and careful calibration with ground truth data, these phase measurements transform into vertical reflectivity profiles. These profiles reveal not just forest height but also the distribution of biomass throughout the canopy, changes in structure over time, and responses to environmental conditions like moisture and seasonal phenology. With ESA’s BIOMASS mission—the first satellite specifically designed for forest tomography—now operational, global-scale 3D forest monitoring has become a reality."
  },
  {
    "objectID": "topics/SART_intro.html#introduction-to-sar-tomography-of-forests",
    "href": "topics/SART_intro.html#introduction-to-sar-tomography-of-forests",
    "title": "SAR Tomography of Forests: An Introduction",
    "section": "",
    "text": "Forests are three-dimensional ecosystems, yet most remote sensing techniques observe them from above, collapsing their vertical structure into a single measurement. Synthetic Aperture Radar (SAR) tomography breaks through this limitation, using the phase information from multiple radar acquisitions to reconstruct the full three-dimensional structure of forests—from the ground surface through the understory to the canopy top.\nFrom Medical Imaging to Forest Mapping\nThe concept of tomography—reconstructing an object’s internal structure from its projections—originated in medical imaging with CT and MRI scans. SAR tomography applies the same principle to forests: by acquiring radar images from multiple slightly different positions (either from repeated aircraft passes or satellite orbits), we create a synthetic aperture perpendicular to the line of sight. This additional dimension enables us to resolve scatterers at different heights within each ground pixel, effectively creating a three-dimensional radar “CT scan” of the forest.\nPenetrating the Canopy with Radar Waves\nUnlike optical sensors that see only the canopy surface, and unlike lidar whose penetration is limited by vegetation density, long-wavelength radar—particularly P-band with its ~70 cm wavelength—can penetrate deep into forest canopies. Different polarizations interact with different structural elements: HH polarization tends to reach the ground, HV cross-polarization highlights volume scattering from branches and leaves, while VV shows intermediate behavior. By analyzing the vertical distribution of radar backscatter and separating ground from volume scattering, TomoSAR provides unique insights into forest structure, height, and biomass.\nFrom Phase Patterns to Forest Metrics\nThe key to SAR tomography lies in interferometric phase—the relative distance encoded in the interference patterns between multiple acquisitions. Through coherence estimation, tomographic inversion algorithms (such as Capon beamforming), and careful calibration with ground truth data, these phase measurements transform into vertical reflectivity profiles. These profiles reveal not just forest height but also the distribution of biomass throughout the canopy, changes in structure over time, and responses to environmental conditions like moisture and seasonal phenology. With ESA’s BIOMASS mission—the first satellite specifically designed for forest tomography—now operational, global-scale 3D forest monitoring has become a reality."
  },
  {
    "objectID": "topics/SART_intro.html#where-to-go-further",
    "href": "topics/SART_intro.html#where-to-go-further",
    "title": "SAR Tomography of Forests: An Introduction",
    "section": "Where to go further?",
    "text": "Where to go further?\nWatch the video lecture in which Dr. Xiao Liu introduces the key concepts for this lesson:\n\nView and download the presentation from the video:\n\nRead the theory\nAnalyze real data with a practical tutorial in Python"
  },
  {
    "objectID": "topics/hyperspectral_theory.html",
    "href": "topics/hyperspectral_theory.html",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "",
    "text": "Remote sensing technology enables us to observe and quantify plant characteristics across scales, from individual leaves to entire ecosystems. At the heart of this capability lies the fundamental principle that different plant compounds interact uniquely with electromagnetic radiation (Gates et al. 1965). While human vision is limited to three broad spectral bands (red, green, and blue), hyperspectral sensors capture reflectance across hundreds of narrow, contiguous wavelength bands, revealing detailed information about plant biochemistry, physiology, and structure that remains invisible to the naked eye (Ustin and Gamon 2010).\nThe interaction between light and vegetation is governed by the absorption, transmission, and reflection properties of various biochemical compounds within plant tissues. Each pigment, structural component, and water molecule creates a characteristic spectral signature—a unique pattern of absorption and reflection across the electromagnetic spectrum (Jacquemoud and Baret 1990). By analyzing these signatures, we can non-destructively estimate pigment concentrations, assess physiological status, detect stress, and classify vegetation types (Peñuelas et al. 1993)."
  },
  {
    "objectID": "topics/hyperspectral_theory.html#introduction-the-spectral-language-of-vegetation",
    "href": "topics/hyperspectral_theory.html#introduction-the-spectral-language-of-vegetation",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "",
    "text": "Remote sensing technology enables us to observe and quantify plant characteristics across scales, from individual leaves to entire ecosystems. At the heart of this capability lies the fundamental principle that different plant compounds interact uniquely with electromagnetic radiation (Gates et al. 1965). While human vision is limited to three broad spectral bands (red, green, and blue), hyperspectral sensors capture reflectance across hundreds of narrow, contiguous wavelength bands, revealing detailed information about plant biochemistry, physiology, and structure that remains invisible to the naked eye (Ustin and Gamon 2010).\nThe interaction between light and vegetation is governed by the absorption, transmission, and reflection properties of various biochemical compounds within plant tissues. Each pigment, structural component, and water molecule creates a characteristic spectral signature—a unique pattern of absorption and reflection across the electromagnetic spectrum (Jacquemoud and Baret 1990). By analyzing these signatures, we can non-destructively estimate pigment concentrations, assess physiological status, detect stress, and classify vegetation types (Peñuelas et al. 1993)."
  },
  {
    "objectID": "topics/hyperspectral_theory.html#seasonal-dynamics-and-plant-functional-strategies",
    "href": "topics/hyperspectral_theory.html#seasonal-dynamics-and-plant-functional-strategies",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "2 Seasonal Dynamics and Plant Functional Strategies",
    "text": "2 Seasonal Dynamics and Plant Functional Strategies\nPlants exhibit remarkable temporal variability in their spectral properties, driven by phenological cycles, resource allocation strategies, and environmental responses (Richardson et al. 2018). Understanding these dynamics is fundamental to interpreting remote sensing data correctly.\nFigure 1 illustrates the dramatic differences between plant functional types and their seasonal trajectories. The comparison between flowers/fruits and leaves demonstrates distinct spectral behaviors, while the seasonal progression from winter dormancy through spring leaf flush, summer maturity, and autumn senescence shows systematic changes in reflectance patterns (Huete et al. 2002). The tree diagrams depicting seasonal leaf development—from bare branches in winter, through flowering in spring, full canopy development in summer, to leaf senescence and abscission in autumn—provide a conceptual framework for understanding temporal spectral variability.\n\n\n\n\n\n\nFigure 1: Seasonal phenological patterns in plant functional traits. a: Photographs showing inflorescence and fruits (top row) and leaf phenology (bottom row) in deciduous trees across seasons. b: Seasonal tree phenology diagrams from winter through autumn. Source: Golan-Goldhirsch A., Kozhoridze G., et al. (unpublished work).\n\n\n\nThe distinction between mature leaves, juvenile leaves, and senescence leaves highlights that not all foliage within a canopy shares identical spectral properties (Asner and Martin 2015). This heterogeneity must be considered when scaling from leaf-level measurements to canopy or landscape observations.\nMoreover, while the general phenological pattern—spring green-up, summer maturity, autumn senescence—is shared across many temperate plant species, the timing, magnitude, and spectral characteristics of these transitions vary systematically among species (Richardson et al. 2018; Asner and Martin 2015). Different species exhibit distinct phenological schedules: some leaf out earlier in spring, others maintain green foliage longer into autumn, and evergreen species show entirely different seasonal patterns. These species-specific differences in phenological timing and pigment dynamics create unique spectral-temporal signatures that enable remote differentiation and mapping of plant species or functional types across landscapes (Ustin and Gamon 2010). By tracking how reflectance changes over time, rather than relying on snapshots from single dates, researchers can leverage these phenological differences to improve vegetation classification and monitor ecosystem composition (Richardson et al. 2018)."
  },
  {
    "objectID": "topics/hyperspectral_theory.html#pigment-specific-spectral-signatures",
    "href": "topics/hyperspectral_theory.html#pigment-specific-spectral-signatures",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "3 Pigment-Specific Spectral Signatures",
    "text": "3 Pigment-Specific Spectral Signatures\nPigment concentrations show typical temporal patterns throughout the growing season. Because each pigment type is characterized by specific spectral signature (Figure 2), specific concentrations of different pigments result in a specific overall spectral response of a leave. This enables us to detect phenology and health status of trees by spectral measurements. The following three key phenological stages can be determined:\n\nEarly season dynamics (March-May): Rapid increases in pigment concentrations as leaves develop, starting with anthocyanins (red juvenile leaves) and followed by chlorophyll content rising sharply during leaf expansion (Richardson et al. 2013).\nMid-season stability (June-August): Relatively stable biochemical composition during peak photosynthetic activity, though with notable variability reflecting environmental conditions and stress responses. Chloropylles are dominating (green mature leaves), unless stress conditions lead to increased dominance of other pigments (anthocyanins, carotenoids).\nLate season senescence (September-October): Declining chlorophyll with differential changes in other compounds, creating the characteristic autumn coloration (Gitelson and Merzlyak 2001).\n\n\n\n\n\n\n\nFigure 2: Spectral signatures of major plant pigments across the visible spectrum (400-800 nm). Top: Chlorophyll spectral reflectance showing characteristic absorption bands at 520-570 nm and 695-735 nm with near-infrared plateau at 750-800 nm. Middle: Carotenoid reflectance patterns with absorption features at 500-520 nm, 540-560 nm, and 700-710 nm. Bottom: Anthocyanin spectral signature showing strong absorption in the green-yellow region (540-560 nm) and at 700-710 nm. Colored boxes indicate key wavelength regions used in vegetation index formulations (Gitelson, Keydan, and Merzlyak 2006).\n\n\n\n\n3.1 Chlorophylls: The Primary Photosynthetic Pigments\nChlorophylls are the dominant pigments in healthy, photosynthetically active vegetation, and their spectral signature forms the foundation of vegetation remote sensing (Gitelson, Gritz, and Merzlyak 2003). Figure 2 shows the characteristic double absorption pattern of chlorophylls: strong absorption centered around 430 nm in the blue region and around 680 nm in the red region, corresponding to the absorption peaks of chlorophyll a and b (Lichtenthaler 1987). Between these absorption features, green light (520-570 nm) experiences relatively less absorption, explaining why healthy vegetation appears green to our eyes.\nThe key spectral regions for chlorophyll detection are:\n\nBlue absorption (430-490 nm): Primary absorption band, but less commonly used for chlorophyll estimation due to interference from carotenoids and atmospheric scattering (Gitelson and N 1996)\nGreen peak (520-570 nm): Local reflectance maximum where chlorophyll absorption is minimal\nRed absorption (660-690 nm): Deep absorption feature, highly sensitive to chlorophyll concentration\nRed edge (690-740 nm): Steep transition from red absorption to near-infrared plateau, extremely sensitive to chlorophyll content (Horler, Dockray, and Barber 1983)\nNear-infrared plateau (750-800 nm and beyond): High reflectance due to leaf internal structure, minimally affected by pigments (Gates et al. 1965)\n\nThe reciprocal reflectance model underlying vegetation indices for chlorophyll typically uses these wavelength regions (Gitelson and Merzlyak 2006):\n\\[(1/R_{695-735} - 1/R_{750-800}) \\times R_{750-800}\\]\nwhere the near-infrared term serves to normalize for structural effects and the difference in reciprocal reflectance quantifies chlorophyll absorption.\n\n\n3.2 Carotenoids: Accessory Pigments and Photoprotection\nCarotenoids serve dual roles in plant physiology: light harvesting in the photosynthetic apparatus and photoprotection against excess light energy (Demmig-Adams and Adams III 2006). Their spectral signature differs systematically from chlorophylls. Figure 2 demonstrates that carotenoids create characteristic absorption features in:\n\nBlue region (440-520 nm): Strong absorption overlapping with chlorophyll\nBlue-green transition (500-540 nm): Primary diagnostic region where carotenoid absorption distinguishes them from chlorophylls\nGreen region (540-560 nm): Moderate absorption creating the yellow-orange appearance of carotenoid-rich tissues\nRed region (650-710 nm): Minimal absorption, contrasting with strong chlorophyll absorption\n\nThe spectral separation of carotenoids from chlorophylls becomes particularly important during senescence or stress conditions when the chlorophyll:carotenoid ratio changes dramatically (Gitelson et al. 2002). Vegetation indices designed to estimate carotenoids exploit the differential absorption at 500-520 nm and 540-560 nm relative to the near-infrared reference, following a similar reciprocal reflectance formulation:\n\\[(1/R_{500-520} - 1/R_{700-710}) \\times R_{750+}\\]\n\n\n3.3 Anthocyanins: Stress Response and Senescence Indicators\nAnthocyanins represent a distinct class of pigments that accumulate in response to various stresses—environmental, pathogenic, or developmental (Gould 2004). Unlike chlorophylls and carotenoids, which reside in chloroplasts, anthocyanins accumulate in vacuoles and can mask the spectral signatures of photosynthetic pigments (Merzlyak, Gitelson, and Chivkunova 1999).\nFigure 2 shows that anthocyanins create strong absorption in:\n\nGreen-yellow region (540-600 nm): Primary absorption feature, explaining the red-purple appearance of anthocyanin-rich tissues\nOverlap with chlorophyll green peak (520-560 nm): This overlap complicates the interpretation of reflectance in this region\n\nThe reciprocal reflectance model for anthocyanins uses (Merzlyak, Gitelson, and Chivkunova 1999):\n\\[(1/R_{540-560} - 1/R_{700-710}) \\times R_{750+}\\]\nwhere the denominator wavelength is chosen to minimize chlorophyll interference while capturing anthocyanin absorption.\n\n\n\n\n\n\nFigure 3: Seasonal variation in anthocyanin content and spectral reflectance. Spectral reflectance curves from March through October showing changes in reflectance patterns (400-750 nm) associated with varying anthocyanin concentrations (labeled in nmol/cm²). Spring months (March-April) show elevated anthocyanin in developing leaves, summer months (May-August) display minimal anthocyanin in mature leaves, and autumn months (September-October) exhibit dramatic anthocyanin accumulation during senescence. The green-yellow region (500-650 nm) shows progressive absorption with increasing anthocyanin content. Source: Gitelson et al., 2006.\n\n\n\nThe temporal dynamics of anthocyanin accumulation are particularly striking. Figure 3 displays spectral reflectance patterns across the growing season (March through October) with varying anthocyanin concentrations labeled on each panel. Key observations include:\n\nSpring accumulation (March-April): Young, developing leaves often contain elevated anthocyanins (15-35 nmol/cm²) for photoprotection during leaf expansion (Steyn et al. 2002)\nSummer reduction (May-August): Mature leaves typically maintain low anthocyanin levels (&lt;7 nmol/cm²) when plants are unstressed\nAutumn senescence (September-October): Dramatic anthocyanin accumulation (up to 24 nmol/cm²) as chlorophyll degrades and autumn coloration develops (Hoch, Zeldin, and McCown 2001)\n\nThe spectral manifestation of these changes is clearly visible in the green-yellow region (500-600 nm), where increasing anthocyanin content progressively suppresses reflectance. This seasonal pattern makes anthocyanin indices valuable for phenological monitoring and stress detection (Gitelson and N 1996)."
  },
  {
    "objectID": "topics/hyperspectral_theory.html#concentration-dependent-spectral-response",
    "href": "topics/hyperspectral_theory.html#concentration-dependent-spectral-response",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "4 Concentration-Dependent Spectral Response",
    "text": "4 Concentration-Dependent Spectral Response\nThe relationship between biochemical concentration and spectral reflectance follows predictable patterns that enable quantitative remote sensing (Jacquemoud and Baret 1990). Figure 4 provides a comprehensive demonstration of this relationship for chlorophylls and carotenoids.\n\n\n\n\n\n\nFigure 4: Concentration-dependent spectral response of chlorophylls and carotenoids. a: Pigment content (nmol/cm²) for chlorophyll (Chl, green bars) and carotenoids (Car, yellow bars) across seven groups with increasing chlorophyll dominance. b: Corresponding spectral reflectance curves (400-750 nm) for each concentration group, showing systematic changes in reflectance magnitude and spectral shape. Source: Gitelson et al., 2006.\n\n\n\nThe left panel of Figure 4 shows chlorophyll (Chl, green bars) and carotenoid (Car, yellow bars) concentrations across seven groups with increasing biochemical content and chlorophyll dominance. Group 7 contains approximately 35 nmol/cm² of chlorophyll and 10 nmol/cm² of carotenoids, representing dense, healthy foliage, while Group 1 has minimal pigment content dominated by carotenoids, which is characteristic of stressed or senescent tissue.\nThe right panel shows how these concentration differences manifest in spectral reflectance (400-750 nm). The numbered curves (1-7) correspond to the concentration groups, revealing several critical patterns:\n\nBlue region (400-500 nm): All curves show low reflectance due to combined chlorophyll and carotenoid absorption, with minimal differentiation between concentration levels.\nGreen peak (520-570 nm): Increasing pigment concentrations progressively suppress the green reflectance peak, but this region shows modest dynamic range.\nRed absorption (650-690 nm): Deep absorption trough that deepens only slightly with increasing chlorophyll concentration due to saturation effects.\nRed edge (690-750 nm): The most dramatic differentiation occurs here, where the slope and position of the red edge shift systematically with chlorophyll content (Horler, Dockray, and Barber 1983).\n\nNote the near-linear spacing of curves as pigment concentration increases in the red edge region. This high sensitivity in the red edge makes it the most valuable spectral region for chlorophyll estimation (Gitelson and Merzlyak 2006). The physical explanation involves the transition from strong chlorophyll absorption in the red to minimal absorption in the near-infrared, creating a steep gradient that shifts in both position (red edge position) and amplitude with changing chlorophyll content.\nThis concentration-reflectance relationship forms the empirical foundation for vegetation indices and regression-based retrieval algorithms (Verrelst et al. 2015). However, the relationships are not always linear due to:\n\nSaturation effects: At high pigment concentrations, absorption becomes nearly complete, reducing sensitivity (Gitelson, Gritz, and Merzlyak 2003).\nMultiple scattering: Photon path length through leaf tissue affects the absorption probability.\nPackage effects: Pigments organized in chloroplasts rather than uniformly distributed. (Jacquemoud and Baret 1990)\nStructural confounding: Leaf thickness, internal air spaces, and surface properties modulate reflectance independently of biochemistry (Asner 1998)."
  },
  {
    "objectID": "topics/hyperspectral_theory.html#phenological-stages-and-stress-detection",
    "href": "topics/hyperspectral_theory.html#phenological-stages-and-stress-detection",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "5 Phenological Stages and Stress Detection",
    "text": "5 Phenological Stages and Stress Detection\nAs we stated above, the appearance of foliage across developmental stages and stress conditions creates distinct spectral signatures that enable phenological monitoring and early stress detection (Carter 1993). Figure 5 presents a comprehensive comparison of three critical vegetation states: green (healthy) leaves, yellow-reddish (stress/senescence) leaves, and red (early development) leaves.\n\n\n\n\n\n\nFigure 5: Spectral signatures of leaves at different phenological stages and stress conditions. Top left: Green (healthy) leaves showing characteristic chlorophyll absorption in blue and red regions with prominent green peak and near-infrared plateau. Top right: Yellow-reddish leaves representing early senescence or stress conditions with elevated visible reflectance and reduced red absorption. Bottom: Red leaves indicating advanced senescence with strong anthocyanin absorption in green-yellow region, minimal chlorophyll absorption, and complete loss of red edge feature. Colored bullet points indicate developmental stage: red (beginning of vegetation), green (healthy development), and yellow-orange (stress/senescence). Source: Gitelson et al., 2006.\n\n\n\n\n5.1 Green Leaves: The Baseline Healthy Signature\nThe green leaf spectra (top left, Figure 5) exhibit the classic vegetation signature (Gates et al. 1965):\n\nLow reflectance in blue (400-500 nm) and red (600-680 nm) due to chlorophyll absorption\nModest green peak around 550 nm where chlorophyll absorption is minimal\nSharp red edge transition (690-750 nm)\nHigh near-infrared plateau (750-800 nm) determined by leaf internal structure\n\nMultiple curves represent natural variability within healthy vegetation—different species, leaf ages, or environmental conditions—but all share the fundamental pattern of strong chlorophyll absorption bracketing the green reflectance peak. This signature corresponds to the “Development (healthy)” stage marked in green on the figure, representing peak photosynthetic activity and optimal physiological function (Peñuelas et al. 1993).\n\n\n5.2 Yellow-Reddish Leaves: Stress and Senescence\nThe yellow-reddish leaf spectra (top right, Figure 5) show systematic departures from the healthy baseline:\n\nElevated reflectance across the entire visible spectrum (400-700 nm) as chlorophyll degrades\nBroadened and increased green-yellow reflectance (500-600 nm) revealing underlying carotenoids\nReduced red absorption depth as chlorophyll concentration declines\nGentler red edge slope indicating lower chlorophyll content\nSlightly reduced near-infrared reflectance in some cases, suggesting changes in leaf internal structure\n\nThese spectral changes correspond to stress or senescence conditions (marked in yellow-orange bullets on the figure) where chlorophyll synthesis is impaired or degradation accelerated (Carter 1993). During stress or senescence, chlorophyll degrades while carotenoids persist longer, creating the characteristic yellow-orange appearance.\n\n\n5.3 Red Leaves: Beginning of Vegetation\nThe red leaf spectra (bottom, Figure 5) represent young, developing leaves at the beginning of the growing season:\n\nElevated visible reflectance (400-700 nm) due to lower chlorophyll content in developing leaves\nStrong absorption in green-yellow region (520-600 nm) from anthocyanin accumulation\nModerate red absorption as chlorophyll is still developing\nLess pronounced red edge compared to mature leaves due to lower chlorophyll content\nLower near-infrared reflectance than mature leaves, reflecting developing leaf structure\n\nThis spectral signature, marked as “Beginning of vegetation” with red bullets in the figure, indicates early spring leaf development where anthocyanins accumulate for photoprotection while the photosynthetic apparatus is still maturing (Steyn et al. 2002). Young leaves often produce anthocyanins to protect developing chloroplasts from photo-oxidative damage during the vulnerable expansion phase. As leaves mature and the photosynthetic machinery becomes fully functional, anthocyanin content typically decreases and chlorophyll dominates, transitioning to the green leaf signature.\n\n\n5.4 The Phenological Continuum\nThe progression from red (early development) through green (mature, healthy) to yellow-reddish (stress/senescence) leaves represents different physiological states that can be monitored remotely (Gitelson and Merzlyak 2001). Importantly, this highlights a challenge in remote sensing: both young developing leaves and senescing leaves can show elevated anthocyanin content, but for different physiological reasons. Distinguishing between these states requires temporal context—spring observations of red spectra likely indicate new growth, while autumn observations suggest senescence. Vegetation indices designed to capture these transitions must account for the changing relationships between pigments and the multiple roles of anthocyanins across the growing season (Richardson et al. 2018)."
  },
  {
    "objectID": "topics/hyperspectral_theory.html#structural-components-beyond-pigments",
    "href": "topics/hyperspectral_theory.html#structural-components-beyond-pigments",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "6 Structural Components: Beyond Pigments",
    "text": "6 Structural Components: Beyond Pigments\nWhile pigments dominate the visible spectrum, structural and biochemical components beyond pigments create diagnostic features in the near-infrared and shortwave infrared regions (Curran 1989). Figure 6 focuses on epicuticular waxes, protective coatings on leaf surfaces that serve multiple functions including water regulation, pathogen defense, and UV protection. Similarly to chlorophyll, wax concentration is highest at the peak of the vegetation season.\n\n\n\n\n\n\nFigure 6: Impact of epicuticular wax on leaf spectral reflectance. a: Scanning electron microscopy (SEM) images showing leaf surface microstructure with intact epicuticular wax crystals (top row) and after wax removal (bottom row) at different times. b: Spectral reflectance curves (410-1710 nm) demonstrating the effect of wax removal on reflectance across visible, near-infrared, and shortwave infrared regions. Three curves show reflectance before wax removal (highest), after 10 seconds of removal (intermediate), and after 30 seconds of removal (lowest), with the greatest differences observed in the near-infrared region (750-1250 nm). Source: Kozhoridze et al. (2016)\n\n\n\nThe electron microscopy images in Figure 6 show dramatic differences in leaf surface structure before and after wax removal, revealing the crystalline wax structures that coat healthy leaf surfaces. The spectral consequences of wax removal are striking:\n\nBefore removal (top curve): Highest reflectance across the entire spectrum, particularly in the near-infrared (750-1700 nm) where wax creates a highly reflective surface\nAfter 10 seconds of removal: Intermediate reflectance as surface wax is partially dissolved\nAfter 30 seconds of removal (bottom curve): Lowest reflectance as most surface wax has been eliminated\n\nNotably, the wax removal experiment demonstrates the phenomenon of blue shift, where the reflectance spectrum shifts systematically toward shorter wavelengths as wax is removed. This blue shift occurs because epicuticular waxes preferentially scatter longer wavelengths; their removal reduces this scattering effect, causing the spectral curves to shift toward the blue end of the spectrum. This phenomenon has important implications for interpreting spectral changes in vegetation experiencing drought stress or other conditions that affect wax production or integrity.\nThe spectral regions most sensitive to wax content are:\n\nBlue-green (410-550 nm): Moderate sensitivity with elevated reflectance in waxy leaves\nNear-infrared (750-1250 nm): High sensitivity throughout this region\nShortwave infrared (1300-1700 nm): Continued sensitivity to wax content\n\nThe practical implication is that vegetation indices targeting wax content must operate in spectral regions where wax effects dominate over pigment absorption. The reciprocal of the square root of reflectance differences in specific wavelength regions provides a measure of wax content:\n\\[\\frac{1}{\\sqrt{R_{750-770} - R_{490-500} - R_{660-690}}}\\]\nBeyond waxes, other structural components create diagnostic spectral features:\n\nCellulose and lignin: Absorption bands in the shortwave infrared around 2100 nm (Curran 1989)\nLeaf water content: Strong absorption features at 970 nm, 1200 nm, 1450 nm, and 1940 nm (Peñuelas et al. 1993)\nLeaf internal structure: Determines the overall magnitude of near-infrared reflectance through multiple scattering (Jacquemoud and Baret 1990)\n\nThese structural and compositional properties interact with pigment signals, creating the full complexity of vegetation spectra that extends from 400 nm through 2500 nm in comprehensive hyperspectral datasets (Ustin and Gamon 2010)."
  },
  {
    "objectID": "topics/hyperspectral_theory.html#from-spectral-signatures-to-quantitative-estimation",
    "href": "topics/hyperspectral_theory.html#from-spectral-signatures-to-quantitative-estimation",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "7 From Spectral Signatures to Quantitative Estimation",
    "text": "7 From Spectral Signatures to Quantitative Estimation\nThe theoretical framework presented here—linking specific biochemical compounds to characteristic spectral features—forms the foundation for quantitative remote sensing of vegetation functional traits (Homolová et al. 2013). The key principles that enable this translation from photons to plant physiology are:\n\nSpectral specificity: Each compound has characteristic absorption features at particular wavelengths, enabling selective detection (Gates et al. 1965)\nConcentration dependence: Reflectance changes systematically with biochemical concentration, allowing quantitative estimation (Jacquemoud and Baret 1990)\nTemporal dynamics: Seasonal patterns in biochemistry create predictable spectral trajectories (Richardson et al. 2018)\nStructural modulation: Leaf and canopy architecture affect spectral signals independently of biochemistry, requiring normalization strategies (Asner 1998)\n\nThe practical implementation of these principles through vegetation indices and statistical models—demonstrated in the following tutorial sections—provides operational tools for extracting biological information from hyperspectral data (Verrelst et al. 2015). By understanding the physical and physiological basis of these spectral-biochemical relationships, researchers can develop robust methods for monitoring plant functional traits across scales, from controlled laboratory measurements to satellite-based global vegetation monitoring (Ustin and Gamon 2010).\n\nThis theoretical foundation prepares you to interpret the analytical methods demonstrated in the practical tutorial that follows, where we transform raw spectral measurements into quantitative estimates of plant functional traits through statistical analysis and machine learning classification."
  },
  {
    "objectID": "topics/hyperspectral_theory.html#references",
    "href": "topics/hyperspectral_theory.html#references",
    "title": "Theoretical Background: Hyperspectral Remote Sensing of Plant Functional Traits",
    "section": "8 References",
    "text": "8 References\n\n\nAsner, Gregory P. 1998. “Biophysical and Biochemical Sources of Variability in Canopy Reflectance.” Remote Sensing of Environment 64 (3): 234–53.\n\n\nAsner, Gregory P, and Roberta E Martin. 2015. “Quantifying Forest Canopy Traits: Imaging Spectroscopy Versus Field Survey.” Remote Sensing of Environment 158: 15–27.\n\n\nCarter, Gregory A. 1993. “Responses of Leaf Spectral Reflectance to Plant Stress.” American Journal of Botany 81 (2): 239–43.\n\n\nCurran, Paul J. 1989. “Remote Sensing of Foliar Chemistry.” Remote Sensing of Environment 30 (3): 271–78.\n\n\nDemmig-Adams, Barbara, and William W Adams III. 2006. “Photoprotection in an Ecological Context: The Remarkable Complexity of Thermal Energy Dissipation.” New Phytologist 172 (1): 11–21.\n\n\nGates, David M, Harry J Keegan, John C Schleter, and Victor R Weidner. 1965. “Spectral Properties of Plants.” Applied Optics 4 (1): 11–20.\n\n\nGitelson, Anatoly A, Yuri Gritz, and Mark N Merzlyak. 2003. “Relationships Between Leaf Chlorophyll Content and Spectral Reflectance and Algorithms for Non-Destructive Chlorophyll Assessment in Higher Plant Leaves.” Journal of Plant Physiology 160 (3): 271–82.\n\n\nGitelson, Anatoly A, Yoram J Kaufman, Robert Stark, and Don Rundquist. 2002. “Novel Algorithms for Remote Estimation of Vegetation Fraction.” Remote Sensing of Environment 80 (1): 76–87.\n\n\nGitelson, Anatoly A, Galina P Keydan, and Mark N Merzlyak. 2006. “Three-Band Model for Noninvasive Estimation of Chlorophyll, Carotenoids, and Anthocyanin Contents in Higher Plant Leaves.” Geophysical Research Letters 33 (11).\n\n\nGitelson, Anatoly A, and Mark N Merzlyak. 2001. “Non-Destructive Assessment of Chlorophyll Carotenoid and Anthocyanin Content in Higher Plant Leaves: Principles and Algorithms.” Remote Sensing for Agriculture and the Environment 1: 78–94.\n\n\n———. 2006. “Remote Estimation of Chlorophyll Content in Higher Plant Leaves.” International Journal of Remote Sensing 27 (12): 2055–66.\n\n\nGitelson, Anatoly A, and Merzylak Mark N. 1996. “Signature Analysis of Leaf Reflectance Spectra: Algorithm Development for Remote Sensing of Chlorophyll.” Journal of Plant Physiology 163 (3): 1176–85.\n\n\nGould, Kevin S. 2004. “Nature’s Swiss Army Knife: The Diverse Protective Roles of Anthocyanins in Leaves.” Journal of Biomedicine and Biotechnology 2004 (5): 314–20.\n\n\nHoch, William A, Erika L Zeldin, and Brent H McCown. 2001. “Physiological Significance of Anthocyanins During Autumnal Leaf Senescence.” Tree Physiology 21 (1): 1–8.\n\n\nHomolová, Lucie, Zbyněk Malenovský, JGPW Clevers, Glenda Garcı́a-Santos, and Michael E Schaepman. 2013. “Review of Optical-Based Remote Sensing for Plant Trait Mapping.” Ecological Complexity 15: 1–16.\n\n\nHorler, DNH, M Dockray, and J Barber. 1983. “The Red Edge of Plant Leaf Reflectance.” International Journal of Remote Sensing 4 (2): 273–88.\n\n\nHuete, Alfredo, Kamel Didan, Tomoaki Miura, E Patricia Rodriguez, Xiang Gao, and Laerte G Ferreira. 2002. “Overview of the Radiometric and Biophysical Performance of the MODIS Vegetation Indices.” Remote Sensing of Environment 83 (1-2): 195–213.\n\n\nJacquemoud, Stéphane, and Frédéric Baret. 1990. “PROSPECT: A Model of Leaf Optical Properties Spectra.” Remote Sensing of Environment 34 (2): 75–91.\n\n\nKozhoridze, Giorgi, Nikolai Orlovsky, Leah Orlovsky, Dan G Blumberg, and Avi Golan-Goldhirsh. 2016. “Remote Sensing Models of Structure-Related Biochemicals and Pigments for Classification of Trees.” Remote Sensing of Environment 186: 184–95.\n\n\nLichtenthaler, Hartmut K. 1987. “Chlorophylls and Carotenoids: Pigments of Photosynthetic Biomembranes.” Methods in Enzymology 148: 350–82.\n\n\nMerzlyak, Mark N, Anatoly A Gitelson, and Olga B Chivkunova. 1999. “Non-Destructive Optical Detection of Pigment Changes During Leaf Senescence and Fruit Ripening.” Physiologia Plantarum 106 (1): 135–41.\n\n\nPeñuelas, Josep, Iolanda Filella, Corina Biel, Lluis Serrano, and Robert Save. 1993. “The Reflectance at the 950–970 Nm Region as an Indicator of Plant Water Status.” International Journal of Remote Sensing 14 (10): 1887–1905.\n\n\nRichardson, Andrew D, Mariah S Carbone, Trevor F Keenan, Claudia I Czimczik, David Y Hollinger, Patty Murakami, Paul G Schaberg, and Xiaomei Xu. 2013. “Seasonal Dynamics and Age of Stemwood Nonstructural Carbohydrates in Temperate Forest Trees.” New Phytologist 197 (3): 850–61.\n\n\nRichardson, Andrew D, Koen Hufkens, Tom Milliman, Donald M Aubrecht, Min Chen, Josh M Gray, Miriam R Johnston, et al. 2018. “Tracking Vegetation Phenology Across Diverse North American Biomes Using PhenoCam Imagery.” Scientific Data 5 (1): 1–24.\n\n\nSteyn, Wibke JA, SJE Wand, DM Holcroft, and Gerhard Jacobs. 2002. “Anthocyanins in Vegetative Tissues: A Proposed Unified Function in Photoprotection.” New Phytologist 155 (3): 349–61.\n\n\nUstin, Susan L, and John A Gamon. 2010. “Remote Sensing of Plant Functional Types.” New Phytologist 186 (4): 795–816.\n\n\nVerrelst, J, Z Malenovský, C Van der Tol, G Camps-Valls, JP Gastellu-Etchegorry, P Lewis, P North, and J Moreno. 2015. “Optical Remote Sensing and the Retrieval of Terrestrial Vegetation Bio-Geophysical Properties–a Review.” ISPRS Journal of Photogrammetry and Remote Sensing 108: 273–90."
  },
  {
    "objectID": "topics/DART_tutorial.html",
    "href": "topics/DART_tutorial.html",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "",
    "text": "This tutorial provides a step-by-step guide for setting up a forest scene simulation in DART (Discrete Anisotropic Radiative Transfer). DART is a comprehensive 3D radiative transfer model designed for simulating radiative budget and remote sensing acquisitions of natural and urban landscapes. This tutorial focuses on creating a high-resolution forest scene with detailed 3D tree models and proper optical properties."
  },
  {
    "objectID": "topics/DART_tutorial.html#overview",
    "href": "topics/DART_tutorial.html#overview",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "",
    "text": "This tutorial provides a step-by-step guide for setting up a forest scene simulation in DART (Discrete Anisotropic Radiative Transfer). DART is a comprehensive 3D radiative transfer model designed for simulating radiative budget and remote sensing acquisitions of natural and urban landscapes. This tutorial focuses on creating a high-resolution forest scene with detailed 3D tree models and proper optical properties."
  },
  {
    "objectID": "topics/DART_tutorial.html#prerequisites",
    "href": "topics/DART_tutorial.html#prerequisites",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nBefore starting this tutorial, ensure you have:\n\nDART software installed (Version 5.10.5 or later)\nThe DART_forest_scene.zip file containing all necessary input files\nBasic understanding of radiative transfer modeling\nSufficient computational resources (12-core processor recommended)\n\n\n\n\n\n\n\nNoteAbout DART\n\n\n\nDART is developed by the CESBIO laboratory (Centre d’Etudes Spatiales de la BIOsphère) and provides both forward and inverse modeling capabilities for optical remote sensing. For more information, visit the DART manual."
  },
  {
    "objectID": "topics/DART_tutorial.html#part-1-initial-setup",
    "href": "topics/DART_tutorial.html#part-1-initial-setup",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "3 Part 1: Initial Setup",
    "text": "3 Part 1: Initial Setup\n\n3.1 Extracting and Organizing Files\nThe first step is to properly organize the DART project files:\n\n\n\n\n\n\nImportantFile Organization\n\n\n\nExtract the DART_forest_scene.zip file and copy all contents except the DART_directory/user_data/simulations/forest_scene/input folder to your DART installation directory. The structure should look like:\n\nDART/database ← Copy database files here\nDART/user_data ← Copy other files here (except input folder)\n\nThis ensures all necessary optical properties, 3D models, and configuration files are in the correct locations.\n\n\n\n\n3.2 Creating a New Simulation\nTo begin your forest scene simulation:\n\nOpen DART\nClick on Simulation → New Simulation\nEnter your simulation name (e.g., “forest_scene”)\nClick “New”\n\n\n\n\n\n\n\nFigure 1: DART main menu showing the Simulation options and New Simulation dialog\n\n\n\nAs shown in Figure 1, the dialog allows you to specify the simulation name and location within your simulations folder.\n\n\n\n\n\n\nTipSimulation Management Tips\n\n\n\n\nKeep all simulations in the default simulations folder for easier management\nYou can create folder structures within the simulations directory\nUse “Choose Simulations…” if you have an existing simulation to import\nUse descriptive names for your simulations for easy identification\n\n\n\n\n\n3.3 Opening the Editor\nAfter creating your simulation:\n\nClick on Parameters → Editor\nThe DART Simulation Editor window will open\nYou’re now ready to configure your simulation parameters\n\n\n\n\n\n\n\nFigure 2: Opening the DART Editor from Parameters menu\n\n\n\nThe Editor interface (Figure 2) provides access to all simulation parameters through an organized tree structure on the left side of the window."
  },
  {
    "objectID": "topics/DART_tutorial.html#part-2-core-simulation-parameters",
    "href": "topics/DART_tutorial.html#part-2-core-simulation-parameters",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "4 Part 2: Core Simulation Parameters",
    "text": "4 Part 2: Core Simulation Parameters\n\n4.1 Basic Configuration\nThe first configuration screen allows you to set fundamental simulation parameters:\n\n\n\n\n\n\nFigure 3: DART Editor showing basic configuration parameters including atmosphere settings, advanced mode, and thread configuration\n\n\n\n\n\n\n\n\n\nImportantCritical Settings\n\n\n\nAtmosphere Simulation: - Enable atmosphere simulation even if you only need “bottom of atmosphere” products - This ensures more accurate radiative transfer calculations\nThreading Configuration: - Switch to Advanced Mode - Set Number of threads in DART tracking to 12 - This optimizes performance on multi-core systems\nRadiative Transfer Mode: - For high spatial resolution forest scenes, use bi-directional Light propagation mode (DART Lux) - This provides the most accurate and computationally efficient results for detailed 3D scenes\nThese settings are highlighted in Figure 3.\n\n\n\n\n\n\n\n\nNotePython Integration\n\n\n\nThe DART team provides Python scripts that can be implemented directly in DART or used for data preparation. These scripts offer additional automation capabilities. Refer to the DART manual for more details on Python integration.\n\n\n\n\n4.2 Spectral Band Configuration\nConfigure the spectral bands for your simulation:\n\nRight-click on “Spectral intervals”\nSelect Add… → Spectral band\nAdd four spectral bands with the following specifications:\n\n\n\n\nBand Number\nMode\nCentral Wavelength (μm)\nBandwidth (μm)\n\n\n\n\n0\nMode R\n0.56\n0.032\n\n\n1\nMode R\n0.65\n0.032\n\n\n2\nMode R\n0.73\n0.032\n\n\n3\nMode R\n0.85\n0.052\n\n\n\n\n\n\n\n\n\nFigure 4: Configuration dialogs for four spectral bands showing Mode, Central wavelength, and Spectral bandwidth parameters\n\n\n\nFigure 4 shows the configuration windows for all four spectral bands. Each band is set to Mode R (reflectance) with specific central wavelengths and bandwidths appropriate for vegetation monitoring.\n\n\n\n\n\n\nTipWorking with Multiple Bands\n\n\n\n\nFor larger numbers of spectral bands, use “Add range of…” method\nIf bands have varying bandwidths, directly modify the $your_simulation_name/input/phase.xml file\nThis approach is more efficient for hyperspectral simulations"
  },
  {
    "objectID": "topics/DART_tutorial.html#part-3-scene-configuration",
    "href": "topics/DART_tutorial.html#part-3-scene-configuration",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "5 Part 3: Scene Configuration",
    "text": "5 Part 3: Scene Configuration\n\n5.1 Bi-directional Parameters\nConfigure the image rendering and computation parameters:\n\n\n\n\n\n\nFigure 5: Bi-directional parameters panel showing pixel size, rendering settings, and radiative budget options\n\n\n\n\n\n\n\n\n\nImportantKey Parameters\n\n\n\nImage Resolution: - Set Target pixel size [m] to 0.05 (5 cm) - This provides high-resolution output suitable for detailed analysis\nOptimized Parameters: These values are results of optimization and consultation with the DART team: - Target sample density per pixel (images): 50 - Maximal rendering time per image: 2000 - Number of repetitions of the user-defined scene: 1\nRadiation Settings: - Sampler: Sobol - Russian Roulette Acceleration: Enable (checked)\nPeriodic Save: - Periodic save method: No periodic save\nAll these parameters are shown in Figure 5 with their recommended values highlighted.\n\n\n\n\n\n\n\n\nNoteParameter Optimization\n\n\n\nWhen starting a new simulation type, it’s recommended to verify these parameters for your specific: - Scene type (forest, urban, agricultural) - Spatial resolution requirements - Spectral resolution needs\nParameter optimization can significantly affect both accuracy and computation time.\n\n\n\n\n5.2 Output Products Configuration\nConfigure which products DART will generate:\n\nNavigate to Products → BRF/BTF\nEnable “Write BRF/BTF files and maps”\nSet Type of color/model of Earth scene element: Reflectance/Temperature\nConfigure Images - stored per band (folder):\n\nFormat: Ilwis (.mpr) and netcdf (.nc)\nMaximum VZA for storing images: 1.0\n\nDisable Sensor plane images\n\n\n\n\n\n\n\nFigure 6: Products configuration panel showing BRF/BTF settings and image format options\n\n\n\nAs shown in Figure 6, only the necessary outputs are enabled to minimize computation time and storage requirements.\n\n\n\n\n\n\nTipOutput Format\n\n\n\n\nDART is transitioning to netcdf as the primary output format\nIlwis format is included here for compatibility with this tutorial\nAdditional products can be enabled but will increase computation time and storage requirements\nOnly enable the products you actually need for your analysis"
  },
  {
    "objectID": "topics/DART_tutorial.html#part-4-optical-properties",
    "href": "topics/DART_tutorial.html#part-4-optical-properties",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "6 Part 4: Optical Properties",
    "text": "6 Part 4: Optical Properties\n\n6.1 Leaf Optical Properties\nDefine the spectral properties of leaves using the PROSPECT model:\n\nRight-click on “Lambertian” section\nSelect Add → Lambertian (or modify the default)\nConfigure the following:\n\nBasic Settings: - Lambertian property name: “leaves” - Disable “Multiplicative factor for database” - Enable “Reflectance, transmittance” in the Prospect section\nPROSPECT Parameters: Use the PROSPECT leaf radiative transfer model with these default biochemical parameters: - Structure coefficient: 1.5 - Chlorophyll: 40 - Carotenoid: 10 - Brown pigment: 0.0 - Equivalent water thickness: 0.012 - Anthocyanin: 0 - Dry matter content: Dry matter content\n\n\n\n\n\n\nFigure 7: Lambertian optical properties configuration for leaves using PROSPECT model with biochemical parameters\n\n\n\nFigure 7 demonstrates the complete PROSPECT configuration for leaf optical properties, including all biochemical parameters that drive the spectral response.\n\n\n\n\n\n\nNotePROSPECT Model\n\n\n\nThe PROSPECT model is a leaf-level radiative transfer model that simulates leaf optical properties based on biochemical composition. These parameters will be used for: - Look-Up Table (LUT) generation - Parameter retrieval from remote sensing images - Sensitivity analysis\nThe multiplicative factor option allows scaling of optical properties by a constant value when needed.\n\n\n\n\n6.2 Bark Optical Properties\nDefine spectral properties for tree bark:\n\nRight-click on “Lambertian”\nSelect Add → Lambertian\nConfigure:\n\nDatabase Selection: - Lambertian property name: “bark” - Disable “Multiplicative factor for database” - 2D lambertian database: Select lanzhot_materials.db - 2D lambertian model: Select trunk_FA_SY_QE_sp_Lanzhot_fix\n\n\n\n\n\n\nFigure 8: Lambertian optical properties configuration for bark using measured spectral data from lanzhot_materials database\n\n\n\nThe bark optical properties (Figure 8) use measured spectral reflectance data from the Lanzhot forest site, providing realistic bark reflectance characteristics.\n\n\n\n\n\n\nTipOptical Property Databases\n\n\n\n\nThe lanzhot_materials.db file should be in your DART/database folder (from the extracted zip)\nYou can also use optical properties from DART’s default Lambertian_vegetation.db\nUse DART Database Manager (Tools menu) to create or modify optical property databases\nMeasured optical properties provide more realistic simulations than generic values\n\n\n\n\n\n6.3 Ground Optical Properties\nDefine the forest floor spectral properties:\n\nRight-click on the bark optical properties\nSelect Duplicate\nModify:\n\nLambertian property name: “ground”\n2D lambertian model: Select forestfloor_LZ_mod_fix_txt\n\n\n\n\n\n\n\n\nFigure 9: Lambertian optical properties configuration for ground/forest floor using forestfloor spectral data\n\n\n\nThis creates optical properties representing the forest understory and leaf litter, as shown in Figure 9."
  },
  {
    "objectID": "topics/DART_tutorial.html#part-5-earth-scene-configuration",
    "href": "topics/DART_tutorial.html#part-5-earth-scene-configuration",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "7 Part 5: Earth Scene Configuration",
    "text": "7 Part 5: Earth Scene Configuration\n\n7.1 Scene Dimensions and Location\nSet up the basic scene parameters:\nScene Dimensions: - X max: 30 m - Y max: 30 m\nGround Properties: - Optical property name: “ground”\nGeographic Location: Set coordinates for central Czech Republic: - Altitude of the DEM zero level: 500 m - Latitude: [50°; 30’] - Longitude: [17°; 50’]\n\n\n\n\n\n\nFigure 10: Earth Scene configuration panel showing scene dimensions, ground optical properties, and geographic location\n\n\n\nFigure 10 displays the complete Earth Scene configuration, including the scene extent, ground properties, and geographic coordinates that define the simulation environment.\n\n\n\n\n\n\nNoteScene Configuration Options\n\n\n\nExactly Periodic Scene: - Isolated scene: Light crossing edges disappears (boundary effects) - Repetitive scene: Scene is virtually replicated around the edges (reduces boundary effects) - Infinite slope: Appropriate for sloped terrain\nCell Dimensions: - These parameters are irrelevant for bi-directional mode - In other modes, they correspond to simulated image spatial resolution"
  },
  {
    "objectID": "topics/DART_tutorial.html#part-6-3d-scene-objects",
    "href": "topics/DART_tutorial.html#part-6-3d-scene-objects",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "8 Part 6: 3D Scene Objects",
    "text": "8 Part 6: 3D Scene Objects\n\n8.1 Importing Tree Field Data\nAdd the tree locations and specifications:\n\nRight-click on “Object fields”\nSelect Add → Field\nField description file: Select beech.txt (from the extracted zip)\nName: “Field”\nEnable “Show this field’s objects in the 2D view”\n\n\n\n\n\n\n\nFigure 11: Field import dialog showing the beech.txt file selection and configuration options\n\n\n\nFigure 11 shows the field import interface where you specify the text file containing tree positions and properties.\n\n\n\n\n\n\nNoteField File Format\n\n\n\nThe beech.txt file contains information for each tree: - Species/object type identifier - XY position coordinates - Scaling factor (tree size) - Rotation angles\nThis format allows efficient specification of multiple objects without manually placing each one. Refer to the DART manual for detailed file format specifications.\n\n\n\n\n8.2 Importing 3D Tree Models\nImport the first tree model:\n\nRight-click on “3D Models”\nSelect Import file\nChoose the first 3D object file: beech_20 (from extracted files)\n\nConfiguration: - Name: “beech_20” - Color: Select a distinctive color (e.g., orange) - Keep other options at default\n\n\n\n\n\n\nFigure 12: 3D model import interface showing file selection, naming options, and display settings for the beech_20 tree model\n\n\n\nThe 3D model import dialog (Figure 12) allows you to specify the model file, assign a name, and configure display properties including color coding for easy identification in the 2D view.\n\n\n\n\n\n\nTip3D Object Creation\n\n\n\nDART includes a “Creation of 3D objects” tool for generating simple geometric crown shapes filled with leaf facets. While less detailed than full 3D scanned models, this can be sufficient for many applications and is computationally efficient.\n\n\n\n\n8.3 Configuring Leaf Groups\nFor the leaves group within the 3D object:\n\nExpand the tree model in the hierarchy\nSelect the leaves group\nConfigure:\n\nGroup Settings: - Name: “leaves” - Enable “Double face” ✓\nOptical Properties: - Type of optical property: Lambertian - Optical property name: “leaves”\nDisplay: - Group’s color: Leaf (green)\n\n\n\n\n\n\nFigure 13: Leaves group configuration showing double face option, optical property assignment, and the 2D view with colored tree crowns\n\n\n\nFigure 13 demonstrates the leaves group configuration within the 3D object hierarchy, with the 2D view showing the spatial distribution of trees in the scene.\n\n\n\n\n\n\nImportantDouble Face Option\n\n\n\nThe “Double face” option is critical for leaves. It ensures: - Both sides of leaf facets have optical properties - Proper simulation of leaf transmittance - Accurate light scattering within the canopy\nWithout this option, the back face of leaves would be transparent, leading to incorrect radiative transfer calculations.\n\n\n\n\n\n\n\n\nNoteTurbid Medium Alternative\n\n\n\nThe “Used to create turbid or fluid volume” option converts 3D objects into a turbid medium representation. This is useful for: - Easily changing LAI values without multiple 3D models - Computational efficiency in some cases\nHowever, it reduces spatial accuracy and makes bi-directional mode less efficient. Use only when appropriate for your application.\n\n\n\n\n8.4 Configuring Woody Parts\nFor the trunk and branches group:\n\nSelect the wooden parts group\nConfigure:\n\nGroup Settings: - Name: “wooden_parts” - Enable “Double face” ✓ (recommended)\nOptical Properties: - Type of optical property: Lambertian - Optical property name: “bark”\nDisplay: - Group’s color: Trunk (brown)\n\n\n\n\n\n\nFigure 14: Woody parts group configuration showing optical property assignment to bark and the 2D scene view\n\n\n\nThe woody parts configuration (Figure 14) assigns bark optical properties to trunk and branch components of the tree model.\n\n\n\n\n\n\nTipDouble Face for Woody Parts\n\n\n\nFor trunks and branches with no transmittance, double face is technically unnecessary if all facet normals are correctly oriented. However, enabling it is safer unless you’re certain about facet orientation, as it prevents rendering artifacts.\n\n\n\n\n8.5 Adding All Tree Models\nRepeat the import and configuration process for all remaining 3D tree models:\n\nbeech_27\nbeech_28\nbeech_62\n\nFor each model: 1. Import the 3D file 2. Set a unique name 3. Assign a distinctive color for visualization 4. Configure leaf groups with “leaves” optical properties 5. Configure woody groups with “bark” optical properties 6. Enable double face for both groups\n\n\n\n\n\n\nFigure 15: Complete 2D view of the forest scene showing all imported tree models as colored circles representing different tree types and sizes\n\n\n\n\n\n\n\n\n\nNoteVisualization\n\n\n\nWhen all models are imported and configured, the 2D editor view (Figure 15) displays colored circles representing tree crowns at their specified locations. Different colors help distinguish between different tree models or size classes. The spatial distribution shown here matches the specifications in the beech.txt field file."
  },
  {
    "objectID": "topics/DART_tutorial.html#part-7-atmospheric-configuration-optional",
    "href": "topics/DART_tutorial.html#part-7-atmospheric-configuration-optional",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "9 Part 7: Atmospheric Configuration (Optional)",
    "text": "9 Part 7: Atmospheric Configuration (Optional)\nThe atmosphere section provides detailed control over atmospheric radiative transfer:\n\n\n\n\n\n\nFigure 16: Atmosphere configuration panel showing atmospheric database selection, gas profiles, aerosol properties, and advanced mode settings"
  },
  {
    "objectID": "topics/DART_tutorial.html#atmospheric-parameters",
    "href": "topics/DART_tutorial.html#atmospheric-parameters",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "10 Atmospheric Parameters",
    "text": "10 Atmospheric Parameters\nIf you have measured or observed atmospheric data, you can configure:\nAtmosphere Database: - Select appropriate atmospheric model (e.g., BIRND_JULY2020.db)\nGas Composition: - Temperature profile (US standard models available) - Ozone and other gas vertical profiles - CO2 mixing ratio - Water vapor content\nAerosol Properties: - Set gas amounts (H2O, CO2) - Configure aerosol optical depth - Define atmospheric components (Downward fluxes, Upward fluxes)\nAdvanced Settings: - Atmosphere extrapolation parameters - Scattering threshold - Maximum scattering iteration number - Atmosphere geometry (discretization, sensor layer altitude)\nThese parameters (Figure 16) significantly affect top-of-atmosphere and atmospheric correction simulations."
  },
  {
    "objectID": "topics/DART_tutorial.html#part-8-running-the-simulation",
    "href": "topics/DART_tutorial.html#part-8-running-the-simulation",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "11 Part 8: Running the Simulation",
    "text": "11 Part 8: Running the Simulation\n\n11.1 Saving and Validation\nBefore running:\n\nClick the Save button in the Editor\nVerify no error messages appear\nReview the 2D scene view to confirm proper placement\nClose the Editor\n\n\n\n11.2 Running DART Modules\nExecute the simulation:\n\nIn the main DART window, click Run → DART\nThis runs all four modules in sequence:\n\nDirection: Calculates sun and viewing geometry\nPhase: Processes spectral and optical properties\nMaket: Constructs the 3D scene\nDart: Performs radiative transfer calculations\n\n\n\n\n\n\n\n\nFigure 17: DART Run menu showing all available modules including Direction, Phase, Maket, Dart, and auxiliary tools\n\n\n\nFigure 17 displays the Run menu with all DART modules. The execution order is important, as each module depends on the outputs of previous modules.\n\n\n\n\n\n\nImportantModule Dependencies\n\n\n\nFirst Run: - All modules must be executed\nSubsequent Runs: - Only run modules affected by parameter changes - Module dependencies: - Direction: Affects sun/sensor geometry - Phase: Affects spectral bands and optical properties - Maket: Affects scene structure and objects - Dart: Affected by all previous modules\nThis selective execution saves considerable computation time during parameter optimization.\n\n\n\n\n\n\n\n\nTipComputation Time\n\n\n\nExpected computation times vary based on: - Number of spectral bands (currently 4) - Scene complexity (30m × 30m with multiple trees) - Spatial resolution (5 cm pixels) - Number of threads (12 recommended)\nFor this configuration, expect 30-60 minutes on a modern 12-core system."
  },
  {
    "objectID": "topics/DART_tutorial.html#part-9-sequence-launcher-for-lut-generation",
    "href": "topics/DART_tutorial.html#part-9-sequence-launcher-for-lut-generation",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "12 Part 9: Sequence Launcher for LUT Generation",
    "text": "12 Part 9: Sequence Launcher for LUT Generation\nThe Sequence Launcher is a powerful tool for generating Look-Up Tables (LUTs) by systematically varying parameters:\n\n12.1 Setting Up a Sequence\n\nClick Run → SequenceLauncher\nClick “Create sequence”\nClick “Add”\nSelect parameters to vary:\n\nNavigate through the parameter tree\nSelect optical properties or structural parameters\nDefine the range of values\n\n\n\n\n\n\n\n\nFigure 18: Sequence Launcher interface showing (left) execution progress, (center) parameter selection tree, and (right) process configuration options\n\n\n\nFigure 18 shows the complete Sequence Launcher workflow, including the parameter selection dialog, the properties group details window, and the main processes configuration panel where you can specify which DART modules to run and how many simulations to execute in parallel.\n\n\n12.2 Configuration Options\nParameter Selection: Choose which parameters to vary, such as: - PROSPECT biochemical parameters (Chlorophyll, LAI, etc.) - Structural parameters (tree dimensions, densities) - Atmospheric conditions - Sun angles\nProcess Configuration: In Preferences → Process and threads to run: - Select which modules to run (based on changed parameters) - Set number of parallel simulations - Each simulation uses the specified number of threads\n\n\n\n\n\n\nTipParallel Processing\n\n\n\nExample configuration for 12-core system: - Threads per simulation: 3 - Parallel simulations: 4 - Total thread usage: 12\nBalance parallel simulations and threads per simulation based on your system’s capabilities and memory.\n\n\n\n\n12.3 Running the Sequence\n\nClick “Prepare and run sequence”\nMonitor progress in the console window\nResults will be saved in numbered subdirectories\n\n\n\n\n\n\n\nNoteLUT Applications\n\n\n\nGenerated LUTs are valuable for: - Parameter retrieval from remote sensing data - Sensitivity analysis - Model inversion - Uncertainty quantification - Training machine learning models\nThe systematic parameter variation creates a database relating scene parameters to simulated reflectance."
  },
  {
    "objectID": "topics/DART_tutorial.html#summary-and-best-practices",
    "href": "topics/DART_tutorial.html#summary-and-best-practices",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "13 Summary and Best Practices",
    "text": "13 Summary and Best Practices\n\nScene SetupOptical PropertiesComputational EfficiencyQuality Control\n\n\nEssential Steps: - Proper file organization in DART directories - Careful spectral band configuration - Appropriate spatial resolution for application - Realistic optical properties from measurements\nCommon Mistakes to Avoid: - Forgetting to enable double face for leaves - Incorrect spectral bandwidth settings - Insufficient spatial resolution - Isolated scene boundaries causing edge effects\n\n\nBest Practices: - Use measured optical properties when available - Validate PROSPECT parameters against lab measurements - Consider seasonal variations in leaf properties - Document all optical property sources\nConsiderations: - Bark properties vary by species and condition - Understory optical properties affect overall scene reflectance - Leaf age and health influence biochemical parameters\n\n\nOptimization Strategies: - Use appropriate thread count for your system - Enable Russian Roulette acceleration - Optimize rendering time parameters - Only generate needed output products - Use periodic scene boundaries to reduce edge effects\nResource Requirements: - RAM: 16-32 GB recommended for large scenes - Storage: Plan for large output files (GB per simulation) - CPU: Multi-core processor essential for reasonable run times\n\n\nValidation Steps: - Visual inspection of 2D scene layout - Check for error messages in Editor - Verify spectral signatures are realistic - Compare with field measurements if available - Test with simplified scenes first\nTroubleshooting: - Save frequently during setup - Test with single spectral band initially - Use smaller scene for parameter testing - Check DART manual for error messages"
  },
  {
    "objectID": "topics/DART_tutorial.html#advanced-topics",
    "href": "topics/DART_tutorial.html#advanced-topics",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "14 Advanced Topics",
    "text": "14 Advanced Topics\n\n14.1 Custom 3D Objects\nCreating your own tree models:\n\n\n\n\n\n\nTip3D Object Requirements\n\n\n\nDART accepts 3D objects in several formats: - OBJ (most common) - PLY - FBX (with conversion)\nRequirements: - Properly defined groups (leaves, trunk, branches) - Correct facet normals - Reasonable polygon count (balance detail vs. computation) - Separate materials for different components\n\n\n\n\n14.2 Python Integration\nDART supports Python scripting for:\n\nAutomated parameter sweeps\nCustom scene generation\nPost-processing workflows\nIntegration with other tools\n\nSee the DART manual for Python API documentation.\n\n\n14.3 Atmosphere Modeling\nFor detailed atmospheric studies:\n\nUse measured atmospheric profiles when available\nConsider aerosol optical depth variations\nAccount for water vapor absorption\nValidate against AERONET data\n\n\n\n14.4 Validation Approaches\nRecommended validation methods:\n\nLab measurements: Compare PROSPECT outputs to spectroradiometer data\nField campaigns: Validate scene reflectance with handheld or UAV spectrometers\nCross-comparison: Compare with other radiative transfer models (PROSAIL, librat)\nSensitivity analysis: Use Sequence Launcher to assess parameter impacts"
  },
  {
    "objectID": "topics/DART_tutorial.html#additional-resources",
    "href": "topics/DART_tutorial.html#additional-resources",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "15 Additional Resources",
    "text": "15 Additional Resources\n\n\n\n\n\n\nNoteLearning Resources\n\n\n\nDART Documentation: - Official DART Website - User manual (included with installation) - Tutorial collection - Forum for user questions\nScientific Background: - Gastellu-Etchegorry et al. publications on DART methodology - Remote sensing textbooks on radiative transfer - PROSPECT model documentation\nCommunity: - DART user mailing list - Annual DART training workshops - CESBIO laboratory contacts\n\n\n\n\n\n\n\n\nWarningImportant Considerations\n\n\n\nModel Limitations: - DART is computationally intensive for large scenes - Accuracy depends on input data quality - Simplified atmospheric models may not capture all effects - 3D object complexity affects computation time\nData Requirements: - Accurate 3D scene geometry - Validated optical properties - Appropriate atmospheric parameters - Sufficient computational resources\nApplication Scope: DART is suitable for: ✓ Forest and vegetation studies ✓ Urban remote sensing ✓ Precision agriculture ✓ Sensor design and testing\nMay be excessive for: ✗ Simple BRDF calculations (use simpler models) ✗ Very large spatial extents (consider other approaches) ✗ Real-time applications (computation time prohibitive)"
  },
  {
    "objectID": "topics/DART_tutorial.html#conclusion",
    "href": "topics/DART_tutorial.html#conclusion",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "16 Conclusion",
    "text": "16 Conclusion\nThis tutorial has walked through the complete workflow for setting up a forest scene simulation in DART. The combination of detailed 3D objects, measured optical properties, and physically-based radiative transfer provides a powerful tool for remote sensing research and application development.\nKey takeaways:\n\nProper setup is crucial: Careful attention to file organization, optical properties, and scene configuration ensures successful simulations\nBi-directional mode is optimal: For high-resolution 3D scenes, DART Lux provides the best balance of accuracy and efficiency\nValidation is essential: Compare simulations with field measurements whenever possible\nSequence Launcher enables systematic analysis: Use this tool for parameter studies and LUT generation\nComputational resources matter: Plan for significant computation time and storage requirements\n\nThe skills developed in this tutorial provide a foundation for more advanced DART applications, including multi-temporal studies, sensor simulation, and parameter retrieval from remote sensing data.\n\nThis tutorial is based on DART version 5.10.5. While the core concepts remain consistent across versions, specific interface details may vary. Always refer to the official DART documentation for the most current information."
  },
  {
    "objectID": "topics/DART_tutorial.html#appendix-common-issues-and-solutions",
    "href": "topics/DART_tutorial.html#appendix-common-issues-and-solutions",
    "title": "DART Forest Scene Setup Tutorial",
    "section": "17 Appendix: Common Issues and Solutions",
    "text": "17 Appendix: Common Issues and Solutions\n\nInstallation IssuesSimulation ErrorsOutput IssuesScene Configuration\n\n\nProblem: DART won’t start - Check Java installation (required) - Verify environment variables - Run from command line for error messages\nProblem: Database files not found - Verify file paths in simulation settings - Check that extracted files are in correct DART directories - Use absolute paths if relative paths fail\n\n\nProblem: Out of memory errors - Reduce scene size or resolution - Decrease number of threads - Increase Java heap size in DART configuration\nProblem: Very slow execution - Verify thread count settings - Check for excessive output products - Consider using coarser resolution for testing\n\n\nProblem: Images not generated - Check “Products” configuration - Verify maximal VZA settings - Ensure sufficient disk space - Check write permissions\nProblem: Unexpected spectral values - Verify optical property assignments - Check for missing double face settings - Validate PROSPECT parameters\n\n\nProblem: Trees not visible in 2D view - Enable “Show this field’s objects in the 2D view” - Check field file format and paths - Verify tree positions are within scene bounds\nProblem: Optical properties not applied - Confirm optical property names match - Check database file paths - Verify group assignments in 3D objects"
  },
  {
    "objectID": "topics/DART_intro.html",
    "href": "topics/DART_intro.html",
    "title": "Radiative Transfer Modeling with DART: An Introduction",
    "section": "",
    "text": "Remote sensing captures light reflected or emitted from Earth’s surface, but these spectral signals encode complex information about vegetation structure, biochemistry, and function. To unlock this information and make quantitative predictions about ecosystem properties, we need more than statistical correlations—we need physical understanding of how light interacts with plant canopies.\nPhysics Meets Remote Sensing\nRadiative Transfer Models (RTMs) are powerful computational tools that simulate light propagation, scattering, and absorption within virtual 3D scenes using the fundamental laws of physics. Unlike empirical models that depend on site-specific calibrations, RTMs are based on physical principles that remain valid across different ecosystems, sensors, and observation conditions. This makes them invaluable for interpreting satellite and airborne imagery, designing new sensors, and retrieving vegetation properties that cannot be directly measured from space.\nFrom Turbid Layers to 3D Forests\nThe evolution of RTMs parallels advances in computational power and our understanding of canopy architecture. Early 1D models treated vegetation as horizontally uniform layers—appropriate for agricultural fields but insufficient for complex forests. Modern 3D RTMs like DART (Discrete Anisotropic Radiative Transfer) can simulate detailed forest scenes with individual trees, accounting for shadows, multiple scattering between canopy elements, and the complex interplay between direct sunlight and diffuse sky radiation.\nThe DART Advantage\nDART represents the state-of-the-art in 3D radiative transfer modeling. Developed by CESBIO (Centre d’Etudes Spatiales de la BIOsphère), it simulates the complete radiative budget of natural and urban landscapes. DART can generate synthetic remote sensing imagery—from optical reflectance to thermal emission to LiDAR waveforms—that mimics what satellites and aircraft would observe over real terrain. This capability enables researchers to test retrieval algorithms, optimize sensor designs, and understand which spectral bands contain information about specific vegetation traits.\nThe Challenge and Opportunity\nUsing RTMs requires careful parameterization: leaf optical properties, 3D canopy structure, atmospheric conditions, and sensor geometry must all be specified. This complexity creates both challenge and opportunity. The challenge lies in obtaining accurate input data through field measurements or existing databases. The opportunity lies in the unprecedented ability to link remote sensing signals to biophysical processes, enabling quantitative ecosystem monitoring from leaf to landscape scales."
  },
  {
    "objectID": "topics/DART_intro.html#introduction-to-radiative-transfer-modeling",
    "href": "topics/DART_intro.html#introduction-to-radiative-transfer-modeling",
    "title": "Radiative Transfer Modeling with DART: An Introduction",
    "section": "",
    "text": "Remote sensing captures light reflected or emitted from Earth’s surface, but these spectral signals encode complex information about vegetation structure, biochemistry, and function. To unlock this information and make quantitative predictions about ecosystem properties, we need more than statistical correlations—we need physical understanding of how light interacts with plant canopies.\nPhysics Meets Remote Sensing\nRadiative Transfer Models (RTMs) are powerful computational tools that simulate light propagation, scattering, and absorption within virtual 3D scenes using the fundamental laws of physics. Unlike empirical models that depend on site-specific calibrations, RTMs are based on physical principles that remain valid across different ecosystems, sensors, and observation conditions. This makes them invaluable for interpreting satellite and airborne imagery, designing new sensors, and retrieving vegetation properties that cannot be directly measured from space.\nFrom Turbid Layers to 3D Forests\nThe evolution of RTMs parallels advances in computational power and our understanding of canopy architecture. Early 1D models treated vegetation as horizontally uniform layers—appropriate for agricultural fields but insufficient for complex forests. Modern 3D RTMs like DART (Discrete Anisotropic Radiative Transfer) can simulate detailed forest scenes with individual trees, accounting for shadows, multiple scattering between canopy elements, and the complex interplay between direct sunlight and diffuse sky radiation.\nThe DART Advantage\nDART represents the state-of-the-art in 3D radiative transfer modeling. Developed by CESBIO (Centre d’Etudes Spatiales de la BIOsphère), it simulates the complete radiative budget of natural and urban landscapes. DART can generate synthetic remote sensing imagery—from optical reflectance to thermal emission to LiDAR waveforms—that mimics what satellites and aircraft would observe over real terrain. This capability enables researchers to test retrieval algorithms, optimize sensor designs, and understand which spectral bands contain information about specific vegetation traits.\nThe Challenge and Opportunity\nUsing RTMs requires careful parameterization: leaf optical properties, 3D canopy structure, atmospheric conditions, and sensor geometry must all be specified. This complexity creates both challenge and opportunity. The challenge lies in obtaining accurate input data through field measurements or existing databases. The opportunity lies in the unprecedented ability to link remote sensing signals to biophysical processes, enabling quantitative ecosystem monitoring from leaf to landscape scales."
  },
  {
    "objectID": "topics/DART_intro.html#where-to-go-further",
    "href": "topics/DART_intro.html#where-to-go-further",
    "title": "Radiative Transfer Modeling with DART: An Introduction",
    "section": "Where to go further?",
    "text": "Where to go further?\nWatch the video lecture in which Dr. Růžena Janoutová introduces the key concepts for this lesson:\n\nView and download the presentation from the video:\n\nRead the theory to understand:\n\nThe basic physical principles of light interaction with vegetation\nHow leaf-level and canopy-level models work\nDifferent levels of RTM complexity (1D, 3D geometrical, 3D complex)\nRTM products (reflectance, thermal, LiDAR) and applications\nDART’s capabilities and the RAMI benchmarking initiative\nPractical retrieval workflows using Look-Up Tables (LUTs)\n\nFollow the practical tutorial to learn how to:\n\nSet up a DART forest scene simulation\nConfigure optical properties using the PROSPECT model\nImport and position 3D tree models\nDefine spectral bands and sensor geometry\nRun simulations and generate synthetic imagery\nCreate Look-Up Tables (LUTs) for trait retrieval"
  },
  {
    "objectID": "notebooks/intro_python.html",
    "href": "notebooks/intro_python.html",
    "title": "Sine function",
    "section": "",
    "text": "Run the following code to see the sine function.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplt.plot(x, y)\nplt.title('Sine Wave')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to Remote Sensing Lessons\nThis site provides interactive tutorials in Python and R using real-world remote sensing data.\nChoose a lesson from the navigation bar to get started."
  },
  {
    "objectID": "notebooks/r_analysis.html",
    "href": "notebooks/r_analysis.html",
    "title": "Simple Data Analysis in R",
    "section": "",
    "text": "This notebook demonstrates a basic data analysis in R using randomly generated data. We’ll compute the summary and plot a histogram.\n\n# Generate 100 random numbers from normal distribution\nx &lt;- rnorm(100)\nsummary(x)\n\n\n# Plot histogram\nhist(x, main = \"Histogram of x\", col = \"skyblue\", border = \"white\")"
  },
  {
    "objectID": "topics/DART_theory.html",
    "href": "topics/DART_theory.html",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "",
    "text": "Remote sensing provides us with unprecedented capabilities to observe Earth’s ecosystems from above. However, the spectral signals captured by satellite and airborne sensors represent complex interactions between electromagnetic radiation and vegetation structures. To unlock the full information content of these signals, we need more than statistical relationships—we need physical understanding of how light interacts with plant canopies.\nRadiative Transfer Models (RTMs) are powerful tools that use physical equations to simulate light interaction within virtual scenes, including forests, agricultural fields, and urban environments. Unlike empirical models that rely on correlations between spectral data and field measurements, RTMs are based on the fundamental laws of physics governing light propagation, scattering, and absorption. This physical foundation makes RTMs invaluable for:\n\nInterpreting remote sensing observations by linking canopy reflectance to biophysical and biochemical properties\nUnderstanding ecosystem processes related to light scattering and energy balance\nDesigning new sensors and optimizing acquisition strategies\nRetrieving vegetation traits that cannot be directly measured from space\n\nThe evolution from simple 1D turbid medium approximations to complex 3D ray-tracing models has paralleled advances in computational power and our understanding of canopy architecture. Today’s RTMs can simulate scenes with remarkable detail, from individual leaf biochemistry to landscape-scale heterogeneity."
  },
  {
    "objectID": "topics/DART_theory.html#introduction-the-power-of-physical-modelling",
    "href": "topics/DART_theory.html#introduction-the-power-of-physical-modelling",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "",
    "text": "Remote sensing provides us with unprecedented capabilities to observe Earth’s ecosystems from above. However, the spectral signals captured by satellite and airborne sensors represent complex interactions between electromagnetic radiation and vegetation structures. To unlock the full information content of these signals, we need more than statistical relationships—we need physical understanding of how light interacts with plant canopies.\nRadiative Transfer Models (RTMs) are powerful tools that use physical equations to simulate light interaction within virtual scenes, including forests, agricultural fields, and urban environments. Unlike empirical models that rely on correlations between spectral data and field measurements, RTMs are based on the fundamental laws of physics governing light propagation, scattering, and absorption. This physical foundation makes RTMs invaluable for:\n\nInterpreting remote sensing observations by linking canopy reflectance to biophysical and biochemical properties\nUnderstanding ecosystem processes related to light scattering and energy balance\nDesigning new sensors and optimizing acquisition strategies\nRetrieving vegetation traits that cannot be directly measured from space\n\nThe evolution from simple 1D turbid medium approximations to complex 3D ray-tracing models has paralleled advances in computational power and our understanding of canopy architecture. Today’s RTMs can simulate scenes with remarkable detail, from individual leaf biochemistry to landscape-scale heterogeneity."
  },
  {
    "objectID": "topics/DART_theory.html#types-of-radiative-transfer-models",
    "href": "topics/DART_theory.html#types-of-radiative-transfer-models",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "2 Types of Radiative Transfer Models",
    "text": "2 Types of Radiative Transfer Models\nRTMs operate at different spatial scales and levels of complexity, each designed for specific applications and data types. Understanding this hierarchy is essential for selecting the appropriate model for a given research question.\n\n2.1 Leaf-Level RTMs\n\n\n\n\n\n\nFigure 1: Leaf-level radiative transfer mechanisms. (a) Light interaction with leaf structure: polarized (red) and non-polarized (black) light interact through surface reflection at micro-facets, absorption by chloroplasts, and scattering within mesophyll tissue. (b) Conceptual diagram showing light interaction with leaf surface and internal structures including epidermis, palisade, spongy mesophyll, and air spaces. (c) Simplified plate model showing light transmission and reflection through mesophyll layers with refractive index n&gt;1. (c) Detailed representation of light path including surface reflection and internal scattering at multiple interfaces. (d) Multi-layer representation with N layers showing transmittance (T_N) and reflectance (R_N) terms. Sources: (a) Li et al. (2025), (b) Xu and Ye (2023).\n\n\n\nLeaf-level RTMs simulate the optical properties of individual leaves based on their internal structure and biochemical composition (Jacquemoud and Baret 1990). Figure 1 illustrates the complex interactions of light within leaf tissues. Key components include:\nInternal Leaf Structure:\n\nUpper and lower epidermis: Protective layers with minimal pigment content\nPalisade mesophyll: Tightly packed cells containing high concentrations of chloroplasts\nSpongy mesophyll: Loosely arranged cells with air spaces creating scattering interfaces\nVascular tissues: Veins transporting water and nutrients\n\nLight Interaction Mechanisms:\nThe plate model (Figure 1 panels b-d) shows how light propagates through leaf layers. At each interface between materials with different refractive indices (typically air n=1 and cell wall material n≈1.4), light undergoes:\n\nSpecular reflection at the surface (~4% of incident light)\nRefraction into the leaf interior following Snell’s law\nAbsorption by pigments (chlorophylls, carotenoids, anthocyanins, water)\nScattering at cell walls and air-cell interfaces\nMultiple internal reflections within the mesophyll structure\n\nPROSPECT Model:\nThe most widely used leaf-level RTM is PROSPECT (Jacquemoud and Baret 1990; Féret et al. 2017), which treats the leaf as a stack of N identical layers, each characterized by a refractive index and specific absorption coefficients. The model requires as inputs:\n\nN: Leaf structure parameter (related to mesophyll thickness and compactness)\nC_ab: Chlorophyll a+b content (μg/cm²)\nC_ar: Carotenoid content (μg/cm²)\n\nC_brown: Brown pigments (arbitrary units)\nC_w: Equivalent water thickness (g/cm² or cm)\nC_m: Dry matter content (g/cm²)\nC_anth: Anthocyanin content (μg/cm²) [in PROSPECT-D and later versions]\n\nThe model outputs hemispherical reflectance and transmittance spectra (typically 400-2500 nm), which serve as inputs to canopy-level models.\n\n\n\n\n\n\nNotePROSPECT Model Evolution\n\n\n\nThe PROSPECT model has evolved through several versions:\n\nPROSPECT (1990): Original version with 4 parameters\nPROSPECT-5 (2008): Added brown pigments\nPROSPECT-D (2017): Added anthocyanins (Féret et al. 2017)\nPROSPECT-PRO (2021): Improved protein absorption features\n\nEach version improves spectral fidelity and retrieval accuracy for specific compounds.\n\n\n\n\n2.2 Canopy-Level RTMs\n\n\n\n\n\n\nFigure 2: Canopy-level radiative transfer in 3D scene. The diagram shows the DART model structure including atmosphere layers (high and mid atmosphere), Earth scene with detailed 3D vegetation (trees, grass, water, topography), and multiple sensor configurations. Key elements include: TOA (Top of Atmosphere), BOA (Bottom of Atmosphere), direct sun irradiance, atmosphere radiance, per-pixel radiation, and both satellite and airborne sensor geometries. Source: Gastellu-Etchegorry et al. (2017).\n\n\n\nCanopy-level RTMs simulate light interaction within entire plant canopies or landscape scenes (Gastellu-Etchegorry et al. 2017). Figure 2 illustrates the comprehensive approach of 3D canopy models like DART, which can represent:\nVertical Structure:\n\nAtmospheric layers: High and mid atmosphere with gas absorption, aerosol scattering\nCanopy layers: Vegetation at multiple heights with varying density\nUnderstory: Ground vegetation, leaf litter, soil\nUrban elements: Buildings, roads, other artificial structures when applicable\n\nHorizontal Heterogeneity:\n\nTree crowns: Individual trees with species-specific architecture\nCanopy gaps: Openings allowing direct sunlight to reach understory\nTopography: Terrain elevation affecting local illumination geometry\nMixed surfaces: Combination of vegetation, water, bare soil\n\nRadiation Components:\nCanopy-level RTMs must account for multiple radiation pathways:\n\nDirect solar radiation: Unscattered photons from the sun\nDiffuse sky radiance: Light scattered by atmosphere before reaching canopy\nMultiple scattering within canopy: Photons bouncing between leaves, stems, ground\nAtmospheric coupling: Light exiting canopy, being scattered by atmosphere, and potentially re-entering\nTopographic effects: Shadows, adjacency effects from nearby terrain\n\nModel Inputs:\nCanopy-level RTMs require comprehensive parameterization:\n\nOptical properties: Leaf/bark/soil reflectance and transmittance spectra\nStructural parameters: LAI (Leaf Area Index), canopy height, crown dimensions\n3D Architecture: Tree positions, sizes, shapes; or turbid medium descriptions\nEnvironmental conditions: Sun angles, atmospheric composition\nSensor geometry: View angles, spatial resolution, spectral bands\n\nModel Outputs:\nThese models generate various products:\n\nBidirectional Reflectance Factor (BRF): Directional reflectance images\nRadiative budget: Absorbed, transmitted, and reflected radiation\nLiDAR waveforms: Simulated laser scanning returns\nBrightness temperature: Thermal radiation (if thermal module enabled)\n\n\n\n2.3 Levels of Complexity\nRTMs can be categorized by their representation of canopy architecture:\n\n\n\n\n\n\nFigure 3: Three levels of canopy representation in radiative transfer models. Left: 1D turbid medium with horizontally homogeneous layers. Center: 3D geometrical objects with simple crown shapes. Right: 3D complex representation with explicit 3D tree structures. Yellow arrows indicate incident direct solar radiation, with scattered radiation paths shown by dashed lines.\n\n\n\n1D Models (Turbid Medium):\nThe simplest approach treats vegetation as horizontally infinite, vertically stratified layers with uniformly distributed scatterers (Figure 3, left panel). Examples include SAIL (Verhoef 1984) and GeoSAIL. Advantages:\n\nComputationally fast\nWell-suited for homogeneous canopies (crops, grasslands)\nMany analytical solutions available\n\nLimitations:\n\nCannot represent gaps, tree crowns, or horizontal heterogeneity\nPoor performance for forests and sparse vegetation\nNeglects directional structure effects\n\n3D Geometrical Models:\nAn intermediate approach represents trees as geometric primitives (cones, ellipsoids, cylinders) filled with turbid medium (Figure 3, center panel). Examples include FLIGHT (North 1996) and discrete models. Advantages:\n\nCaptures crown-scale structure and gaps\nMore realistic than 1D for forests\nModerate computational demands\n\nLimitations:\n\nStill simplified crown architecture\nCannot represent branch-level structure\nLess accurate for detailed structural studies\n\n3D Complex Models:\nThe most sophisticated approach explicitly represents 3D tree architecture using detailed geometric meshes or voxelized structures (Figure 3, right panel). Examples include DART-Lux (Gastellu-Etchegorry et al. 2017), LESS (Qi et al. 2019), and Helios++ (Bailey and Mahaffee 2019). Advantages:\n\nHighest realism and accuracy\nCan incorporate terrestrial laser scanning (TLS) data\nSuitable for LiDAR simulation and fine-scale studies\n\nLimitations:\n\nComputationally intensive\nRequires detailed 3D vegetation data\nComplex parameterization\n\n\n\n\n\n\n\nImportantChoosing the Right Model\n\n\n\nModel selection depends on:\n\nApplication scale: Leaf, plot, landscape\nAvailable data: Biochemical measurements, structural data, TLS point clouds\nResearch questions: What traits need to be retrieved?\nComputational resources: Runtime and memory constraints\nEcosystem type: Homogeneous crops vs. heterogeneous forests\n\nFor high-resolution hyperspectral imagery of forests, 3D complex models offer the best performance."
  },
  {
    "objectID": "topics/DART_theory.html#radiative-transfer-model-products",
    "href": "topics/DART_theory.html#radiative-transfer-model-products",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "3 Radiative Transfer Model Products",
    "text": "3 Radiative Transfer Model Products\nRTMs generate a variety of outputs useful for different remote sensing applications:\n\n3.1 Optical Products\nBidirectional Reflectance Factor (BRF):\nThe most common output, BRF quantifies how reflectance varies with illumination and viewing geometry. It is defined as the ratio of reflected radiance in a given direction to the radiance that would be reflected by an ideal Lambertian surface under identical illumination (Schaepman-Strub et al. 2006):\n\\[\\text{BRF}(\\theta_s, \\phi_s, \\theta_v, \\phi_v, \\lambda) = \\frac{\\pi \\cdot L(\\theta_v, \\phi_v, \\lambda)}{E(\\theta_s, \\phi_s, \\lambda)}\\]\nwhere:\n\n\\(\\theta_s, \\phi_s\\): Solar zenith and azimuth angles\n\\(\\theta_v, \\phi_v\\): View zenith and azimuth angles\n\n\\(\\lambda\\): Wavelength\n\\(L\\): Reflected radiance\n\\(E\\): Incoming irradiance\n\n\n\n\n\n\n\nFigure 4: Multi-angle BRF patterns for a forest canopy. Polar plots showing BRF at four wavelengths (550 nm, 665 nm, 780 nm, 1650 nm) across viewing hemisphere. (a) Canopy simulation design. (b) BRF vs. viewing zenith angle for different solar positions (shown as colored lines). (c) Polar projections with solar position marked by star symbol. Patterns reveal strong forward scattering (hotspot) and angular anisotropy varying with wavelength. Source: Hanuš st al. (2023).\n\n\n\nFigure 4 illustrates how BRF varies with viewing geometry at different wavelengths. Key features include:\n\nHotspot: Peak reflectance when sun and sensor are aligned (zero phase angle)\nDarkspot: Minimum reflectance in backscattering direction (opposite sun)\nBowl shape: General decrease in reflectance with increasing view zenith angle\nSpectral dependence: Angular patterns differ between visible and NIR wavelengths\n\nRTMs can simulate BRF for any combination of sun-sensor geometries, enabling:\n\nBRDF normalization of multi-temporal imagery (Hanuš et al. 2023)\nAtmosphere-corrected reflectance estimation\nOptimal view angle selection for trait retrieval\n\n\n\n3.2 Thermal Products\nSome RTMs (e.g., DART) simulate brightness temperature by coupling radiative transfer with energy balance equations (Gastellu-Etchegorry et al. 2017). This enables:\n\nCanopy temperature mapping from thermal imagery\nEvapotranspiration estimation via surface energy balance\nWater stress detection using thermal-optical indices\n\n\n\n3.3 LiDAR Products\nAdvanced 3D RTMs can simulate laser scanning returns (Gastellu-Etchegorry et al. 2017), providing:\n\nAirborne LiDAR waveforms: Full-waveform returns at nadir or off-nadir angles\nTerrestrial Laser Scanning (TLS) point clouds: Ground-based returns from tree structure\nMobile LiDAR: Vehicle-mounted or handheld scanner simulations\n\nThese synthetic LiDAR data support:\n\nSensor design: Testing new LiDAR configurations before deployment\nAlgorithm development: Validating extraction methods for canopy height, LAI, biomass\nPoint cloud interpretation: Understanding how 3D structure affects returns\n\n\n\n3.4 Radiative Budget\nRTMs compute the absorption, transmission, and reflection of radiation throughout the canopy:\n\nAbsorbed PAR (Photosynthetically Active Radiation): Critical for photosynthesis models\nTransmitted radiation: Light reaching understory\nVertical profiles: Radiation availability at different canopy heights\nPer-triangle/voxel budget: Spatially explicit energy balance\n\nThese outputs link remote sensing to ecosystem functioning and carbon cycle models."
  },
  {
    "objectID": "topics/DART_theory.html#applications-of-radiative-transfer-models",
    "href": "topics/DART_theory.html#applications-of-radiative-transfer-models",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "4 Applications of Radiative Transfer Models",
    "text": "4 Applications of Radiative Transfer Models\nRTMs serve multiple roles in ecosystem science and remote sensing (Malenovskỳ et al. 2009):\n\n4.1 1. BRDF/BRF Correction and Normalization\nMulti-temporal remote sensing imagery suffers from varying sun-sensor geometries that create artificial reflectance changes unrelated to vegetation state. RTMs enable physics-based corrections (Hanuš et al. 2023):\nProblem: Reflectance of the same surface can vary significantly depending on viewing angle.\nSolution:\n\nSimulate BRF patterns for the actual scene using RTM\nNormalize observed reflectance to a standard geometry (e.g., nadir view, 45° solar zenith)\nApply corrections accounting for topography, adjacency effects\n\nBenefits:\n\nImproved phenology tracking\nBetter change detection\nEnhanced time-series analysis\n\n\n\n4.2 2. Biophysical and Biochemical Trait Retrieval\nThe primary application of RTMs is estimating vegetation properties from remote sensing spectra. This is typically accomplished through Look-Up Table (LUT) inversion (Dorigo et al. 2007):\nWorkflow:\n\nGenerate LUT: Run RTM thousands of times varying input traits (LAI, chlorophyll, etc.)\nCreate spectral database: Store simulated spectra with corresponding trait values\nMatch observations: Find LUT entry with spectrum closest to observed pixel\nRetrieve traits: Extract corresponding trait values\n\nRetrieved Traits Include:\n\nBiochemical: Chlorophyll (Cab), carotenoids (Car), water content (Cw), dry matter (Cm), anthocyanins (Canth)\nStructural: LAI, canopy height, canopy cover, clumping index\nPhotosynthetic: Absorbed PAR, light use efficiency\n\n\n\n\n\n\n\nFigure 5: Vegetation trait retrieval workflow. Left: True-color (CASI 2016) and false-color (SASI 2016) hyperspectral images of a forest study site. Right: Retrieved trait maps showing spatial distribution of leaf chlorophyll content (Cab), leaf carotenoids content (Car), leaf water content (Cw), and leave area index (LAI). Color scales indicate concentration gradients across the heterogeneous canopy. Source: Janoutová et al. (2021).\n\n\n\nFigure 5 demonstrates trait retrieval results from airborne hyperspectral imagery, showing detailed spatial patterns of biochemical content and canopy structure.\n\n\n4.3 3. Sensitivity Analysis\nBefore collecting expensive remote sensing data or designing retrieval algorithms, RTMs can reveal which spectral bands contain information about specific traits (Malenovskỳ et al. 2009):\nApproach:\n\nVary one trait while holding others constant\nSimulate spectra for each trait value\nQuantify spectral sensitivity: \\(\\partial \\text{BRF}(\\lambda) / \\partial \\text{Trait}\\)\nIdentify optimal wavebands for trait estimation\n\n\n\n\n\n\n\nFigure 6: Sensitivity analysis for different tree types and canopy representations. Simulated RGB, CIR (Color Infrared), and 3D canopy views for three species (Norway spruce, white peppermint, European beech) using three modeling approaches: 3D detailed, turbid medium, and simple geometric representation. Each representation creates different spatial patterns and spectral responses, revealing the importance of architectural fidelity for accurate simulations. Source: Janoutová et al. (2021).\n\n\n\nFigure 6 illustrates how different levels of structural representation (3D detailed vs. turbid medium vs. simple geometry) affect simulated imagery. This type of analysis reveals:\n\nWhich traits are retrievable given sensor specifications\nHow structural simplifications affect spectral accuracy\nTrade-offs between model complexity and simulation quality\n\n\n\n4.4 4. Radiative Budget Modeling\nUnderstanding how much light is absorbed, transmitted, or reflected by vegetation is fundamental to ecosystem energy balance and photosynthesis:\nApplications:\n\nCarbon cycle modeling: Linking absorbed PAR to gross primary productivity\nEnergy balance studies: Partitioning radiation into sensible and latent heat fluxes\nVertical profile analysis: Light availability for understory species\nClimate model parameterization: Albedo and roughness for land surface schemes\n\n\n\n4.5 5. Sensor Simulation and Mission Design\nBefore launching new satellites or airborne campaigns, RTMs can test sensor configurations and optimize acquisition strategies:\nQuestions Addressed:\n\nWhat spatial resolution is needed to resolve canopy gaps?\nAre 10 nm bands better than 20 nm bands for chlorophyll retrieval?\nWhat view angles maximize sensitivity to LAI?\nHow does along-track vs. across-track scanning affect shadowing?\n\nCase Study: The FLEX (Fluorescence Explorer) satellite mission used RTM simulations extensively to define optimal spectral bands, view angles, and overpass times for measuring solar-induced chlorophyll fluorescence."
  },
  {
    "objectID": "topics/DART_theory.html#the-radiation-transfer-model-intercomparison-rami-initiative",
    "href": "topics/DART_theory.html#the-radiation-transfer-model-intercomparison-rami-initiative",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "5 The Radiation transfer Model Intercomparison (RAMI) Initiative",
    "text": "5 The Radiation transfer Model Intercomparison (RAMI) Initiative\nGiven the diversity of RTMs with different underlying assumptions and implementations, the community recognized the need for systematic model benchmarking. The RAMI initiative provides standardized test scenes and reference solutions (Widlowski et al. 2013).\n\n\n\n\n\n\nFigure 7: Evolution of the RAMI initiative from 1999 to 2022. Timeline showing progression through RAMI phases: RAMI-1 (1999) for basic benchmarking, RAMI-2 (2002) for expanded scenarios, RAMI-3 (2005) for 3D heterogeneous scenes, ROMC (2007) for online model checker, RAMI4PILPS (2008/2009) coupling with land surface models, RAMI-IV (2009/2015) for realistic forest scenes, RAMI-V (2020) for comprehensive validation, and RAMI4ATM (2022) for atmospheric coupling. Source: Source: https://rami-benchmark.jrc.ec.europa.eu/_www/index.php\n\n\n\nRAMI Objectives:\n\nIdentify model errors through systematic comparison\nProvide reference datasets for model validation\nEstablish best practices for model use\nBenchmark computational performance\n\nTest Scenarios:\n\nAbstract scenes: Homogeneous canopies, geometric primitives with known solutions\nRealistic scenes: Laser-scanned forests, heterogeneous landscapes\nMulti-scale challenges: Leaf-to-landscape scaling tests\n\nParticipating Models:\nOver 30 RTMs have participated in RAMI exercises, including DART, FLIGHT, FRT, RAYTRAN, Sprint, SCOPE, and librat. Results have driven substantial improvements in model accuracy and efficiency.\n\n\n\n\n\n\nNoteRAMI Best Practices\n\n\n\nRAMI exercises revealed common issues:\n\nInterpolation errors in tabulated optical properties\nInsufficient angular sampling for anisotropic scattering\nNumerical precision problems in multi-scattering calculations\nBoundary condition artifacts at scene edges\n\nModern models incorporate fixes for these issues, but users should remain aware of potential limitations."
  },
  {
    "objectID": "topics/DART_theory.html#the-dart-model-a-comprehensive-3d-rtm",
    "href": "topics/DART_theory.html#the-dart-model-a-comprehensive-3d-rtm",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "6 The DART Model: A Comprehensive 3D RTM",
    "text": "6 The DART Model: A Comprehensive 3D RTM\nAmong canopy-level RTMs, DART (Discrete Anisotropic Radiative Transfer) stands out for its comprehensiveness, computational efficiency, and active development (Gastellu-Etchegorry et al. 2017).\n\n\n\n\n\n\nFigure 8: DART model architecture showing three computational modes. Left: DART-FT using adapted discrete ordinates with voxelized atmosphere and scene. Center: DART-RC using forward Monte Carlo ray tracing with photon tracking. Right: DART-Lux using bi-directional path tracing with explicit triangle meshes. All modes simulate optical, thermal, and LiDAR products for natural and urban landscapes. Source: DART User Manual - https://dart.omp.eu/Public/documentation/contenu/documentation/DART_User_Manual.pdf\n\n\n\n\n6.1 Key Features\n1. Multiple Computational Modes:\nDART offers three ray-tracing engines optimized for different applications (Figure 8):\n\nDART-FT (Flux Tracking): Discrete ordinates method, fast for large scenes, good for satellite simulations\nDART-RC (Ray Carlo): Forward Monte Carlo, accurate for complex geometry, suitable for airborne sensing\n\nDART-Lux: Bi-directional path tracing, highest accuracy for detailed 3D scenes, optimal for UAV/TLS simulation\n\n2. Comprehensive Product Suite:\n\nOptical: BRF at any wavelength, any view angle\nThermal: Brightness temperature accounting for 3D structure\nLiDAR: Full-waveform airborne, TLS, and mobile laser scanning\nRadiative budget: 3D voxel grid of absorbed/transmitted radiation\nFluorescence: Solar-induced chlorophyll fluorescence (SIF)\n\n3. Integrated Leaf Model:\nPROSPECT is built into DART, enabling seamless simulation from biochemical traits to canopy reflectance. Users can specify Cab, Car, Cw, etc., and DART automatically computes leaf optical properties.\n4. Atmosphere Modeling:\nDART couples canopy and atmosphere radiative transfer, simulating:\n\nGas absorption (H2O, O2, O3, CO2)\nAerosol scattering (multiple aerosol models)\nTop-of-atmosphere (TOA) and bottom-of-atmosphere (BOA) products\nAtmospheric correction via inversion\n\n5. Flexible Scene Construction:\nUsers can build scenes using:\n\n3D geometric objects: Import OBJ/PLY meshes from TLS, photogrammetry, or modeling software\nVoxel grids: Define LAI and optical properties per voxel\nTurbid medium: Quick parametric description for homogeneous stands\nMixed representations: Combine approaches (e.g., explicit crowns + turbid understory)\n\n\n\n6.2 DART Advantages and Limitations\n\nAdvantagesDisadvantages\n\n\nComprehensive Capabilities:\n\nSimulate almost any remote sensing scenario\nOptical, thermal, LiDAR in one framework\nNatural and urban landscapes\n\nComputational Efficiency:\n\nHighly optimized code (C++/CUDA)\nParallelized (multi-core CPU, GPU acceleration)\nLarge, detailed scenes feasible on desktop computers\n\nActive Development:\n\nRegular updates with new features\nOptimization ongoing\nResponsive development team\n\nValidated Accuracy:\n\nConsistently performs well in RAMI exercises\nPeer-reviewed and widely used in literature\n\nStrong Community Support:\n\nActive user forum\nComprehensive manual (600+ pages)\nRegular training workshops (2-3 per year)\nPrepared tutorials and example scenes\n\n\n\nSteep Learning Curve:\n\nComplex graphical interface with hundreds of parameters\nOverwhelming for new users\nRequires understanding of radiative transfer theory\n\nParameterization Challenges:\n\nRequires detailed input data (optical properties, structure)\nMany parameters create opportunity for errors\nSensitivity to some parameters not always intuitive\n\nComputational Demands:\n\nDespite optimization, complex 3D scenes still take hours\nLarge LUTs require significant storage and computation time\nGPU acceleration helps but not always available\n\nProduct Interpretation:\n\nRich output requires expertise to interpret correctly\nMultiple products can be confusing\nUnits, coordinate systems need attention\n\nSoftware Maturity:\n\nSome features still experimental\nOccasional bugs (though rapidly fixed)\nDocumentation sometimes lags new features\n\n\n\n\n\n\n\n\n\n\nTipGetting Started with DART\n\n\n\nFor newcomers to DART:\n\nStart Simple: Begin with abstract scenes (homogeneous canopy) before attempting complex forests\nUse Tutorials: Work through provided examples in the DART manual\nAttend Training: DART summer schools offer hands-on guidance\nEngage Community: Post questions on the forum—developers and experienced users respond quickly\nValidate Incrementally: Compare simulations with field measurements at each step\nLeverage Tools: Use DART’s built-in tools (database manager, 3D viewer, sequence launcher) to streamline workflows"
  },
  {
    "objectID": "topics/DART_theory.html#vegetation-traits-retrieval",
    "href": "topics/DART_theory.html#vegetation-traits-retrieval",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "7 Vegetation Traits Retrieval",
    "text": "7 Vegetation Traits Retrieval\nThe primary application of RTMs in ecosystem monitoring is retrieving biophysical and biochemical traits from remote sensing imagery. Unlike empirical methods that rely on statistical correlations, RTM-based retrieval has a physical foundation, making it more robust across different ecosystems, sensors, and viewing conditions (Dorigo et al. 2007).\n\n7.1 Retrieval Workflow Overview\n\n\n\n\n\n\nFigure 9: Vegetation traits retrieval workflow. The process flows from bottom to top: (1) Leaf & canopy RT models are parameterized with ground measurements, (2) Radiative transfer simulations generate a database of spectral signatures for quantitative vegetation parameters (chlorophylls shown as example), (3) Retrieval algorithm compares this database with reflectance image data (after corrections), (4) Validation using ground measurements confirms accuracy. The workflow represents bottom-up scaling from leaf to canopy level. Source: Malenovský et al. (2009).\n\n\n\nFigure 9 illustrates the complete RTM-based retrieval workflow (Malenovskỳ et al. 2009). The process involves:\n\nParameterization: Field measurements provide inputs for leaf and canopy RT models\nDatabase generation: RTM simulations create a Look-Up Table (LUT) of spectra for different trait combinations\nImage matching: Retrieval algorithms compare observed spectra with the LUT database\nValidation: Retrieved traits are compared with independent field measurements\n\nThis approach enables quantitative trait estimation from spectral data, bridging the gap between remote sensing observations and ecosystem properties.\n\n\n7.2 Choosing the Right Approach\nBefore starting a retrieval study, several key questions determine the appropriate RTM complexity:\nWhat type of remote sensing data will be analyzed?\n\nSensor platform: Satellite, airborne, or UAV\nData type: Reflectance (BRF), point cloud, thermal imagery\nResolution: Spatial (pixel size) and spectral (number/width of bands)\nEcosystem type: Forest, agricultural field, or urban area\n\nWhat traits need to be retrieved?\n\nBiochemical traits: Chlorophyll content, carotenoids, water content, dry matter\nStructural traits: LAI, canopy cover, tree height, crown dimensions\nOther properties: Absorbed PAR, biomass, species composition\n\nExample decision:\nFor high-resolution UAV hyperspectral imagery (5 cm pixels, 32-52 nm bands) over a forest site, with the goal of retrieving biochemical traits like chlorophyll and carotenoids:\n→ 3D complex RTM (like DART) is required\nThe fine spatial resolution captures individual tree crowns and intra-crown variation, requiring explicit 3D representation. The hyperspectral data contains detailed biochemical information that can be extracted through physics-based modeling.\n\n\n7.3 Field Data Collection\nAccurate RTM parameterization requires comprehensive field measurements:\n\n\n\n\n\n\nFigure 10: Field measurement equipment and spectral data. (a) Spectroradiometers with contact probe (bark), pistol grip (ground), and integrating sphere (leaves). (b) Example spectral reflectance and transmittance curves for leaves (green wavelengths show characteristic chlorophyll absorption), and reflectance spectra for twigs/bark (brown) and ground (dashed black). These field measurements provide the optical properties needed to parameterize RTMs.\n\n\n\nOptical Properties:\nMeasure reflectance and transmittance of all scene elements using spectroradiometers:\n\nLeaves: Integrating sphere for both reflectance and transmittance\nBark/trunks: Contact probe for reflectance\nGround: Pistol grip for soil/litter reflectance\n\nBiochemical Analysis:\nLaboratory measurements complement spectral data:\n\nChlorophyll content (a and b)\nCarotenoid content\nWater and dry matter content\nThese can be used directly in PROSPECT model or to validate leaf spectra\n\nStructural Parameters:\nDocument canopy architecture:\n\nLAI (Leaf Area Index) and canopy cover\nTree positions, heights, crown dimensions\nFor detailed studies: Terrestrial laser scanning (TLS) for 3D structure\n\nAlternative Data Sources:\nWhen field measurements are limited, use databases:\n\nTRY database for plant traits\nICP Forest, ICOS networks for forest plots\nPublished spectral libraries for common species\n\n\n\n7.4 Scene Configuration\nRTM simulations require careful scene parameterization. General parameters apply to all RTM studies:\nSite Location and Sun Geometry:\n\nCoordinates: Latitude and longitude of study site\nDate and time: Determine solar zenith and azimuth angles\n\nCan be calculated using solar position algorithms (available in Python/R libraries or online tools)\n\nSensor geometry: View zenith and azimuth angles for BRF simulation\n\nSpectral and Spatial Configuration:\n\nSpectral bands: Match your actual sensor (e.g., hyperspectral: 400-2500 nm, Δλ = 5-10 nm)\nPixel resolution: Match image resolution (e.g., 5 cm for UAV, 10 m for Sentinel-2)\n\n\n\n7.5 3D Forest Scene Parameters\nFor forest ecosystems, additional structural detail is required (Hanousek et al. 2024):\nTwo Approaches:\n\nGeneral scene (for LUT generation):\n\nSmaller scene extent (e.g., 30m × 30m)\nMultiple structural combinations (varying LAI and canopy cover)\nCaptures full variability of the forest type\nRequires more simulations but creates comprehensive LUT\n\nExact scene (for site-specific simulation):\n\nLarger scene matching study site\nFewer combinations (only realistic structural parameters for that site)\nIncludes all factors affecting spectra at specific location\nUsed when you have detailed field data for one location\n\n\n\n\n\n\n\n\nFigure 11: 3D forest scene representation in DART. (a) Top-down view showing tree positions in systematic grid pattern with brown ground visible between crowns. (b) Oblique 3D view showing detailed tree architecture with individual crowns, trunks, and branches. Scene dimensions are 30m × 30m with trees positioned to achieve target canopy cover and LAI. Source: Hanousek et al. (2024).\n\n\n\nScene Components (Figure 11):\n\nTree positions: Define x, y coordinates for each tree\n3D tree models: Import detailed geometry (from DART database, TLS, or modeling software)\nOptical properties: Assign leaf, bark, and ground spectra to scene elements\nStructural parameters: Set LAI, canopy cover through tree density and size\n\n\n\n\n\n\n\nNoteLUT vs. Site-Specific Simulation\n\n\n\nThe DART tutorial you’ve completed shows how to create and run a single simulation with specific parameters. For trait retrieval, researchers typically create Look-Up Tables (LUTs) by running hundreds or thousands of simulations with systematically varied parameters.\nHowever, as a tutorial user, you’ll likely work with pre-generated LUTs created by research groups. Your role is understanding:\n\nHow LUTs are structured (traits → spectra relationships)\nHow to apply retrieval algorithms to your imagery\nHow scene parameters affect simulated spectra\n\nCreating large LUTs requires computational resources and expertise typically available in research labs.\n\n\n\n\n7.6 Look-Up Table Design\nLUTs form the core of RTM-based retrieval, systematically exploring the relationship between traits and spectra (Hanousek et al. 2024):\n\n\n\n\n\n\nFigure 12: Look-Up Table generation strategy. Flow diagram shows how leaf traits (chlorophyll, carotenoids, water, dry matter, structural parameter) combine with structural and scene parameters (canopy cover, LAI, sun zenith angle, sun azimuth angle). Total possible combinations: 3.5 million. Through random sampling (2000 combinations) and filtering for realism (1728 combinations remain after excluding unrealistic scenarios like low canopy cover with high LAI), these are organized into trait groups, then expanded across structural-geometric combinations, yielding a final database of 3,456,000 spectra. Source: Hanousek et al. (2024).\n\n\n\nFigure 12 shows a sophisticated LUT design for broadleaf forests. Key principles:\nParameter Space Definition:\n\nLeaf biochemistry: Chlorophyll, carotenoids, water, dry matter ranges based on literature and field data\nCanopy structure: LAI, canopy cover ranges representing forest variability\nGeometric conditions: Solar angles covering different times of day and seasons\n\nSampling Strategy:\n\nStart with all possible combinations (millions)\nApply realistic constraints (e.g., high LAI requires sufficient canopy cover)\nUse random or Latin Hypercube Sampling for efficient space coverage\nResult: Computationally feasible database (thousands to millions of spectra)\n\nPractical Considerations:\nFor the tutorial user, the key insight is that trait retrieval requires this systematic parameter exploration. Pre-generated LUTs available from research groups save you from running thousands of DART simulations yourself.\n\n\n7.7 Processing Simulated Images\nBefore using LUT spectra for retrieval, simulated images must be processed consistently with real imagery (Hanousek et al. 2024):\n\n\n\n\n\n\nFigure 13: Processing of simulated DART imagery. (a) RGB composite showing forest canopy with green sunlit leaves, dark shadows between crowns, and brown/purple mixed pixels at crown edges. (b) Binary mask isolating only sunlit leaf pixels (bright green) while excluding shadows, woody parts, and ground. This masking ensures that LUT spectra represent the same surface types as pixels used in retrieval from real imagery. Source: Hanousek et al. (2024).\n\n\n\nCritical Step: Masking (Figure 13)\nApply identical masking to simulated and observed imagery:\n\nInclude: Sunlit leaf pixels (target for trait retrieval)\nExclude:\n\nDeep shadows (too dark, unreliable spectra)\nTrunks and branches (woody parts have different optical properties)\nGround/soil (background contamination)\nMixed pixels (boundaries, reduce ambiguity)\n\n\nSpectral Processing:\n\nConvolve high-resolution RTM spectra to sensor bands\nApply atmospheric correction (if needed)\nExtract mean/median spectra from masked regions\n\nConsistency is Key:\nWhatever processing you apply to simulated images (masking, smoothing, aggregation) must be applied identically to real images. Inconsistency is a major source of retrieval errors.\n\n\n7.8 Applying Retrieval to Real Imagery\nWith a prepared LUT and processed imagery, retrieval algorithms estimate traits by matching observed spectra to the database (Slanináková et al. 2025):\n\n\n\n\n\n\nFigure 14: Comparison of retrieval methods on seasonal hyperspectral imagery. Three rows show results from Statistical (empirical regression), ALSS (Adaptive LUT Subset Selection), and LUT (full Look-Up Table matching) methods. Four columns represent different dates: 26 Apr 2019, 21 Jun 2021, 18 Jul 2019, and 22 Oct 2020. Color scale shows chlorophyll content (Cab) from 10-60 μg/cm². All methods capture seasonal variation (low chlorophyll in spring/autumn, high in summer) and spatial patterns (individual tree crowns), but differ in smoothness and detail. ALSS balances accuracy and robustness. Source: Slanináková et al. (2025).\n\n\n\nCommon Retrieval Approaches:\n\nStatistical methods: Empirical regression calibrated on field samples\n\nFast, simple, but site and sensor-specific\nShown in left column of Figure 14\n\nFull LUT matching: Find closest spectrum in entire database\n\nMost detail, but sensitive to model errors\nShown in right column of Figure 14\n\nAdaptive LUT Subset Selection (ALSS): Iteratively narrow search space\n\nBalances detail and robustness (Slanináková et al. 2025)\nShown in middle column of Figure 14\nRecommended for operational use\n\n\nKey Observations from Figure 14:\n\nAll methods capture seasonal dynamics (spring → summer → autumn)\nSpatial patterns reveal individual tree differences\nALSS provides intermediate detail level, reducing noise while preserving important variation\nChoice of method depends on priorities: speed vs. detail vs. robustness\n\nValidation:\nAlways compare retrieved traits against independent field measurements:\n\nCalculate RMSE, R², and bias\nCheck for systematic errors across trait range\nValidate on multiple dates to assess temporal consistency\n\n\n\n\n\n\n\nImportantPractical Retrieval Workflow\n\n\n\nAs a DART tutorial user, your practical retrieval workflow will likely be:\n\nObtain imagery: Acquire hyperspectral/multispectral data of your study site\nPre-process: Atmospheric correction, geometric correction, masking\nAccess LUT: Use pre-generated LUT from collaborators or published studies\nApply retrieval: Use provided algorithms (Python/R scripts) to match imagery to LUT\nValidate: Compare with field measurements\n\nYou typically won’t generate your own LUT unless working on a long-term research project with computational resources. The DART tutorial teaches you the principles behind LUT generation so you understand the retrieval process."
  },
  {
    "objectID": "topics/DART_theory.html#challenges-and-future-directions",
    "href": "topics/DART_theory.html#challenges-and-future-directions",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "8 Challenges and Future Directions",
    "text": "8 Challenges and Future Directions\nWhile RTM-based retrieval is powerful, several challenges remain:\nCurrent Limitations:\n\nComputational cost: Generating large LUTs requires significant time (days to weeks)\nParameterization burden: Accurate simulations need extensive field data\nModel assumptions: Even 3D RTMs simplify reality (leaf clumping, bark texture, soil variability)\nScale mismatches: Bridging leaf-level measurements to landscape imagery\nValidation scarcity: Limited sites with comprehensive trait measurements\n\nEmerging Solutions:\n\nMachine learning emulators: Neural networks approximate RTM behavior, reducing computation by orders of magnitude (Verrelst et al. 2012)\nTLS integration: Laser scanning provides unprecedented structural detail (Janoutová et al. 2021)\nMulti-sensor fusion: Combine optical, LiDAR, and thermal for comprehensive characterization\nUncertainty quantification: Bayesian approaches provide confidence intervals, not just point estimates\nOperational pipelines: Automated systems for routine trait mapping from satellites\n\nVision for the Future:\nThe next decade will see RTM-based trait products become operational:\n\nReal-time mapping from satellite imagery\nGlobal coverage at moderate resolution (10-30 m)\nIntegration with ecosystem models for carbon cycle and climate studies\nBiodiversity monitoring through spectral diversity proxies\n\nAchieving this requires continued collaboration among remote sensing scientists, ecologists, and operational agencies."
  },
  {
    "objectID": "topics/DART_theory.html#summary-and-key-takeaways",
    "href": "topics/DART_theory.html#summary-and-key-takeaways",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "9 Summary and Key Takeaways",
    "text": "9 Summary and Key Takeaways\n\nRTM FundamentalsDART ModelRetrieval WorkflowPractical AdviceFuture Outlook\n\n\n\nPhysical basis: RTMs simulate light-vegetation interactions using physics, not correlations\nScales: From leaf (micrometers) to landscape (kilometers)\nModels: 1D (homogeneous), 3D geometrical (tree shapes), 3D complex (explicit structure)\nProducts: BRF, thermal, LiDAR, radiative budget\n\n\n\n\nComprehensive: Optical, thermal, and LiDAR in one framework\nValidated: Strong performance in RAMI benchmarks\nFlexible: Multiple scene representations, 3D object import\nPractical: Tutorial teaches basic workflow for forest scenes\n\nBest for high-resolution studies requiring detailed 3D representation.\n\n\n\nField measurements: Optical properties, traits, structure\nScene setup: Configure RTM with realistic parameters\nLUT generation: Systematic parameter exploration (typically done by research labs)\nImage processing: Consistent masking and spectral processing\nRetrieval: Match observed spectra to LUT database\nValidation: Compare with independent field data\n\nCritical: Consistency between simulated and observed data processing.\n\n\n\nTutorial users: Focus on understanding principles, use pre-generated LUTs\nResearchers: Generate custom LUTs for specific ecosystems/sensors\nValidation essential: Always compare retrievals with field measurements\nMethod selection: ALSS balances accuracy and robustness\nStart simple: Master basic simulations before complex scenes\n\n\n\n\nEmulation: ML-accelerated RTMs for operational speed\nTLS integration: Explicit 3D structure from laser scanning\nMulti-sensor fusion: Optical + LiDAR + thermal\nUncertainty: Probabilistic retrievals with confidence\nGlobal products: Routine trait mapping from satellites\n\nRTMs enable quantitative ecosystem monitoring from space."
  },
  {
    "objectID": "topics/DART_theory.html#glossary-of-key-terms",
    "href": "topics/DART_theory.html#glossary-of-key-terms",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "10 Glossary of Key Terms",
    "text": "10 Glossary of Key Terms\n\n\n\n\n\n\nNoteExpand Glossary\n\n\n\n\n\nAbsorption coefficient: Wavelength-specific rate at which a material absorbs photons, typically per unit path length.\nBRDF (Bidirectional Reflectance Distribution Function): Mathematical function describing how reflectance varies with all possible illumination and viewing geometries.\nBRF (Bidirectional Reflectance Factor): Ratio of reflected radiance to that from an ideal Lambertian reflector under the same illumination, for specific sun-sensor geometry.\nCanopy: Collective foliage and structure of vegetation over an area.\nDART: Discrete Anisotropic Radiative Transfer model, a comprehensive 3D RTM.\nFlux: Radiant energy passing through a surface per unit time and area (W/m²).\nHotspot: Peak in reflectance when sun and sensor are aligned (zero phase angle), due to absence of shadows.\nIrradiance: Radiant power incident on a surface per unit area (W/m²).\nLAI (Leaf Area Index): One-sided leaf area per unit ground area (m²/m²).\nLUT (Look-Up Table): Database of simulated spectra and corresponding trait values for inversion.\nMonte Carlo: Ray-tracing method using random sampling to simulate photon paths.\nPhase angle: Angle between sun, target, and sensor directions.\nPROSPECT: Plate model for simulating leaf optical properties from biochemistry.\nRadiance: Radiant power per unit solid angle per unit projected area (W/m²/sr).\nRAMI: RAdiation transfer Model Intercomparison initiative for benchmarking RTMs.\nReflectance: Ratio of reflected to incident radiant flux (dimensionless, 0-1).\nRTM (Radiative Transfer Model): Physical model simulating light propagation and interaction in vegetation.\nScattering: Redirection of photons due to interaction with particles or surfaces.\nTransmittance: Ratio of transmitted to incident radiant flux (dimensionless, 0-1).\nTurbid medium: Approximation of canopy as homogeneous layer with uniformly distributed scatterers.\nVZA (View Zenith Angle): Angle between nadir (vertical) and sensor view direction."
  },
  {
    "objectID": "topics/DART_theory.html#additional-resources",
    "href": "topics/DART_theory.html#additional-resources",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "11 Additional Resources",
    "text": "11 Additional Resources\n\n\n\n\n\n\nTipLearning Materials\n\n\n\n\n\nDART Resources:\n\nDART Official Website: Downloads, manual, forum\nDART User Manual: Comprehensive guide\nDART Summer Schools: Annual training workshops\n\nKey Publications:\n\nDART model: Gastellu-Etchegorry et al. (2017)\nRetrieval methods: Dorigo et al. (2007), Verrelst et al. (2019)\nLUT design: Hanousek et al. (2024)\nALSS method: Slanináková et al. (2025)\nTLS integration: Janoutová et al. (2021)\n\nSoftware and Tools:\n\nDART: https://dart.omp.eu/\nPROSPECT: Integrated in DART\nRetrieval algorithms: Often provided as Python/R scripts by research groups"
  },
  {
    "objectID": "topics/DART_theory.html#references",
    "href": "topics/DART_theory.html#references",
    "title": "Theoretical Background: Radiative Transfer Modelling for Ecosystem Monitoring",
    "section": "12 References",
    "text": "12 References\n\n\nBailey, Brian N., and Walter F. Mahaffee. 2019. “Helios++: A New Extensible, Scalable 3D Plant and Environmental Biophysical Modelling Framework in C++.” Frontiers in Plant Science 10: 1185. https://doi.org/10.3389/fpls.2019.01185.\n\n\nDorigo, W. A., R. Zurita-Milla, A. J. W. de Wit, J. Brazile, R. Singh, and M. E. Schaepman. 2007. “A Review on Reflective Remote Sensing and Data Assimilation Techniques for Enhanced Agroecosystem Modeling.” International Journal of Applied Earth Observation and Geoinformation 9 (2): 165–93. https://doi.org/10.1016/j.jag.2006.05.003.\n\n\nFéret, Jean-Baptiste, Anatoly A. Gitelson, Shawn D. Noble, and Stéphane Jacquemoud. 2017. “PROSPECT-D: Towards Modeling Leaf Optical Properties Through a Complete Lifecycle.” Remote Sensing of Environment 193: 204–15. https://doi.org/10.1016/j.rse.2017.03.004.\n\n\nGastellu-Etchegorry, Jean-Philippe, Nicolas Lauret, Tiangang Yin, Lucas Landier, Abdelaziz Kallel, Zbyněk Malenovský, Ahmad Al Bitar, et al. 2017. “DART: Recent Advances in Remote Sensing Data Modeling With Atmosphere, Polarization, and Chlorophyll Fluorescence.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 10 (6): 2640–49. https://doi.org/10.1109/JSTARS.2017.2685528.\n\n\nHanousek, Tomáš, Tereza Slanináková, Tomáš Rebok, and Růžena Janoutová. 2024. “High Spatial and Spectral Resolution Dataset of Hyperspectral Look-up Tables for 3.5 Million Traits and Structural Combinations of Central European Temperate Broadleaf Forests.” Data in Brief 57: 111105. https://doi.org/10.1016/j.dib.2024.111105.\n\n\nHanuš, Jan, Lukáš Slezák, Tomáš Fabiánek, Lukáš Fajmon, Tomáš Hanousek, Růžena Janoutová, Daniel Kopkáně, et al. 2023. “Flying Laboratory of Imaging Systems: Fusion of Airborne Hyperspectral and Laser Scanning for Ecosystem Research.” Remote Sensing 15 (12): 3130. https://doi.org/10.3390/rs15123130.\n\n\nJacquemoud, Stéphane, and Frédéric Baret. 1990. “PROSPECT: A Model of Leaf Optical Properties Spectra.” Remote Sensing of Environment 34 (2): 75–91. https://doi.org/10.1016/0034-4257(90)90100-Z.\n\n\nJanoutová, Růžena, Lucie Homolová, Jan Novotný, Barbora Navrátilová, Miloš Pikl, and Zbyněk Malenovský. 2021. “Detailed Reconstruction of Trees from Terrestrial Laser Scans for Remote Sensing and Radiative Transfer Modelling Applications.” In Silico Plants 3 (2): diab026. https://doi.org/10.1093/insilicoplants/diab026.\n\n\nMalenovskỳ, Zbyněk, Kumud Bandhu Mishra, František Zemek, Uwe Rascher, and Ladislav Nedbal. 2009. “Scientific and Technical Challenges in Remote Sensing of Plant Canopy Reflectance and Fluorescence.” Journal of Experimental Botany 60 (11): 2987–3004. https://doi.org/10.1093/jxb/erp156.\n\n\nNorth, Peter R. J. 1996. “Three-Dimensional Forest Light Interaction Model Using a Monte Carlo Method.” IEEE Transactions on Geoscience and Remote Sensing 34 (4): 946–56. https://doi.org/10.1109/36.508411.\n\n\nQi, Jianbo, Donghui Xie, Tiangang Yin, Guangjian Yan, Jean-Philippe Gastellu-Etchegorry, Linyuan Li, Wuming Zhang, Xihan Mu, and Leslie K. Norford. 2019. “LESS: LargE-Scale Remote Sensing Data and Image Simulation Framework over Heterogeneous 3D Scenes.” Remote Sensing of Environment 221: 695–706. https://doi.org/10.1016/j.rse.2018.11.036.\n\n\nSchaepman-Strub, Gabriela, Michael E Schaepman, Thomas H Painter, Stefan Dangel, and John V Martonchik. 2006. “Reflectance Quantities in Optical Remote Sensing—Definitions and Case Studies.” Remote Sensing of Environment 103 (1): 27–42. https://doi.org/10.1016/j.rse.2006.03.002.\n\n\nSlanináková, Tereza, Marian Švik, Tomáš Rebok, Tomáš Hanousek, and Růžena Janoutová. 2025. “Introducing a New Adaptive Look-up Table Subset Selection Method for Leaf Chlorophyll and Carotenoids Retrieval in Broadleaved Forests.” Remote Sensing Letters 16 (6): 676–86. https://doi.org/10.1080/2150704X.2025.2495992.\n\n\nVerhoef, Wout. 1984. “Light Scattering by Leaf Layers with Application to Canopy Reflectance Modeling: The SAIL Model.” Remote Sensing of Environment 16 (2): 125–41. https://doi.org/10.1016/0034-4257(84)90057-9.\n\n\nVerrelst, Jochem, Gustau Camps-Valls, Jordi Muñoz-Marí, Juan Pablo Rivera, Frank Veroustraete, Jan G. P. W. Clevers, and José Moreno. 2012. “Machine Learning Regression Algorithms for Biophysical Parameter Retrieval: Opportunities for Sentinel-2 and -3.” Remote Sensing of Environment 118: 273–84. https://doi.org/10.1016/j.rse.2011.11.002.\n\n\nVerrelst, Jochem, Zbyněk Malenovský, Christiaan Van der Tol, Gustau Camps-Valls, Jean-Philippe Gastellu-Etchegorry, Philip Lewis, Peter North, and José Moreno. 2019. “Quantifying Vegetation Biophysical Variables from Imaging Spectroscopy Data: A Review on Retrieval Methods.” Surveys in Geophysics 40 (3): 589–629. https://doi.org/10.1007/s10712-018-9478-y.\n\n\nWidlowski, J-L, Bernard Pinty, Maciej Lopatka, Clement Atzberger, Daniela Buzica, Michaël Chelle, Mathias Disney, et al. 2013. “The Fourth Radiation Transfer Model Intercomparison (RAMI-IV): Proficiency Testing of Canopy Reflectance Models with ISO-13528.” Journal of Geophysical Research: Atmospheres 118 (13): 6869–90. https://doi.org/10.1002/jgrd.50497.\n\n\n\nThis theoretical background document is designed to accompany practical tutorials on RTM application. For hands-on guidance on using DART for forest scenes, see the companion “DART Forest Scene Setup Tutorial”."
  },
  {
    "objectID": "topics/hyperspectral_intro.html",
    "href": "topics/hyperspectral_intro.html",
    "title": "Hyperspectral Data Analysis: An Introduction",
    "section": "",
    "text": "Remote sensing reveals the invisible language of plants through their interaction with light. While our eyes perceive only three broad color channels, hyperspectral sensors measure reflectance across hundreds of narrow wavelength bands from 400 to 2400 nm, creating unique spectral signatures that encode information about plant biochemistry and physiology.\nThe Color-Chemistry Connection\nEvery plant pigment and structural compound absorbs and reflects light at specific wavelengths, creating diagnostic spectral patterns. Chlorophylls dominate healthy green vegetation with strong absorption around 680 nm (red) and 430 nm (blue), while carotenoids absorb in the blue-green region (480-560 nm). Anthocyanins, stress-response pigments that accumulate during senescence or environmental stress, create characteristic absorption patterns in the green-yellow region (500-600 nm). Even structural components like leaf waxes and cellulose leave distinct spectral fingerprints in the near-infrared and shortwave infrared regions.\nFrom Spectral Signatures to Biological Insights\nThese spectral patterns change dynamically across seasons and in response to stress. As leaves transition from spring flush through summer maturity to autumn senescence, their reflectance spectra transform, revealing shifts in pigment composition, water content, and structural integrity. By measuring these spectral changes, we can track plant phenology, detect stress before visible symptoms appear, and estimate biochemical concentrations non-destructively from leaf to landscape scales.\nThe Analysis Challenge\nThe richness of hyperspectral data—hundreds of correlated wavelength measurements per sample—creates both opportunity and challenge. This tutorial demonstrates how to extract meaningful biological information from high-dimensional spectral data using statistical analysis, vegetation indices, and machine learning classification techniques, bridging the gap between raw reflectance measurements and actionable ecological understanding."
  },
  {
    "objectID": "topics/hyperspectral_intro.html#introduction-to-hyperspectral-remote-sensing-of-plant-functional-traits",
    "href": "topics/hyperspectral_intro.html#introduction-to-hyperspectral-remote-sensing-of-plant-functional-traits",
    "title": "Hyperspectral Data Analysis: An Introduction",
    "section": "",
    "text": "Remote sensing reveals the invisible language of plants through their interaction with light. While our eyes perceive only three broad color channels, hyperspectral sensors measure reflectance across hundreds of narrow wavelength bands from 400 to 2400 nm, creating unique spectral signatures that encode information about plant biochemistry and physiology.\nThe Color-Chemistry Connection\nEvery plant pigment and structural compound absorbs and reflects light at specific wavelengths, creating diagnostic spectral patterns. Chlorophylls dominate healthy green vegetation with strong absorption around 680 nm (red) and 430 nm (blue), while carotenoids absorb in the blue-green region (480-560 nm). Anthocyanins, stress-response pigments that accumulate during senescence or environmental stress, create characteristic absorption patterns in the green-yellow region (500-600 nm). Even structural components like leaf waxes and cellulose leave distinct spectral fingerprints in the near-infrared and shortwave infrared regions.\nFrom Spectral Signatures to Biological Insights\nThese spectral patterns change dynamically across seasons and in response to stress. As leaves transition from spring flush through summer maturity to autumn senescence, their reflectance spectra transform, revealing shifts in pigment composition, water content, and structural integrity. By measuring these spectral changes, we can track plant phenology, detect stress before visible symptoms appear, and estimate biochemical concentrations non-destructively from leaf to landscape scales.\nThe Analysis Challenge\nThe richness of hyperspectral data—hundreds of correlated wavelength measurements per sample—creates both opportunity and challenge. This tutorial demonstrates how to extract meaningful biological information from high-dimensional spectral data using statistical analysis, vegetation indices, and machine learning classification techniques, bridging the gap between raw reflectance measurements and actionable ecological understanding."
  },
  {
    "objectID": "topics/hyperspectral_intro.html#where-to-go-further",
    "href": "topics/hyperspectral_intro.html#where-to-go-further",
    "title": "Hyperspectral Data Analysis: An Introduction",
    "section": "Where to go further?",
    "text": "Where to go further?\nWatch the video lecture in which Dr. Giorgi Kozhoridze introduces the key concepts for this lesson:\n\nView and download the presentation from the video:\n\nRead the theory\nAnalyze real data with a practical tutorial in R"
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html",
    "href": "topics/hyperspectral_tutorial.html",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "",
    "text": "This tutorial demonstrates how to analyze hyperspectral data for plant classification and biomarker estimation. We’ll work with spectral reflectance data from plant samples collected in different months (March and July) across various plant groups and classes. The analysis includes data preprocessing, visualization, statistical testing, and machine learning classification using both raw spectral data and derived vegetation indices."
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#overview",
    "href": "topics/hyperspectral_tutorial.html#overview",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "",
    "text": "This tutorial demonstrates how to analyze hyperspectral data for plant classification and biomarker estimation. We’ll work with spectral reflectance data from plant samples collected in different months (March and July) across various plant groups and classes. The analysis includes data preprocessing, visualization, statistical testing, and machine learning classification using both raw spectral data and derived vegetation indices."
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#prerequisites",
    "href": "topics/hyperspectral_tutorial.html#prerequisites",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nBefore starting, you’ll need to download the data and install and load several R packages for data manipulation, visualization, and machine learning.\n\n# Install required packages (uncomment if needed)\n# install.packages(c(\"tidyverse\", \"ggpubr\", \"dplyr\", \"caret\", \"ggplot2\",\n#                    \"patchwork\", \"ggforce\", \"pls\"))\n# \n# # Install BiocManager and mixOmics for advanced multivariate analysis\n# install.packages(\"BiocManager\")\n# BiocManager::install(\"mixOmics\")\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(ggforce)\nlibrary(pls)\nlibrary(mixOmics)"
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#part-1-data-preparation-and-exploration",
    "href": "topics/hyperspectral_tutorial.html#part-1-data-preparation-and-exploration",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "3 Part 1: Data Preparation and Exploration",
    "text": "3 Part 1: Data Preparation and Exploration\n\n3.1 Loading and Inspecting the Data\nFirst, we load our hyperspectral dataset and examine its structure:\n\n# Load the hyperspectral data\ndata &lt;- read.csv(\"SamplesForWorkshop.csv\")\n\n# Inspect the basic structure\nhead(data[,1:21])\n\n  Month     Group SampleN Class       Chla       Chlb          C    Antho\n1 March Deciduous     101   SP1 0.04112532 0.01990401 0.02018104 0.001240\n2 March Deciduous     103   SP1 0.03786589 0.01835909 0.01925522 0.000829\n3 March Deciduous     106   SP1 0.06864117 0.03369560 0.02865739 0.000932\n4 March Deciduous     107   SP1 0.09533804 0.04758828 0.04024224 0.000903\n5 March Deciduous     108   SP1 0.05198037 0.02378042 0.02842488 0.000441\n6 March Deciduous     109   SP1 0.03009055 0.01016909 0.01941967 0.000037\n   Cellulose     Wax       X400       X401       X402       X403       X404\n1         NA      NA 0.01935004 0.01953985 0.01971527 0.01990466 0.02013160\n2 0.02858881 0.00138 0.02714292 0.02732804 0.02751218 0.02771258 0.02794536\n3         NA      NA 0.03921675 0.03947513 0.03973155 0.04001420 0.04034739\n4 0.02790119 0.00132 0.07315494 0.07358253 0.07399735 0.07442525 0.07488811\n5 0.02567766 0.00159 0.04725425 0.04754305 0.04782556 0.04812899 0.04847630\n6         NA      NA 0.12793647 0.12843469 0.12890088 0.12936994 0.12988197\n        X405       X406       X407       X408       X409       X410\n1 0.02035488 0.02059546 0.02085226 0.02109311 0.02135383 0.02163444\n2 0.02818707 0.02844890 0.02872564 0.02899116 0.02927687 0.02958125\n3 0.04068215 0.04104167 0.04142650 0.04180553 0.04220256 0.04262228\n4 0.07534337 0.07581604 0.07630594 0.07677775 0.07726508 0.07776967\n5 0.04883025 0.04920400 0.04959861 0.04999560 0.05041397 0.05085308\n6 0.13043007 0.13095214 0.13144691 0.13196342 0.13249848 0.13304018\n\n\n\n\n\n\n\n\nNoteDataset Structure\n\n\n\nThe dataset contains:\n\nMonth: Sampling time (March, July)\nGroup: Plant functional groups (Deciduous, Evergreen)\n\nClass: Plant species (SP0, SP1, SP2)\nBiomarkers: Chemical measurements in leaves: Chlorophylus A, Chlorophylus B, Carotenoids, Anthocyanins, Cellulose, Wax)\nSpectral bands: Reflectance values at different wavelengths (X400, X401, …, X2400)\n\nFor more details about the dataset, including measurement units, see Kozhoridze et al. (2016).\n\n\n\n\n3.2 Data Type Conversion\nNext, we convert categorical variables to factors and identify spectral columns:\n\n# Convert categorical variables to factors\ndata$Month &lt;- factor(data$Month, levels = c(\"March\", \"July\"))\ndata$Group &lt;- as.factor(data$Group)\ndata$Class &lt;- as.factor(data$Class)\n\n# Identify spectral band columns (wavelengths from 400-2400 nm)\nspectral_cols &lt;- colnames(data)[grepl(\"^X\\\\d+$\", colnames(data))]\ncat(\"Number of spectral band columns found:\", length(spectral_cols), \"\\n\")\n\nNumber of spectral band columns found: 1753 \n\n\n\n\n\n\n\n\nImportantWhy Factor Conversion Matters\n\n\n\nConverting to factors ensures proper statistical analysis and visualization. The spectral columns represent reflectance measurements at specific wavelengths, forming the hyperspectral signature of each sample.\n\n\n\n\n3.3 Creating Long Format for Analysis\nTransform the wide spectral data into long format for easier plotting and analysis:\n\n# Reshape spectral data to long format\ndata_long &lt;- data %&gt;%\n  pivot_longer(cols = all_of(spectral_cols), \n               names_to = \"Wavelength\", \n               values_to = \"Reflectance\") %&gt;%\n  mutate(Wavelength = as.numeric(sub(\"X\", \"\", Wavelength)))\n\n# Check the data structure\nprint(table(data$Class, data$Group, data$Month))\n\n, ,  = March\n\n     \n      Deciduous Evergreen\n  SP0         0         6\n  SP1         6         0\n  SP2         3         0\n\n, ,  = July\n\n     \n      Deciduous Evergreen\n  SP0         0         6\n  SP1         6         0\n  SP2         3         0"
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#part-2-biomarker-analysis-and-visualization",
    "href": "topics/hyperspectral_tutorial.html#part-2-biomarker-analysis-and-visualization",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "4 Part 2: Biomarker Analysis and Visualization",
    "text": "4 Part 2: Biomarker Analysis and Visualization\n\n4.1 Comparing Biomarkers Across Groups\nCreate comprehensive visualizations to compare biomarker levels across different plant species and traits:\n\n# Prepare data for biomarker comparison\ndf_long &lt;- data %&gt;%\n  pivot_longer(cols = c(Chla, Chlb, C, Antho, Cellulose, Wax),\n               names_to = \"Biomarker\",\n               values_to = \"Value\")\n\n# Create boxplots comparing biomarkers\nggplot(df_long, aes(x = Group, y = Value, fill = Class)) +\n  geom_boxplot() +\n  facet_grid(Biomarker ~ Month, scales = \"free_y\") +\n  labs(title = \"Comparison of Biomarkers by Group and Class\",\n       y = \"Value\", x = \"Group\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nComparison of biomarkers by group, class, and month\n\n\n\n\n\n\n\n\n\n\nTipUnderstanding Biomarkers\n\n\n\nEach biomarker represents different plant physiological processes:\n\nChla/Chlb: Chlorophyll content (photosynthetic capacity)\nC: Carotenoid content (accessory pigments for light harvesting and photoprotection)\nAntho: Anthocyanin content (stress response pigments)\nCellulose: Structural carbohydrates\nWax: Protective leaf surface compounds\n\n\n\n\n\n4.2 Statistical Testing\nPerform pairwise comparisons to identify significant differences:\n\n# Calculate pairwise comparisons for each month and biomarker\nvalid_comparisons &lt;- df_long %&gt;%\n  group_by(Month, Biomarker) %&gt;%\n  filter(!is.na(Value)) %&gt;%\n  filter(Class %in% c(\"SP0\", \"SP1\", \"SP2\")) %&gt;%\n  compare_means(formula = Value ~ Class,\n                group.by = c(\"Month\", \"Biomarker\"),\n                method = \"t.test\",\n                p.adjust.method = \"none\",\n                comparisons = list(c(\"SP0\", \"SP1\"), c(\"SP0\", \"SP2\"), c(\"SP1\", \"SP2\")),\n                na.rm = TRUE)\n\n# Add position information for significance bars\nglobal_max_y &lt;- max(df_long$Value, na.rm = TRUE)\nvalid_comparisons &lt;- valid_comparisons %&gt;%\n  group_by(Month, Biomarker) %&gt;%\n  arrange(p) %&gt;%\n  mutate(y.position = global_max_y * 1.05 + (row_number() - 1) * global_max_y * 0.1) %&gt;%\n  ungroup()\n\n# Create plot with significance indicators\np &lt;- ggboxplot(df_long, x = \"Class\", y = \"Value\", fill = \"Class\",\n               facet.by = c(\"Month\", \"Biomarker\"),\n               short.panel.labs = TRUE,\n               add = \"jitter\",\n               palette = \"jco\") +\n  stat_pvalue_manual(valid_comparisons,\n                     label = \"p.signif\",\n                     y.position = \"y.position\",\n                     tip.length = 0.01) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nprint(p)\n\n\n\n\nBiomarker comparisons with significance testing\n\n\n\n\n\n\n\n\n\n\nNoteStatistical Significance\n\n\n\nStatistical testing helps identify which differences are statistically significant rather than due to random variation. The significance indicators (, , ) show the strength of evidence against the null hypothesis."
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#part-3-spectral-analysis-by-biomarker-levels",
    "href": "topics/hyperspectral_tutorial.html#part-3-spectral-analysis-by-biomarker-levels",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "5 Part 3: Spectral Analysis by Biomarker Levels",
    "text": "5 Part 3: Spectral Analysis by Biomarker Levels\nThis section explores how spectral signatures relate to biomarker concentrations.\n\n5.1 Creating High/Low Groups Based on Quantiles\n\n\n\n\n\n\nNoteQuantile-Based Biomarker Grouping\n\n\n\nTo identify samples with distinctly high versus low biomarker concentrations, we use the 75th percentile (third quartile) as a threshold within each sampling month. Samples with biomarker values at or above the 75th percentile are classified as “High” (representing the top 25% of samples), while all remaining samples are classified as “Low” (bottom 75%). This approach ensures we’re comparing the most extreme cases - plants with genuinely elevated biomarker levels against the majority with typical or lower concentrations - making spectral differences more pronounced and biologically meaningful.\n\n\nThe following function implements this quantile-based approach to analyze spectral differences between high and low biomarker groups:\n\nanalyze_spectral_by_biomarker &lt;- function(biomarker_name) {\n  # Filter data to specific plant groups\n  data1 &lt;- subset(data, Group == \"Deciduous\" | Group == \"Evergreen\")\n  data1 &lt;- data1 %&gt;%\n    mutate(Class = as.character(Class),\n           Class = str_trim(Class)) %&gt;%\n    filter(!is.na(.data[[biomarker_name]]))\n  \n  # Group samples by 3rd quantile (top 25% vs. bottom 75%)\n  data1 &lt;- data1 %&gt;%\n    group_by(Month) %&gt;%\n    mutate(q3_val = quantile(.data[[biomarker_name]], 0.75, na.rm = TRUE),\n           BioGroup = if_else(.data[[biomarker_name]] &gt;= q3_val, \"High\", \"Low\")) %&gt;%\n    ungroup()\n  \n  # Extract spectral columns\n  spectral_cols &lt;- colnames(data1)[str_detect(colnames(data1), \"^X\\\\d{3,4}$\")]\n  \n  # Reshape to long format\n  spectral_long &lt;- data1 %&gt;%\n    dplyr::select(Month, BioGroup, all_of(spectral_cols)) %&gt;%\n    pivot_longer(cols = all_of(spectral_cols), \n                 names_to = \"Wavelength\", \n                 values_to = \"Reflectance\") %&gt;%\n    mutate(Wavelength = as.numeric(str_remove(Wavelength, \"^X\"))) %&gt;%\n    filter(!is.na(Reflectance))\n  \n  # Filter to visible and near-infrared range\n  spectral_filtered &lt;- spectral_long %&gt;%\n    filter(Wavelength &gt;= 400, Wavelength &lt;= 2400)\n  \n  # Calculate mean reflectance for each group\n  avg_spectral &lt;- spectral_filtered %&gt;%\n    group_by(Month, BioGroup, Wavelength) %&gt;%\n    summarise(MeanReflectance = mean(Reflectance, na.rm = TRUE), .groups = \"drop\")\n  \n  # Create plot\n  ggplot(avg_spectral, aes(x = Wavelength, y = MeanReflectance, color = BioGroup)) +\n    geom_line(linewidth = 1) +\n    facet_wrap(~ Month) +\n    theme_minimal() +\n    labs(title = paste(\"Spectral Reflectance by\", biomarker_name, \"Level\"),\n         x = \"Wavelength (nm)\",\n         y = \"Mean Reflectance\",\n         color = paste(biomarker_name, \"Level\"))\n}\n\n\n\n5.2 Applying the Analysis to Different Biomarkers\n\n# Analyze different biomarkers\nbiomarkers &lt;- c(\"Antho\", \"C\", \"Chlb\", \"Chla\", \"Cellulose\", \"Wax\")\n\nfor(biomarker in biomarkers) {\n  plot &lt;- analyze_spectral_by_biomarker(biomarker)\n  print(plot)\n}\n\n\n\n\nSpectral signatures for different biomarker levels\n\n\n\n\n\n\n\nSpectral signatures for different biomarker levels\n\n\n\n\n\n\n\nSpectral signatures for different biomarker levels\n\n\n\n\n\n\n\nSpectral signatures for different biomarker levels\n\n\n\n\n\n\n\nSpectral signatures for different biomarker levels\n\n\n\n\n\n\n\nSpectral signatures for different biomarker levels\n\n\n\n\n\n\n\n\n\n\nTipUnderstanding Spectral Signatures\n\n\n\nDifferent biomarkers create distinct spectral patterns:\n\nChlorophyll: Strong absorption around 680 nm (red) and 430 nm (blue)\nCarotenoids: Absorption around 480 nm (blue) and 500-550 nm (green), with reflectance peak around 550 nm (yellow)\nAnthocyanins: Absorption in green-yellow region (500-600 nm)\nCellulose: Absorption features in near-infrared (1400-1500 nm, 1900-2000 nm)\nWax: Affects overall reflectance levels, especially in near-infrared"
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#part-4-machine-learning-classification",
    "href": "topics/hyperspectral_tutorial.html#part-4-machine-learning-classification",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "6 Part 4: Machine Learning Classification",
    "text": "6 Part 4: Machine Learning Classification\n\n6.1 Classification Using Full Spectral Data\nWe’ll use Partial Least Squares Discriminant Analysis (PLS-DA) to classify plant samples based on their spectral signatures."
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#what-is-pls-da",
    "href": "topics/hyperspectral_tutorial.html#what-is-pls-da",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "7 What is PLS-DA?",
    "text": "7 What is PLS-DA?\nPartial Least Squares Discriminant Analysis (PLS-DA) is a supervised classification technique that combines dimensionality reduction with discriminant analysis, making it particularly well-suited for high-dimensional data like hyperspectral datasets. Unlike traditional discriminant analysis methods that can fail when the number of variables exceeds the number of samples, PLS-DA first projects the data onto a lower-dimensional space of latent variables (components) that maximize the covariance between the predictor variables (spectral bands) and the class labels. This projection simultaneously reduces noise, handles multicollinearity among spectral bands, and identifies the most discriminative spectral features. The method then performs classification in this reduced space, making it robust for datasets with hundreds or thousands of correlated variables and relatively few samples - a common scenario in remote sensing applications."
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#part-5-classification-using-vegetation-indices",
    "href": "topics/hyperspectral_tutorial.html#part-5-classification-using-vegetation-indices",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "8 Part 5: Classification Using Vegetation Indices",
    "text": "8 Part 5: Classification Using Vegetation Indices\nInstead of using all spectral bands, we can create vegetation indices that capture specific plant properties.\n\n8.1 Calculating Vegetation Indices\n\n# Define wavelength ranges for index calculations\ncols_750_770 &lt;- paste0(\"X\", 750:770)\ncols_490_500 &lt;- paste0(\"X\", 490:500)\ncols_660_690 &lt;- paste0(\"X\", 660:690)\ncols_690_720 &lt;- paste0(\"X\", 690:720)\ncols_760_800 &lt;- paste0(\"X\", 760:800)\ncols_510_520 &lt;- paste0(\"X\", 510:520)\ncols_690_710 &lt;- paste0(\"X\", 690:710)\ncols_530_570 &lt;- paste0(\"X\", 540:560)\n\n# Calculate mean reflectance for each range\nmean_750_770 &lt;- rowMeans(data[, cols_750_770], na.rm = TRUE)\nmean_490_500 &lt;- rowMeans(data[, cols_490_500], na.rm = TRUE)\nmean_660_690 &lt;- rowMeans(data[, cols_660_690], na.rm = TRUE)\nmean_690_720 &lt;- rowMeans(data[, cols_690_720], na.rm = TRUE)\nmean_760_800 &lt;- rowMeans(data[, cols_760_800], na.rm = TRUE)\nmean_510_520 &lt;- rowMeans(data[, cols_510_520], na.rm = TRUE)\nmean_690_710 &lt;- rowMeans(data[, cols_690_710], na.rm = TRUE)\nmean_530_570 &lt;- rowMeans(data[, cols_530_570], na.rm = TRUE)\n\n# Calculate vegetation indices\nVI_Chl &lt;- (1/mean_690_720 - 1/mean_760_800) * mean_760_800      # Chlorophyll index\nVI_C &lt;- (1/mean_510_520 - 1/mean_690_710) * mean_760_800        # Carbon index\nVI_Antho &lt;- (1/mean_530_570 - 1/mean_690_710) * mean_760_800    # Anthocyanin index\nVI_Cell &lt;- with(data, 100 * (0.5 * (X2030 + X2210) - X2100))   # Cellulose index\nVI_Wax &lt;- 1 / sqrt(mean_750_770 - mean_490_500 - mean_660_690)  # Wax index\n\n# Add indices to dataset\ndata$VI_Chl &lt;- VI_Chl\ndata$VI_C &lt;- VI_C\ndata$VI_Antho &lt;- VI_Antho\ndata$VI_Cell &lt;- VI_Cell\ndata$VI_Wax &lt;- VI_Wax\n\n\n\n\n\n\n\nTipWhy Use Vegetation Indices\n\n\n\nThese indices are designed to enhance specific plant properties while reducing the effects of:\n\nAtmospheric conditions\nSoil background\nIllumination variations\nNoise in individual spectral bands\n\n\n\n\n\n8.2 Classification Using Indices\n\n# Prepare index-based classification data\nClassData &lt;- subset(data, select = c(\"Month\", \"Group\", \"Class\", \n                                    \"VI_Chl\", \"VI_C\", \"VI_Antho\", \"VI_Cell\", \"VI_Wax\"))\n\n# Scale indices separately for each month\nMarchData &lt;- scale(ClassData[1:15, 4:8])\nJulyData &lt;- scale(ClassData[16:30, 4:8])\nClass_Data_Scaled &lt;- rbind(MarchData, JulyData)\nClass_Data_Scaled &lt;- cbind(ClassData[, 1:3], Class_Data_Scaled)\n\n# Modified PLS-DA function for vegetation indices\nrun_plsda_for_month_indices &lt;- function(data, month) {\n  df &lt;- subset(data, Month == month)\n  df &lt;- df[, !duplicated(names(df))]\n  \n  vi_cols &lt;- grep(\"^VI_\", names(df))\n  X &lt;- as.matrix(df[, vi_cols])\n  Y &lt;- factor(df$Class)\n  \n  plsda_model &lt;- plsda(X, Y, ncomp = 3)\n  \n  pred_out &lt;- predict(plsda_model, X)\n  pred &lt;- if (is.list(pred_out) && \"class\" %in% names(pred_out)) {\n    pred_out$class$max.dist[, 2]\n  } else {\n    as.factor(pred_out)\n  }\n  \n  cm &lt;- confusionMatrix(factor(pred, levels = levels(Y)), Y)\n  OA &lt;- cm$overall[\"Accuracy\"]\n  Kappa &lt;- cm$overall[\"Kappa\"]\n  \n  scores_all &lt;- plsda_model$variates$X[, 1:2]\n  plot_df &lt;- data.frame(scores_all, Class = Y)\n  colnames(plot_df)[1:2] &lt;- c(\"Comp.1\", \"Comp.2\")\n  \n  p &lt;- ggplot(plot_df, aes(x = Comp.1, y = Comp.2, color = Class)) +\n    geom_point(size = 3) +\n    geom_mark_ellipse(aes(fill = Class), alpha = 0.2, show.legend = FALSE) +\n    theme_minimal() +\n    ggtitle(sprintf(\"%s - OA=%.3f, Kappa=%.3f\", month, OA, Kappa)) +\n    theme(plot.title = element_text(hjust = 0.5))\n  \n  return(list(plot = p, OA = OA, Kappa = Kappa))\n}\n\n# Run classification with indices\nClass_Data_Scaled_M &lt;- subset(Class_Data_Scaled, Month == \"March\")\nClass_Data_Scaled_J &lt;- subset(Class_Data_Scaled, Month == \"July\")\n\nmarch_res &lt;- run_plsda_for_month_indices(Class_Data_Scaled_M, \"March\")\njuly_res &lt;- run_plsda_for_month_indices(Class_Data_Scaled_J, \"July\")\n\ncombined_plot &lt;- march_res$plot + july_res$plot + plot_layout(ncol = 2)\nprint(combined_plot)\n\n\n\n\nPLS-DA classification results using vegetation indices\n\n\n\n\n\n\n8.3 VIP Analysis for Vegetation Indices\n\n# VIP analysis for vegetation indices\nrun_plsda_and_get_vip_indices &lt;- function(data, month, ncomp = 3) {\n  df &lt;- subset(data, Month == month)\n  df &lt;- df[, !duplicated(names(df))]\n  \n  vi_cols &lt;- grep(\"^VI_\", names(df))\n  X &lt;- as.matrix(df[, vi_cols])\n  Y &lt;- factor(df$Class)\n  \n  plsda_model &lt;- mixOmics::plsda(X, Y, ncomp = ncomp)\n  vip_scores &lt;- mixOmics::vip(plsda_model)[,1]\n  \n  return(list(model = plsda_model, VIP = vip_scores))\n}\n\n# Extract VIP scores\nmarch_VIP &lt;- run_plsda_and_get_vip_indices(Class_Data_Scaled_M, \"March\")\njuly_VIP &lt;- run_plsda_and_get_vip_indices(Class_Data_Scaled_J, \"July\")\n\n# Prepare and plot VIP data\nvip_df &lt;- data.frame(\n  Variable = names(march_VIP$VIP),\n  March = as.numeric(march_VIP$VIP),\n  July = as.numeric(july_VIP$VIP)\n)\n\nvip_long &lt;- vip_df %&gt;%\n  pivot_longer(cols = c(\"March\", \"July\"), names_to = \"Month\", values_to = \"VIP\")\n\nvip_plot &lt;- ggplot(vip_long, aes(x = Variable, y = VIP, fill = Month)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8), width = 0.7) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\") +\n  geom_text(aes(label = round(VIP, 2)), position = position_dodge(width = 0.8), \n            vjust = -0.5, size = 3) +\n  theme_minimal() +\n  labs(title = \"VIP Scores Comparison: March vs July (Vegetation Indices)\", \n       y = \"VIP Score\", x = NULL, fill = \"Month\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nprint(vip_plot)\n\n\n\n\nVIP scores for vegetation indices"
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#summary-and-key-takeaways",
    "href": "topics/hyperspectral_tutorial.html#summary-and-key-takeaways",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "9 Summary and Key Takeaways",
    "text": "9 Summary and Key Takeaways\n\nData PreprocessingExploratory AnalysisClassification ApproachesModel ValidationBest Practices\n\n\n\nAlways scale spectral data before analysis\nHandle missing values appropriately\nConvert categorical variables to factors\nConsider temporal effects when designing analysis\n\n\n\n\nVisualize biomarker distributions across groups\nExamine spectral signatures for different conditions\nUse statistical tests to identify significant differences\nLook for patterns across different wavelength regions\n\n\n\nFull spectral data: - Pros: More information, captures subtle spectral features - Cons: High dimensionality, potential overfitting, computational complexity\nVegetation indices:\n- Pros: Reduced dimensionality, interpretable, robust to noise - Cons: May lose some spectral information, limited to pre-defined indices\n\n\n\nUse appropriate cross-validation strategies\nReport multiple accuracy metrics (OA, Kappa)\nInterpret VIP scores to understand important features\nConsider seasonal/temporal variations in model performance\n\n\n\n\nAlways validate results across different time periods\nConsider biological relevance when interpreting results\nUse multiple approaches to confirm findings\nDocument preprocessing steps for reproducibility"
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#advanced-topics-and-extensions",
    "href": "topics/hyperspectral_tutorial.html#advanced-topics-and-extensions",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "10 Advanced Topics and Extensions",
    "text": "10 Advanced Topics and Extensions\n\n10.1 Cross-Validation Strategies\nFor more robust model evaluation, implement proper cross-validation:\n\n# Example of k-fold cross-validation for PLS-DA\nperform_cv_plsda &lt;- function(data, k_folds = 5) {\n  # Prepare data\n  ind_cols &lt;- grep(\"VI\", names(data))\n  X &lt;- as.matrix(data[, ind_cols])\n  Y &lt;- factor(data$Class)\n  \n  # Create folds\n  folds &lt;- createFolds(Y, k = k_folds, list = TRUE)\n  \n  cv_results &lt;- map_dfr(names(folds), function(fold_name) {\n    test_idx &lt;- folds[[fold_name]]\n    train_idx &lt;- setdiff(1:nrow(X), test_idx)\n    \n    # Train model\n    plsda_model &lt;- plsda(X[train_idx, ], Y[train_idx], ncomp = 3)\n    \n    # Predict on test set\n    pred &lt;- predict(plsda_model, X[test_idx, ])\n    pred_class &lt;- pred$class$max.dist[, 2]  # Use component 2\n    \n    # Calculate metrics\n    cm &lt;- confusionMatrix(factor(pred_class, levels = levels(Y)), Y[test_idx])\n    \n    tibble(\n      fold = fold_name,\n      accuracy = cm$overall[\"Accuracy\"],\n      kappa = cm$overall[\"Kappa\"]\n    )\n  })\n  \n  return(cv_results)\n}\n\n# Example usage (uncomment to run)\nmarch_data &lt;- subset(Class_Data_Scaled, Month == \"March\")\ncv_results &lt;- perform_cv_plsda(march_data)\nprint(paste(\"Mean CV Accuracy:\", round(mean(cv_results$accuracy), 3)))\n\n[1] \"Mean CV Accuracy: 0.517\""
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#why-cross-validation-matters",
    "href": "topics/hyperspectral_tutorial.html#why-cross-validation-matters",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "11 Why Cross-Validation Matters",
    "text": "11 Why Cross-Validation Matters\nCross-validation prevents overly optimistic performance estimates by testing models on data they haven’t seen during training. The earlier classification results used the same data for both training and testing (self-prediction), which typically inflates accuracy metrics since models can memorize training patterns rather than learn generalizable features. K-fold cross-validation splits data into multiple train-test partitions, providing more realistic estimates of how well the model will perform on new, unseen samples - the true test of a classification algorithm’s practical utility."
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#conclusion",
    "href": "topics/hyperspectral_tutorial.html#conclusion",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "12 Conclusion",
    "text": "12 Conclusion\nThis tutorial provides a comprehensive workflow for analyzing hyperspectral data in plant science applications. The methods demonstrated here can be adapted to various remote sensing and precision agriculture applications. The combination of statistical analysis, machine learning, and domain knowledge creates a robust framework for understanding plant spectral properties and their relationship to physiological characteristics.\nThe integration of multiple analytical approaches—from basic statistical comparisons to advanced machine learning techniques—provides researchers with a complete toolkit for hyperspectral data analysis. The emphasis on both full spectral analysis and vegetation indices demonstrates the trade-offs between detailed spectral information and practical, interpretable measures.\n\n\n\n\n\n\nNoteNext Steps\n\n\n\nTo further develop your hyperspectral analysis skills:\n\nCross-validation: Implement robust validation strategies with your own datasets\nAlternative algorithms: Experiment with Random Forest, SVM, or deep learning approaches\nFeature selection: Apply dimensionality reduction techniques to optimize model performance\nTemporal analysis: Explore how spectral signatures change over time or growing seasons\nScale integration: Connect lab measurements to field and satellite observations\nOperational applications: Develop automated pipelines for routine monitoring tasks\n\n\n13 Additional Resources\n\nSpectral libraries: USGS, ECOSIS for reference spectra\nSoftware packages: hsdar, RStoolbox for specialized hyperspectral analysis\nRemote sensing: Integration with Google Earth Engine or other platforms\nField validation: Best practices for ground-truthing remote sensing predictions\n\n\n\n\n\n\n\n\n\n\nWarningImportant Considerations\n\n\n\nWhen applying these methods to your own data:\n\nSample size: Ensure adequate samples per class for reliable statistics\nData quality: Check for instrument calibration and atmospheric corrections\nBiological relevance: Validate that spectral differences align with known plant physiology\n\nTemporal effects: Account for seasonal, phenological, and environmental variations\nScale effects: Consider how lab results translate to field and landscape scales\nModel generalization: Test models across different sites, sensors, and conditions\n\n\n\n\nThis tutorial was designed to provide both theoretical understanding and practical implementation guidance for hyperspectral data analysis. The modular structure allows researchers to adapt specific components to their unique research questions and datasets."
  },
  {
    "objectID": "topics/hyperspectral_tutorial.html#additional-resources",
    "href": "topics/hyperspectral_tutorial.html#additional-resources",
    "title": "Hyperspectral Data Analysis Tutorial: From Spectral Reflectance to Plant Classification",
    "section": "13 Additional Resources",
    "text": "13 Additional Resources\n\nSpectral libraries: USGS, ECOSIS for reference spectra\nSoftware packages: hsdar, RStoolbox for specialized hyperspectral analysis\nRemote sensing: Integration with Google Earth Engine or other platforms\nField validation: Best practices for ground-truthing remote sensing predictions"
  },
  {
    "objectID": "topics/SART_theory.html",
    "href": "topics/SART_theory.html",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "",
    "text": "Synthetic Aperture Radar (SAR) tomography, or TomoSAR, represents a powerful technique for reconstructing three-dimensional forest structure from radar observations. The term “tomography” derives from Greek: “tomos” (slice or section) and “graphia” (to describe or write). In essence, tomography enables the reconstruction of an object’s internal structure by analyzing its projections from multiple viewing angles.\nThe fundamental principle of tomography is well known from medical imaging, where techniques such as computed tomography (CT) or magnetic resonance imaging (MRI) reconstruct the internal structure of the human body by taking slice measurements from different angles. SAR tomography applies similar concepts to remote sensing of forests, using multiple radar acquisitions from different spatial positions to resolve the vertical structure within each image pixel.\n\n\n\nVideo\n\n\nFigure 1: Tomographic reconstruction principle. Multiple projections through an object (in this case, showing projections through the Greek letter π) are combined to reconstruct the 2D or 3D structure. Each projection represents the integrated signal along a specific viewing angle. By combining projections from multiple angles, the internal structure can be recovered. This same principle applies to SAR tomography, where multiple SAR images from different baselines provide the “projections” needed to reconstruct the vertical structure of forests. Source: https://en.wikipedia.org/wiki/Tomographic_reconstruction.\n\n\n\nAs illustrated in Figure 1, projections taken from various angles contain information about the object’s internal structure. Through mathematical processing, including filtering techniques such as high-pass filters, a clear reconstruction of the original object can be achieved. While medical tomography scans a patient lying on a table through a rotating scanner, SAR tomography achieves similar results by acquiring radar images from different flight tracks or orbital positions."
  },
  {
    "objectID": "topics/SART_theory.html#introduction-tomography-and-three-dimensional-forest-structure",
    "href": "topics/SART_theory.html#introduction-tomography-and-three-dimensional-forest-structure",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "",
    "text": "Synthetic Aperture Radar (SAR) tomography, or TomoSAR, represents a powerful technique for reconstructing three-dimensional forest structure from radar observations. The term “tomography” derives from Greek: “tomos” (slice or section) and “graphia” (to describe or write). In essence, tomography enables the reconstruction of an object’s internal structure by analyzing its projections from multiple viewing angles.\nThe fundamental principle of tomography is well known from medical imaging, where techniques such as computed tomography (CT) or magnetic resonance imaging (MRI) reconstruct the internal structure of the human body by taking slice measurements from different angles. SAR tomography applies similar concepts to remote sensing of forests, using multiple radar acquisitions from different spatial positions to resolve the vertical structure within each image pixel.\n\n\n\nVideo\n\n\nFigure 1: Tomographic reconstruction principle. Multiple projections through an object (in this case, showing projections through the Greek letter π) are combined to reconstruct the 2D or 3D structure. Each projection represents the integrated signal along a specific viewing angle. By combining projections from multiple angles, the internal structure can be recovered. This same principle applies to SAR tomography, where multiple SAR images from different baselines provide the “projections” needed to reconstruct the vertical structure of forests. Source: https://en.wikipedia.org/wiki/Tomographic_reconstruction.\n\n\n\nAs illustrated in Figure 1, projections taken from various angles contain information about the object’s internal structure. Through mathematical processing, including filtering techniques such as high-pass filters, a clear reconstruction of the original object can be achieved. While medical tomography scans a patient lying on a table through a rotating scanner, SAR tomography achieves similar results by acquiring radar images from different flight tracks or orbital positions."
  },
  {
    "objectID": "topics/SART_theory.html#airborne-and-spaceborne-tomosar-systems",
    "href": "topics/SART_theory.html#airborne-and-spaceborne-tomosar-systems",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "2 Airborne and Spaceborne TomoSAR Systems",
    "text": "2 Airborne and Spaceborne TomoSAR Systems\n\n2.1 Airborne TomoSAR: High-Resolution Forest Mapping\nAirborne SAR tomography systems provide the highest resolution observations of forest structure. The F-SAR system operated by the German Aerospace Center (DLR) exemplifies this capability, offering full polarimetric data in four wavelength bands (X-, C-, S-, and L-band).\n\n\n\n\n\n\nFigure 2: Airborne TomoSAR visualization of forest structure using F-SAR L-band data. The tomographic reconstruction reveals detailed vertical structure, with surface terrain visible in flat areas and multi-layered forest structure in dense vegetation. Source: DLR/EO-College.\n\n\n\nFigure 2 demonstrates airborne TomoSAR’s ability to penetrate forest canopies and reveal both canopy structure and underlying terrain. When scanning flat terrain, a relatively uniform surface appears, while forest areas show distinct layering—returns from the canopy top, internal canopy structure, and the ground beneath. In particularly dense forests, complex vertical structure becomes visible, analogous to scanning a body in a medical tomography system.\nThe advantage of airborne TomoSAR is complete forest coverage with fine spatial detail. However, compared to airborne lidar systems, the vertical resolution of SAR tomography is typically coarser, though it provides unique information about electromagnetic scattering properties related to forest structure, moisture content, and biomass.\n\n\n2.2 Spaceborne TomoSAR: The BIOMASS Mission\nUntil recently, no satellite mission was specifically designed for SAR tomography of forests. The European Space Agency’s (ESA) BIOMASS mission, launched in 2024, represents the first satellite dedicated to tomographic forest observations.\n\n\n\n\n\n\nFigure 3: Spaceborne TomoSAR concept with ESA’s BIOMASS mission. The picture illustrates the satellite acquiring multiple images from different orbital positions to create the synthetic aperture needed for tomography. The satellite carries a large P-band SAR antenna and will acquire images repeatedly over the same area as its orbit naturally varies, building up the necessary baselines for 3D forest reconstruction. It enables deep canopy penetration for three-dimensional forest structure reconstruction and biomass estimation. Source: European Space Agency - ESA.\n\n\n\nThe BIOMASS satellite, depicted in Figure 3, operates at P-band (longer wavelength around 70 cm), which provides superior penetration through forest canopies compared to shorter wavelengths. The mission strategy involves repeated observations of the same location—approximately seven acquisitions within an 18-day period—enabling tomographic reconstruction of forest vertical structure.\nA key innovation of P-band observations is the ability to image not only the upper canopy but also deeper structural elements including tree trunks and understory vegetation. This capability is essential for accurate forest height retrieval and above-ground biomass (AGB) estimation, particularly in dense tropical forests where shorter wavelengths may saturate."
  },
  {
    "objectID": "topics/SART_theory.html#fundamental-principles-phase-and-interferometry",
    "href": "topics/SART_theory.html#fundamental-principles-phase-and-interferometry",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "3 Fundamental Principles: Phase and Interferometry",
    "text": "3 Fundamental Principles: Phase and Interferometry\n\n3.1 Phase as a Relative Distance Measure\nThe foundation of SAR tomography lies in interferometry, which exploits phase measurements to determine relative distances. In radar systems, phase represents the position within a wave cycle and can be understood as a relative distance measurement.\n\n\n\n\n\n\nFigure 4: Phase as a relative distance indicator. Left: Complex number representation showing phase angle φ. Center: Sinusoidal wave showing the relationship between wavelength λ and phase. Right: Phase difference between two points (A, B₁, B₂) separated by distance d relates to their relative positions. Source: Iain H. Woodhouse, 2017\n\n\n\nAs illustrated in Figure 4, electromagnetic waves can be represented as rotating vectors (phasors) or sinusoidal functions. The key relationship is that one complete 2π rotation corresponds to one wavelength (λ) of propagation. If we know the phase difference (Δφ) between two observations, we can estimate the corresponding path difference:\n\\[\\Delta\\phi = \\frac{2\\pi}{\\lambda} \\cdot 2\\Delta R\\]\nwhere ΔR is the range difference and the factor of 2 accounts for the two-way travel (to target and back).\nConsider two radar sensors (B₁ and B₂) observing the same forest target. The phase difference between their observations encodes information about the relative distance from each sensor to the target. This principle forms the basis of SAR interferometry (InSAR).\n\n\n3.2 SAR Interferometry (InSAR)\nInSAR combines observations from two or more SAR acquisitions to measure surface topography and detect changes. The term “interferometry” refers to measuring properties through the interference of waves.\n\n\n\n\n\n\nFigure 5: InSAR principle illustrated with interference patterns. Left: Two wave sources create characteristic fringe patterns where waves constructively or destructively interfere. Right: Applying this principle from space, two SAR acquisitions from slightly different positions create interferometric fringes that encode topographic information. Maxima occur when path difference ΔR = nλ, corresponding to phase difference Δφ = n·2π. Source: Iain H. Woodhouse, 2017\n\n\n\nFigure 5 demonstrates the interferometric principle. When two wave sources emit coherent radiation, regions where the waves arrive in phase create maxima (constructive interference), while regions where they arrive out of phase create minima (destructive interference). This creates characteristic fringe patterns.\nIn spaceborne InSAR, even over completely flat terrain, fringe patterns emerge because different locations have different distances to the two sensor positions. Steep slopes facing the sensor produce dense fringes, while slopes facing away from the sensor show sparse fringes. These fringe patterns directly relate to surface topography.\n\n\n3.3 From Phase Images to Elevation Models\nWhile individual SAR phase images appear chaotic and random, the phase difference between two carefully selected acquisitions reveals meaningful patterns.\n\n\n\n\n\n\nFigure 6: From random phase to meaningful interferometric patterns. Left: Amplitude and phase of a single SAR image—the phase appears random. Center: Phase difference between two SAR images (wrapped interferogram) reveals systematic fringe patterns. Right: After unwrapping and further processing, a Digital Elevation Model (DEM) is derived. Source: DLR\n\n\n\nFigure 6 illustrates this transformation. A single SAR image’s phase appears as random noise, but when two images are properly aligned and their phases differenced, regular fringe patterns emerge (the wrapped interferogram). Through phase unwrapping and geometric processing, these fringes can be converted into topographic height information, producing a Digital Elevation Model (DEM).\n\n\n3.4 Coherence: Quality of Interferometric Phase\nNot all interferometric phase measurements are equally reliable. Coherence quantifies the quality and consistency of the interferometric phase.\n\n\n\n\n\n\nFigure 7: Coherence as an indicator of interferometric phase quality. Interferograms with different coherence values (1.0, 0.8, 0.4, 0.2) show progressively degraded fringe visibility. High coherence (γ ≈ 1) indicates clear, reliable phase patterns, while low coherence (γ ≈ 0) indicates noisy, unreliable phase information. Source: Irena Hajnsek\n\n\n\nCoherence (\\(\\hat{\\gamma}\\)) is calculated as:\n\\[\\hat{\\gamma} = \\frac{\\sum_{n=1}^{N} y_1 y_2^*}{\\sqrt{\\sum_{n=1}^{N} |y_1|^2 \\sum_{n=1}^{N} |y_2|^2}}\\]\nwhere \\(y_1\\) and \\(y_2\\) are complex SAR observations, and the asterisk denotes complex conjugation. Coherence ranges from 0 (completely decorrelated) to 1 (perfectly correlated).\nAs shown in Figure 7:\n\nCoherence = 1.0: Perfect fringe visibility with minimal noise\nCoherence = 0.8: Clear fringes with some degradation\nCoherence = 0.4: Noisy fringes, information partially degraded\nCoherence = 0.2: Fringes barely visible, unreliable phase\n\nHigh coherence indicates low phase noise and reliable distance measurements. In forest environments, coherence typically decreases over time as vegetation changes (temporal decorrelation) and varies with wavelength and environmental conditions. Coherence serves as a critical input parameter for TomoSAR processing, indicating which measurements are sufficiently reliable for tomographic inversion."
  },
  {
    "objectID": "topics/SART_theory.html#tomosar-three-dimensional-resolution-cells",
    "href": "topics/SART_theory.html#tomosar-three-dimensional-resolution-cells",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "4 TomoSAR: Three-Dimensional Resolution Cells",
    "text": "4 TomoSAR: Three-Dimensional Resolution Cells\n\n4.1 From 2D Pixels to 3D Voxels\nConventional SAR imaging collapses all scatterers within a resolution cell into a single pixel. TomoSAR extends this to resolve the vertical distribution of scatterers within each pixel, effectively creating three-dimensional resolution cells.\n\n\n\n\n\n\nFigure 8: TomoSAR principle: achieving three-dimensional resolution. Multiple flight tracks or orbital passes create a synthetic aperture in the cross-range (elevation) direction, enabling vertical resolution within each SAR pixel. The 3D resolution cell dimensions depend on pulse bandwidth W (slant range), synthetic aperture in azimuth (Aaz), and synthetic aperture in cross-range elevation (Acr). Source: Nesrin Salepci, EO-College (lincense: CC BY-SA 4.0)\n\n\n\nFigure 8 illustrates the TomoSAR geometry. While conventional SAR creates a synthetic aperture along the flight direction (azimuth) to achieve fine azimuth resolution, TomoSAR adds multiple observations from different cross-track positions to create an additional synthetic aperture perpendicular to the line of sight. This elevation aperture enables vertical resolution.\nThe three-dimensional resolution cell is characterized by:\n\nSlant range resolution \\(\\delta_{sr} = \\frac{c}{2W}\\), where \\(c\\) is the speed of light and \\(W\\) is the signal bandwidth\nAzimuth resolution \\(\\delta_{az} = \\frac{\\lambda r_0}{2A_{az}}\\), where \\(r_0\\) is the range distance and \\(A_{az}\\) is the synthetic aperture in azimuth\nCross-range elevation resolution \\(\\delta_{cr} = \\frac{\\lambda r_0}{2A_{cr}}\\), where \\(A_{cr}\\) is the synthetic aperture in elevation\nVertical (height) resolution \\(\\delta_z = \\delta_{cr} \\sin(\\theta_i)\\), where \\(\\theta_i\\) is the incidence angle\n\nThe tomographic resolution cell is oriented according to the radar viewing geometry, with the vertical extent depending on the number of flight tracks, their separation, and the wavelength used."
  },
  {
    "objectID": "topics/SART_theory.html#tomosar-of-forests-key-influencing-factors",
    "href": "topics/SART_theory.html#tomosar-of-forests-key-influencing-factors",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "5 TomoSAR of Forests: Key Influencing Factors",
    "text": "5 TomoSAR of Forests: Key Influencing Factors\nThe reflectivity profiles retrieved through TomoSAR depend on multiple interacting factors related to the sensor, platform, processing algorithms, and forest characteristics themselves.\n\n\n\n\n\n\nFigure 9: TomoSAR reflectivity profiles showing vertical forest structure. Left: 3D visualization of TomoSAR data over forest, with azimuth and slant range dimensions. Right: Vertical profiles at different height layers (0-50m), revealing surface, understory, and canopy scattering components. Key factors influencing these profiles include polarization, wavelength, flight configuration, inversion algorithms, and forest vertical structure. Source: Multiple\n\n\n\nAs illustrated in Figure 9, TomoSAR provides vertical profiles of radar reflectivity, typically showing distinct peaks corresponding to the ground surface and canopy elements. However, the characteristics of these profiles are sensitive to:\n\nSensor properties: Polarization and wavelength\nPlatform configuration: Number and spacing of flight tracks or orbital passes\nProcessing methods: Choice of inversion algorithm (e.g., Capon, beamforming)\nForest characteristics: Vertical structure, biomass, and moisture content\n\n\n5.1 Effect of Polarization\nDifferent polarizations interact with forest structure in distinct ways. HH polarization (horizontal transmit and receive) tends to penetrate more deeply, showing stronger ground returns. HV or VH cross-polarization emphasizes volume scattering from branches and foliage. VV polarization shows intermediate behavior.\n\n\n\n\n\n\nFigure 10: TomoSAR profiles for different polarizations. Left: False-color composite (HH:Red, HV:Green, VV:Blue) showing structural differences. Right: Vertical profiles for HH, HV, and VV polarizations, and the combined RGB representation. F-SAR P-band data with 11 flight tracks. HH shows stronger ground return, HV shows more distributed volume scattering, VV is intermediate. Source: Multiple\n\n\n\nFigure 10 demonstrates these polarimetric differences. In the false-color composite on the left, different forest types and structures appear in different colors because each exhibits distinct polarimetric scattering properties. The vertical profiles on the right reveal:\n\nHH polarization: Strong ground peak, indicating penetration to the surface\nHV polarization: More distributed scattering throughout the canopy volume, with reduced ground contribution\nVV polarization: Intermediate characteristics between HH and HV\n\nThis polarimetric sensitivity enables scattering mechanism decomposition, separating ground scattering from volume scattering.\n\n\n5.2 Scattering Mechanism Decomposition\nPolarimetric TomoSAR can decompose the total radar return into contributions from different scattering mechanisms, primarily ground scattering and volume scattering.\n\n\n\n\n\n\nFigure 11: Decomposition of TomoSAR profiles into ground and volume scattering mechanisms. Left: Top image shows mean reflectivity for HH polarization. Middle and bottom images show Capon-processed profiles for HH and HV. Right: Separated ground and volume scattering components for the first and second scattering mechanisms. Forest areas show strong volume scattering (canopy) while ground scattering dominates in cleared areas. Source: Tebaldini (2009)\n\n\n\nAs illustrated in Figure 11, advanced polarimetric-interferometric algorithms can separate:\n\nGround scattering: Surface returns from the soil or understory ground, appearing as a concentrated peak at low elevation\nVolume scattering: Distributed returns from the canopy, appearing as a broader vertical distribution\n\nThis decomposition is particularly valuable for accurate ground surface estimation beneath forests, which is essential for biomass retrieval and change detection.\n\n\n5.3 Effect of Wavelength\nDifferent radar wavelengths penetrate forest canopies to different depths, resulting in distinct TomoSAR signatures.\n\n\n\n\n\n\nFigure 12: TomoSAR profiles at different wavelengths. Top to bottom: P-band HV 2009, L-band HV 2009, L-band HV 2013, X-band HV 2013. P-band shows deepest penetration with clear ground returns even in dense forest. L-band shows moderate penetration. X-band shows minimal penetration, with scattering concentrated in the upper canopy. Source: Pardini et al. (2019). Tree illustrations on the right indicate the relative penetration depth of each wavelength. Source: Thuy Le Toan, CESBIO, France.\n\n\n\nFigure 12 compares profiles across wavelengths ranging from X-band (~3 cm) to P-band (~70 cm):\n\nX-band (HV, 2013): Minimal penetration, scattering concentrated in the uppermost canopy layer. Suitable for canopy surface mapping but limited for biomass estimation in dense forests.\nL-band (HV, 2009 and 2013): Moderate penetration, capturing both canopy and some ground returns. Sensitivity to forest structure and biomass, but may saturate in very dense forests.\nP-band (HV, 2009): Deep penetration, clear ground returns even beneath dense canopy. Strong sensitivity to forest height and biomass across a wide dynamic range.\n\nThe wavelength selection profoundly affects TomoSAR’s sensitivity to forest structure, height, and biomass. P-band is optimal for global forest mapping missions like BIOMASS, while L-band offers a good compromise between penetration and spatial resolution for regional studies.\n\n\n5.4 Effect of Flight Configuration\nThe number and spatial arrangement of flight tracks or orbital passes directly determine the tomographic resolution and ambiguity properties.\n\n\n\n\n\n\nFigure 13: Effect of flight track number on TomoSAR profiles. Left: Results using 10 flight tracks showing high vertical resolution with clear separation of ground and canopy peaks. Right: Results using only 5 flight tracks showing degraded resolution and increased noise. Fewer tracks result in broader main lobes and higher sidelobes in the vertical profile, reducing the ability to separate close scatterers.\n\n\n\nAs demonstrated in Figure 13:\n\n10 flight tracks: Sharp, well-defined vertical profiles with distinct ground and canopy peaks. Higher vertical resolution enables precise forest structure characterization.\n5 flight tracks: Broader, noisier profiles with reduced ability to separate scatterers. Increased sidelobes can create ambiguous peaks.\n\nThe vertical resolution improves with the total span of the synthetic aperture in elevation. More tracks also improve signal-to-noise ratio and reduce ambiguities. However, practical constraints (temporal decorrelation, mission complexity, cost) limit the number of acquisitions. Optimizing the baseline distribution—the spacing between flight tracks—is critical for achieving the desired vertical resolution while maintaining adequate coherence.\n\n\n5.5 Effect of Inversion Algorithms\nDifferent tomographic inversion algorithms trade off between resolution, noise suppression, and computational complexity.\n\n\n\n\n\n\nFigure 14: Comparison of Capon and beamforming algorithms for TomoSAR. Left: Capon algorithm provides higher resolution with sharper peaks and better separation of ground and canopy. Right: Beamforming (conventional) algorithm shows broader peaks and reduced resolution but is more robust to noise and requires less computation. The choice of algorithm affects the interpretability and quantitative use of TomoSAR profiles.\n\n\n\nFigure 14 compares two common approaches:\n\nCapon (adaptive) beamforming: Provides superior resolution by adaptively minimizing contributions from directions other than the target. Sharper peaks enable better separation of close scatterers (e.g., ground and low canopy). More sensitive to noise and requires careful coherence estimation.\nConventional (Fourier) beamforming: Lower resolution but more robust. Broader peaks may merge close scatterers but provides stable estimates even with lower coherence.\n\nOther advanced algorithms include compressive sensing methods and model-based approaches. The algorithm selection depends on the data quality (coherence, number of acquisitions) and the application requirements (height precision, computational resources)."
  },
  {
    "objectID": "topics/SART_theory.html#sensitivity-to-forest-height-and-biomass",
    "href": "topics/SART_theory.html#sensitivity-to-forest-height-and-biomass",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "6 Sensitivity to Forest Height and Biomass",
    "text": "6 Sensitivity to Forest Height and Biomass\nA primary application of TomoSAR is estimating forest structural parameters, particularly tree height and above-ground biomass (AGB).\n\n\n\n\n\n\nFigure 15: TomoSAR sensitivity to forest height and biomass. (a) Lidar reference data showing forest height (RH100, 0-50m) and AGB (0-600 Mg/ha) over Lopé, Gabon. (b) TomoSAR backscatter profiles for different height classes (0-15m, 15-30m, 30-45m, 45-60m) and AGB classes (0-200, 200-400, 400-600, 600-800 Mg/ha). Profiles systematically shift in height and shape with increasing forest height and biomass, demonstrating TomoSAR’s sensitivity to forest structure. AfriSAR airborne campaign data. Source: Liu et al. (2024)\n\n\n\nFigure 15 presents results from the AfriSAR airborne campaign in tropical forests of Gabon. Key observations:\n\nHeight sensitivity: Forests of different heights produce TomoSAR profiles with distinct vertical extents. Taller forests show peaks at higher elevations, while shorter forests concentrate scattering at lower heights.\nBiomass sensitivity: As AGB increases, TomoSAR backscatter intensity and vertical extent change systematically. The distribution of scattering through the canopy relates to biomass density.\nProfile shape changes: Not only peak position but also profile shape (width, skewness, secondary peaks) varies with forest structure.\n\nQuantitatively relating TomoSAR profiles to height and AGB requires careful analysis. The phase center of the profile is typically lower than the canopy top, necessitating correction factors. Various metrics can be extracted from profiles:\n\nPhase center height: The centroid of the vertical reflectivity profile\nBackscatter intensity at specific heights: E.g., reflectivity integrated from 20-30m\nProfile moments: Height of maximum, variance, skewness\nPolarimetric-interferometric parameters: Ground-to-volume ratio, extinction coefficient\n\nMachine learning approaches can combine multiple TomoSAR features with training data to develop robust height and biomass estimation models."
  },
  {
    "objectID": "topics/SART_theory.html#environmental-and-temporal-effects",
    "href": "topics/SART_theory.html#environmental-and-temporal-effects",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "7 Environmental and Temporal Effects",
    "text": "7 Environmental and Temporal Effects\nTomoSAR observations are not static—they respond to weather conditions, seasonal changes, and forest dynamics.\n\n\n\n\n\n\nFigure 16: TomoSAR response to weather and seasonal effects. Left panels: Tomographic slices before and after rainfall, showing reduced ground power and increased phase center height after rain due to moisture absorption. Right panels: Spring (leaf-on) versus autumn (leaf-off) observations, showing increased ground power in autumn when leaves have fallen. Weather and phenology significantly affect TomoSAR profiles and must be considered in operational applications. Source: Pardini et al. (2014)\n\n\n\nFigure 16 illustrates two key temporal influences:\n\n7.1 Weather Effects (Moisture)\n\nAfter rainfall: Increased moisture content in canopy and soil enhances absorption, reducing radar penetration. Ground returns weaken relative to volume returns, and the apparent phase center height increases.\nDry conditions: Lower moisture content allows deeper penetration, strengthening ground returns and lowering the phase center.\n\nThese effects are particularly pronounced at shorter wavelengths (X- and C-band) where water absorption is stronger. P-band is less affected but not immune. Consistency in environmental conditions across acquisitions is important for reliable tomographic reconstruction.\n\n\n7.2 Seasonal Effects (Phenology)\n\nLeaf-on (spring/summer): Full canopy with maximum foliage density creates strong volume scattering, reducing ground visibility.\nLeaf-off (autumn/winter): In deciduous forests, leaf fall reduces volume scattering and dramatically increases ground returns. Bare branches scatter less, allowing better ground surface mapping.\n\nSeasonal effects are critical for deciduous and mixed forests in temperate and boreal regions. Tropical evergreen forests show less pronounced phenological variation but may respond to wet/dry season transitions. Understanding these temporal dynamics enables optimal acquisition planning and improved interpretation of multi-temporal TomoSAR data."
  },
  {
    "objectID": "topics/SART_theory.html#from-tomographic-profiles-to-forest-metrics",
    "href": "topics/SART_theory.html#from-tomographic-profiles-to-forest-metrics",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "8 From Tomographic Profiles to Forest Metrics",
    "text": "8 From Tomographic Profiles to Forest Metrics\nThe theoretical principles presented here—phase-based distance measurement, interferometric coherence, multi-baseline tomographic inversion, and sensitivity to forest structure—provide the foundation for operational forest monitoring with TomoSAR. The key steps in translating tomographic radar data into forest structural information are:\n\nAcquisition planning: Selecting appropriate wavelength, polarization, and baseline configuration for the forest type and application\nCoherence optimization: Minimizing temporal decorrelation through rapid repeat acquisitions or simultaneous multi-antenna systems\nTomographic inversion: Applying suitable algorithms to reconstruct vertical reflectivity profiles\nGround surface estimation: Separating ground from volume scattering to establish terrain reference\nFeature extraction: Deriving height, biomass-sensitive metrics from profile characteristics\nValidation and calibration: Comparing with ground truth data (lidar, field measurements) to develop quantitative relationships\nScaling and mapping: Applying validated models to produce forest height and biomass maps\n\nThe integration of TomoSAR with complementary technologies—particularly lidar for validation and optical imagery for context—enables robust, multi-sensor forest monitoring systems. Spaceborne missions like BIOMASS will extend these capabilities globally, providing unprecedented insights into three-dimensional forest structure and its changes over time.\n\nThis theoretical foundation prepares you to interpret and apply TomoSAR methods in forest remote sensing, understanding both the physical principles underlying the technique and the practical factors that influence its performance. The tutorial section demonstrate these concepts through hands-on analysis of real TomoSAR data, transforming vertical reflectivity profiles into quantitative estimates of forest height and biomass."
  },
  {
    "objectID": "topics/SART_theory.html#references",
    "href": "topics/SART_theory.html#references",
    "title": "Theoretical Background: Synthetic Aperture Radar Tomography of Forests",
    "section": "9 References",
    "text": "9 References\nKey references for further reading:\n\nWoodhouse, I.H. (2006). Introduction to Microwave Remote Sensing. CRC Press. https://doi.org/10.1201/9781315272573\nTebaldini, S. (2009). Algebraic Synthesis of Forest Scenarios From Multibaseline PolInSAR Data. IEEE Transactions on Geoscience and Remote Sensing, 47(12), 4132-4142.\nPardini, M., Cantini, A., Lombardini, F., & Papathanassiou, K. (2014). 3-D Structure Of Forests: First Analysis of Tomogram Changes Due to Weather and Seasonal Effects at L-Band. EUSAR 2014. 10th European Conference on Synthetic Aperture Radar, Berlin, Germany.\nPardini, M., Armston, J., Qi, W., et al. (2019). Early Lessons on Combining Lidar and Multi-baseline SAR Measurements for Forest Structure Characterization. Surveys in Geophysics, 40, 803-837. https://doi.org/10.1007/s10712-019-09553-9\nLiu, X., Neigh, C.S.R., Pardini, M., & Forkel, M. (2024). Estimating forest height and above-ground biomass in tropical forests using P-band TomoSAR and GEDI observations. International Journal of Remote Sensing, 45(9), 3129-3148.https://doi.org/10.1080/01431161.2024.2343134\nEO-College SAR Tomography Tutorial\nESA PolInSAR Workshop materials"
  }
]